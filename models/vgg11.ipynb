{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MbtUcKfE_I4g"
      },
      "source": [
        "**Installs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzg4cO9xLvUG",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fxs_3zcG_NZd"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C7N0hU-VLx8W",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg11\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "#NUM_CLASSES = 102\n",
        "NUM_CLASSES = 5\n",
        "DEVICE = 'cuda'\n",
        "MOMENTUM = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uvABcepY_Vfe"
      },
      "source": [
        "**Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vztVCv3fQXjR",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def get_datasets(train_data_dir, test_data_dir, compose=[transforms.Resize(224),\n",
        "                                                         transforms.CenterCrop(224),\n",
        "                                                         transforms.ToTensor()\n",
        "                                                         ]):\n",
        "    train_transform = transforms.Compose(compose)\n",
        "    eval_transform = transforms.Compose([\n",
        "          #transforms.Resize(224),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor()#,\n",
        "          #transforms.Normalize((45.6068733, 0.81077038, 57.85301916), (66.92374056, 9.88349788, 49.96761776))\n",
        "          ])\n",
        "\n",
        "    '''\n",
        "    if not os.path.isdir('./Homework2-Caltech101'):\n",
        "        !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n",
        "\n",
        "    '''\n",
        "    if not os.path.isdir('./AIML_project'):\n",
        "        !git clone https://github.com/anphetamina/AIML_project.git\n",
        "    \n",
        "    train_dataset = torchvision.datasets.ImageFolder(train_data_dir, transform=train_transform)\n",
        "    test_dataset = torchvision.datasets.ImageFolder(test_data_dir, transform=eval_transform)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def test_network(net, test_dataset, batch_size):\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    net.train(False)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    sum_test_losses = 0.0\n",
        "    running_corrects = 0\n",
        "    for images, labels in test_dataloader:\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      test_loss = criterion(outputs, labels)\n",
        "      sum_test_losses += test_loss.item()*images.size(0)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "    # Calculate loss\n",
        "    test_loss = sum_test_losses / float(len(test_dataset))\n",
        "\n",
        "    return accuracy, test_loss\n",
        "\n",
        "def train_network(net, parameters_to_optimize, learning_rate, num_epochs, batch_size, weight_decay, step_size, gamma, train_dataset, val_dataset=None, verbosity=False, plot=False):\n",
        "  \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=learning_rate, momentum=MOMENTUM, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    net = net.to(DEVICE)\n",
        "    best_net = vgg11()\n",
        "    best_net = best_net.to(DEVICE)\n",
        "    best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "\n",
        "    current_step = 0\n",
        "    best_val_accuracy = 0.0\n",
        "    best_val_loss = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_running_corrects = 0\n",
        "        sum_train_losses = 0.0\n",
        "\n",
        "        for images, labels in train_dataloader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            net.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            train_running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "            loss = criterion(outputs, labels)\n",
        "            sum_train_losses += loss.item()*images.size(0)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            current_step += 1\n",
        "        \n",
        "        # Calculate accuracy on train set\n",
        "        train_accuracy = train_running_corrects / float(len(train_dataset))\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        if val_dataset is not None:\n",
        "            val_accuracy, val_loss = test_network(net, val_dataset, batch_size)\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_val_loss = val_loss\n",
        "                best_net.load_state_dict(net.state_dict())\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "            #acc_diff = train_accuracy-val_accuracy\n",
        "            #if acc_diff > 0.25:\n",
        "              #print(\"overfit -> train_accuracy {}, val_accuracy {}\".format(train_accuracy, val_accuracy))\n",
        "              #return best_net, best_val_accuracy, best_val_loss\n",
        "\n",
        "        \n",
        "\n",
        "        # Calculate loss on training set\n",
        "        train_loss = sum_train_losses/float(len(train_dataset))\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        if verbosity:\n",
        "            if val_dataset is not None:\n",
        "                print(\"train_acc: {}, val_acc: {}, train_loss: {}, val_loss: {} ({} / {})\".format(train_accuracy, val_accuracy, train_loss, val_loss, epoch+1, num_epochs))\n",
        "            else:\n",
        "                print(\"train_acc: {}, train_loss: {} ({} / {})\".format(train_accuracy, train_loss, epoch+1, num_epochs))\n",
        "        \n",
        "\n",
        "        if train_accuracy < 0.25 and epoch > num_epochs*0.1 or train_accuracy < 0.35 and epoch > num_epochs*0.5:\n",
        "          print(\"underfit -> train_accuracy = {}\".format(train_accuracy))\n",
        "          return best_net, best_val_accuracy, best_val_loss\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    if plot:\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        line1, = ax.plot(train_losses, label='Loss on training set')\n",
        "        line2, = ax.plot(train_accuracies, label='Accuracy on training set')\n",
        "        ax.legend()\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.show()\n",
        "\n",
        "        if val_dataset is not None:\n",
        "            fig, ax = plt.subplots()\n",
        "            line1, = ax.plot(val_accuracies, label='Accuracy on validation set', color='C2')\n",
        "            line2, = ax.plot(train_accuracies, label='Accuracy on training set', color='C3')\n",
        "            ax.legend()\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.show()\n",
        "        \n",
        "            fig, ax = plt.subplots()\n",
        "            line1, = ax.plot(val_losses, label='Loss on validation set', color='C1')\n",
        "            line2, = ax.plot(train_losses, label='Loss on training set', color='C7')\n",
        "            ax.legend()\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.show()\n",
        "\n",
        "    \n",
        "    return best_net, best_val_accuracy, best_val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6fTm2sD_BOt"
      },
      "source": [
        "**Train + validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XtBXC1cO_A6A",
        "outputId": "59c7f2ef-bbd5-424d-e27b-78ac91d020c6",
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# lr 0.0006444500508054211, batch 14, decay 2.1280582227123365e-05, gamma 0.19924404264743992, val accuracy 0.6305418719211823, val loss 1.0403618915327664 [5 / 50]\n",
        "# lr 0.00038041059192815333, batch 9, decay 3.8372561126798785e-05, gamma 0.057680309789029396, val accuracy 0.6108374384236454, val loss 0.9877617955207825 [38 / 50]\n",
        "# lr 0.00043660847130590896, batch 10, decay 0.00025031720443271155, gamma 0.011678955740792939, val accuracy 0.5615763546798029, val loss 1.0453474251507537 [43 / 50]\n",
        "# lr 0.00027531434783290124, batch 9, decay 4.3783604017624755e-06, gamma 0.11844128056704877, val accuracy 0.5517241379310345, val loss 1.117455956383879 [44 / 50]\n",
        "# lr 0.0007220498435995008, batch 14, decay 2.228552014354877e-05, gamma 0.08113961287843949, val accuracy 0.625615763546798, val loss 0.9968108699239534 [49 / 50]\n",
        "# lr 0.0008377019231346562, batch 8, decay 2.4427015675775187e-06, gamma 0.00903130010323455, val accuracy 0.5849802371541502, val loss 1.0701400147596367 [1 / 50]\n",
        "# lr 0.0010316163585472981, batch 8, decay 1.8309942558988887e-05, gamma 0.002673690056313373, val accuracy 0.5592885375494071, val loss 1.0480610431418589 [5 / 50]\n",
        "# lr 0.0016661746592012004, batch 8, decay 3.3763075569909223e-06, gamma 0.006052773438030023, val accuracy 0.6067193675889329, val loss 1.0441360360548901 [6 / 50]\n",
        "# lr 0.0005075392266021225, batch 12, decay 0.00015097107216674634, gamma 0.23307879269069465, val accuracy 0.6650246305418719, val loss 1.0321336309310838 [12 / 50]\n",
        "# lr 0.0005293020727186627, batch 11, decay 0.0001569197221403604, gamma 0.0797979883084818, val accuracy 0.6059113300492611, val loss 1.0818345975406065 [13 / 50]\n",
        "# lr 0.0008554266290399184, batch 11, decay 0.0006533010900552039, gamma 0.06721270304757815, val accuracy 0.7941176470588235, val loss 0.9317472677897004 [18 / 50]\n",
        "BATCH_SIZE = 11\n",
        "LR = 0.0008554266290399184\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.0006533010900552039\n",
        "NUM_EPOCHS = 100\n",
        "STEP_SIZE = 60\n",
        "GAMMA = 0.06721270304757815\n",
        "\n",
        "\n",
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec-224'\n",
        "#TRAIN_DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n",
        "compose=[#transforms.Resize(224),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.RandomGrayscale(),\n",
        "         transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "         transforms.ToTensor()\n",
        "         ]\n",
        "train_dataset, val_dataset = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, compose)\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 10]\n",
        "val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 10]\n",
        "val_dataset = Subset(val_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))\n",
        "\n",
        "net = vgg11()\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "best_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True, plot=True)\n",
        "\n",
        "print('val accuracy {}'.format(val_accuracy))\n",
        "print('val loss {}'.format(val_loss))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set 745\n",
            "validation set 83\n",
            "train_acc: 0.23221476510067113, val_acc: 0.3855421686746988, train_loss: 1.6004910485056423, val_loss: 1.5808396784656018 (1 / 100)\n",
            "train_acc: 0.24563758389261744, val_acc: 0.3493975903614458, train_loss: 1.576837316775482, val_loss: 1.5616827011108398 (2 / 100)\n",
            "train_acc: 0.28053691275167786, val_acc: 0.24096385542168675, train_loss: 1.5653064606173726, val_loss: 1.5495410855994167 (3 / 100)\n",
            "train_acc: 0.2966442953020134, val_acc: 0.26506024096385544, train_loss: 1.5495984461483538, val_loss: 1.5110053168721946 (4 / 100)\n",
            "train_acc: 0.33557046979865773, val_acc: 0.40963855421686746, train_loss: 1.497584098137465, val_loss: 1.4164257365536976 (5 / 100)\n",
            "train_acc: 0.3610738255033557, val_acc: 0.3855421686746988, train_loss: 1.4401303955372549, val_loss: 1.3749975971428745 (6 / 100)\n",
            "train_acc: 0.3610738255033557, val_acc: 0.37349397590361444, train_loss: 1.352284965339123, val_loss: 1.2897028808134148 (7 / 100)\n",
            "train_acc: 0.3932885906040268, val_acc: 0.42168674698795183, train_loss: 1.3528201581647732, val_loss: 1.2952981770756733 (8 / 100)\n",
            "train_acc: 0.40939597315436244, val_acc: 0.5060240963855421, train_loss: 1.286464682841461, val_loss: 1.2457146895937172 (9 / 100)\n",
            "train_acc: 0.42416107382550333, val_acc: 0.42168674698795183, train_loss: 1.247344714923193, val_loss: 1.2093518031648842 (10 / 100)\n",
            "train_acc: 0.46442953020134226, val_acc: 0.43373493975903615, train_loss: 1.2340261348141919, val_loss: 1.2018444473484913 (11 / 100)\n",
            "train_acc: 0.45503355704697984, val_acc: 0.46987951807228917, train_loss: 1.2298952741110885, val_loss: 1.3291301109704627 (12 / 100)\n",
            "train_acc: 0.4469798657718121, val_acc: 0.40963855421686746, train_loss: 1.209813902362081, val_loss: 1.3337255946124893 (13 / 100)\n",
            "train_acc: 0.4483221476510067, val_acc: 0.4457831325301205, train_loss: 1.2748365667042316, val_loss: 1.2625294729887722 (14 / 100)\n",
            "train_acc: 0.48053691275167787, val_acc: 0.5180722891566265, train_loss: 1.1729807255252096, val_loss: 1.1508866225380496 (15 / 100)\n",
            "train_acc: 0.4684563758389262, val_acc: 0.4819277108433735, train_loss: 1.1378929289395376, val_loss: 1.1277154088020325 (16 / 100)\n",
            "train_acc: 0.4684563758389262, val_acc: 0.4457831325301205, train_loss: 1.126265020738512, val_loss: 1.1347998574555638 (17 / 100)\n",
            "train_acc: 0.4604026845637584, val_acc: 0.40963855421686746, train_loss: 1.1311144429565276, val_loss: 1.1941223532320506 (18 / 100)\n",
            "train_acc: 0.5114093959731544, val_acc: 0.4939759036144578, train_loss: 1.1166739803032586, val_loss: 1.2145208271153003 (19 / 100)\n",
            "train_acc: 0.5073825503355704, val_acc: 0.4939759036144578, train_loss: 1.0955679730280934, val_loss: 1.0506560242319682 (20 / 100)\n",
            "train_acc: 0.5167785234899329, val_acc: 0.5180722891566265, train_loss: 1.0741357724938616, val_loss: 1.0289264393140034 (21 / 100)\n",
            "train_acc: 0.5221476510067115, val_acc: 0.4819277108433735, train_loss: 1.0525101349657815, val_loss: 1.1070762398731278 (22 / 100)\n",
            "train_acc: 0.5248322147651007, val_acc: 0.5542168674698795, train_loss: 1.044237240368888, val_loss: 1.0505836685019803 (23 / 100)\n",
            "train_acc: 0.5302013422818792, val_acc: 0.4819277108433735, train_loss: 1.0316489982124943, val_loss: 1.018858913197575 (24 / 100)\n",
            "train_acc: 0.5395973154362416, val_acc: 0.5662650602409639, train_loss: 1.0040066269420138, val_loss: 0.9792828617325748 (25 / 100)\n",
            "train_acc: 0.5798657718120805, val_acc: 0.46987951807228917, train_loss: 0.96553222669051, val_loss: 0.9958701413798045 (26 / 100)\n",
            "train_acc: 0.5624161073825503, val_acc: 0.5783132530120482, train_loss: 0.9549579288335455, val_loss: 0.934886513704277 (27 / 100)\n",
            "train_acc: 0.5919463087248322, val_acc: 0.5903614457831325, train_loss: 0.9519903329394808, val_loss: 0.91991309588214 (28 / 100)\n",
            "train_acc: 0.5624161073825503, val_acc: 0.5301204819277109, train_loss: 0.9780079277569815, val_loss: 0.9068129644336471 (29 / 100)\n",
            "train_acc: 0.5932885906040268, val_acc: 0.5421686746987951, train_loss: 0.9329899180655511, val_loss: 0.8848652480596519 (30 / 100)\n",
            "train_acc: 0.5852348993288591, val_acc: 0.5903614457831325, train_loss: 0.9194756058238497, val_loss: 0.8690519103084702 (31 / 100)\n",
            "train_acc: 0.6174496644295302, val_acc: 0.6024096385542169, train_loss: 0.894920318158681, val_loss: 0.8481341909213238 (32 / 100)\n",
            "train_acc: 0.614765100671141, val_acc: 0.5662650602409639, train_loss: 0.8403983126150681, val_loss: 0.822054506784462 (33 / 100)\n",
            "train_acc: 0.636241610738255, val_acc: 0.5903614457831325, train_loss: 0.8559321349499209, val_loss: 0.8166071449417666 (34 / 100)\n",
            "train_acc: 0.6295302013422819, val_acc: 0.5662650602409639, train_loss: 0.8478221238859548, val_loss: 0.8557676601122661 (35 / 100)\n",
            "train_acc: 0.6416107382550336, val_acc: 0.5662650602409639, train_loss: 0.7970329987922771, val_loss: 0.8792572660618517 (36 / 100)\n",
            "train_acc: 0.6563758389261745, val_acc: 0.5662650602409639, train_loss: 0.8153983694594978, val_loss: 0.9129814247050917 (37 / 100)\n",
            "train_acc: 0.6845637583892618, val_acc: 0.6746987951807228, train_loss: 0.7430312090672102, val_loss: 0.743530230349805 (38 / 100)\n",
            "train_acc: 0.676510067114094, val_acc: 0.6024096385542169, train_loss: 0.7129233799524756, val_loss: 0.6989649943558567 (39 / 100)\n",
            "train_acc: 0.6939597315436241, val_acc: 0.5903614457831325, train_loss: 0.7201897358134289, val_loss: 0.8336099468799959 (40 / 100)\n",
            "train_acc: 0.6899328859060403, val_acc: 0.6144578313253012, train_loss: 0.7123936238304881, val_loss: 0.7459342673600439 (41 / 100)\n",
            "train_acc: 0.7395973154362416, val_acc: 0.7228915662650602, train_loss: 0.6670922125545924, val_loss: 0.6106328220970659 (42 / 100)\n",
            "train_acc: 0.7476510067114094, val_acc: 0.6385542168674698, train_loss: 0.6313711321993962, val_loss: 0.7311621322689286 (43 / 100)\n",
            "train_acc: 0.7785234899328859, val_acc: 0.6987951807228916, train_loss: 0.5763022973033406, val_loss: 0.7208408756428454 (44 / 100)\n",
            "train_acc: 0.7838926174496644, val_acc: 0.7228915662650602, train_loss: 0.5203568086228114, val_loss: 0.5905121216572911 (45 / 100)\n",
            "train_acc: 0.7664429530201342, val_acc: 0.7469879518072289, train_loss: 0.5649438601012198, val_loss: 0.5577352567609534 (46 / 100)\n",
            "train_acc: 0.7946308724832215, val_acc: 0.7831325301204819, train_loss: 0.4651199334219798, val_loss: 0.5487387465784349 (47 / 100)\n",
            "train_acc: 0.8201342281879195, val_acc: 0.7590361445783133, train_loss: 0.4353168011111701, val_loss: 0.5262975076954048 (48 / 100)\n",
            "train_acc: 0.7959731543624161, val_acc: 0.6265060240963856, train_loss: 0.4878965275999684, val_loss: 1.0480043155601226 (49 / 100)\n",
            "train_acc: 0.825503355704698, val_acc: 0.7710843373493976, train_loss: 0.46335385517786015, val_loss: 0.5181772349828697 (50 / 100)\n",
            "train_acc: 0.8630872483221477, val_acc: 0.7951807228915663, train_loss: 0.37494061166208065, val_loss: 0.6139257036777864 (51 / 100)\n",
            "train_acc: 0.8442953020134228, val_acc: 0.7469879518072289, train_loss: 0.3884284551411667, val_loss: 0.61879678663001 (52 / 100)\n",
            "train_acc: 0.8966442953020134, val_acc: 0.6506024096385542, train_loss: 0.2921674149998482, val_loss: 1.1543410007494042 (53 / 100)\n",
            "train_acc: 0.8832214765100671, val_acc: 0.7951807228915663, train_loss: 0.31297173331797923, val_loss: 0.5297708834510252 (54 / 100)\n",
            "train_acc: 0.8536912751677852, val_acc: 0.7469879518072289, train_loss: 0.3886587739290807, val_loss: 0.6519749609820814 (55 / 100)\n",
            "train_acc: 0.8845637583892617, val_acc: 0.7710843373493976, train_loss: 0.30513191360155206, val_loss: 0.5582293600562107 (56 / 100)\n",
            "train_acc: 0.876510067114094, val_acc: 0.7349397590361446, train_loss: 0.2861609472949433, val_loss: 0.714414165860199 (57 / 100)\n",
            "train_acc: 0.9315436241610738, val_acc: 0.7831325301204819, train_loss: 0.1948374897004074, val_loss: 0.6520066372601383 (58 / 100)\n",
            "train_acc: 0.9181208053691275, val_acc: 0.6626506024096386, train_loss: 0.2316607667695756, val_loss: 0.9192473546388638 (59 / 100)\n",
            "train_acc: 0.9328859060402684, val_acc: 0.8433734939759037, train_loss: 0.19757431434567443, val_loss: 0.489986171833722 (60 / 100)\n",
            "train_acc: 0.9489932885906041, val_acc: 0.8192771084337349, train_loss: 0.144219143281147, val_loss: 0.4817037458520338 (61 / 100)\n",
            "train_acc: 0.9704697986577181, val_acc: 0.8072289156626506, train_loss: 0.10643517111909369, val_loss: 0.48127610833350437 (62 / 100)\n",
            "train_acc: 0.9731543624161074, val_acc: 0.8192771084337349, train_loss: 0.08817947365838921, val_loss: 0.5304614240864673 (63 / 100)\n",
            "train_acc: 0.9798657718120806, val_acc: 0.8313253012048193, train_loss: 0.08334595457613218, val_loss: 0.5557034847786627 (64 / 100)\n",
            "train_acc: 0.9758389261744966, val_acc: 0.8072289156626506, train_loss: 0.08983079051448895, val_loss: 0.5819424225623349 (65 / 100)\n",
            "train_acc: 0.9812080536912752, val_acc: 0.8192771084337349, train_loss: 0.0669344875730274, val_loss: 0.5552848128893648 (66 / 100)\n",
            "train_acc: 0.9677852348993289, val_acc: 0.8072289156626506, train_loss: 0.09566327087181127, val_loss: 0.5649849199746029 (67 / 100)\n",
            "train_acc: 0.9731543624161074, val_acc: 0.8313253012048193, train_loss: 0.09368847216754501, val_loss: 0.542092823569315 (68 / 100)\n",
            "train_acc: 0.9731543624161074, val_acc: 0.8313253012048193, train_loss: 0.08368389435681271, val_loss: 0.5483703897839569 (69 / 100)\n",
            "train_acc: 0.9771812080536912, val_acc: 0.8192771084337349, train_loss: 0.06597305738192126, val_loss: 0.5743252346853176 (70 / 100)\n",
            "train_acc: 0.978523489932886, val_acc: 0.8192771084337349, train_loss: 0.06725686336429977, val_loss: 0.5517041577721935 (71 / 100)\n",
            "train_acc: 0.974496644295302, val_acc: 0.8192771084337349, train_loss: 0.07910750646444176, val_loss: 0.590831056686051 (72 / 100)\n",
            "train_acc: 0.978523489932886, val_acc: 0.8072289156626506, train_loss: 0.07479764326716204, val_loss: 0.7131003198853458 (73 / 100)\n",
            "train_acc: 0.9838926174496644, val_acc: 0.8192771084337349, train_loss: 0.0588632503768973, val_loss: 0.6094901353255453 (74 / 100)\n",
            "train_acc: 0.9812080536912752, val_acc: 0.8192771084337349, train_loss: 0.060372870168711165, val_loss: 0.6227645457733467 (75 / 100)\n",
            "train_acc: 0.9664429530201343, val_acc: 0.8192771084337349, train_loss: 0.08422798765666983, val_loss: 0.5882129367351173 (76 / 100)\n",
            "train_acc: 0.9852348993288591, val_acc: 0.8313253012048193, train_loss: 0.0658671878156371, val_loss: 0.5930554306650736 (77 / 100)\n",
            "train_acc: 0.9879194630872483, val_acc: 0.8192771084337349, train_loss: 0.04359377808275089, val_loss: 0.614859878298748 (78 / 100)\n",
            "train_acc: 0.9825503355704698, val_acc: 0.8313253012048193, train_loss: 0.052910179019149316, val_loss: 0.6183420690786408 (79 / 100)\n",
            "train_acc: 0.9825503355704698, val_acc: 0.8313253012048193, train_loss: 0.05496910690672183, val_loss: 0.6520039953448507 (80 / 100)\n",
            "train_acc: 0.9798657718120806, val_acc: 0.8072289156626506, train_loss: 0.06130144094826416, val_loss: 0.6905504070132612 (81 / 100)\n",
            "train_acc: 0.978523489932886, val_acc: 0.8313253012048193, train_loss: 0.06028269273369579, val_loss: 0.6445762069828539 (82 / 100)\n",
            "train_acc: 0.9852348993288591, val_acc: 0.8313253012048193, train_loss: 0.05304493955691031, val_loss: 0.6448141936078129 (83 / 100)\n",
            "train_acc: 0.974496644295302, val_acc: 0.8313253012048193, train_loss: 0.07488278416180733, val_loss: 0.6180939518184547 (84 / 100)\n",
            "train_acc: 0.9852348993288591, val_acc: 0.8072289156626506, train_loss: 0.053545931174737035, val_loss: 0.6559268353304949 (85 / 100)\n",
            "train_acc: 0.9865771812080537, val_acc: 0.8072289156626506, train_loss: 0.053700566302747436, val_loss: 0.6988359753446407 (86 / 100)\n",
            "train_acc: 0.9771812080536912, val_acc: 0.8192771084337349, train_loss: 0.05932411375291874, val_loss: 0.6979589046551341 (87 / 100)\n",
            "train_acc: 0.9771812080536912, val_acc: 0.8192771084337349, train_loss: 0.05854563188720369, val_loss: 0.6768637170723404 (88 / 100)\n",
            "train_acc: 0.9852348993288591, val_acc: 0.8192771084337349, train_loss: 0.05009266315317156, val_loss: 0.6981908503067062 (89 / 100)\n",
            "train_acc: 0.9825503355704698, val_acc: 0.8313253012048193, train_loss: 0.047560056300279874, val_loss: 0.6659651991114559 (90 / 100)\n",
            "train_acc: 0.9892617449664429, val_acc: 0.8192771084337349, train_loss: 0.056182727870303234, val_loss: 0.7195166801831808 (91 / 100)\n",
            "train_acc: 0.9812080536912752, val_acc: 0.8192771084337349, train_loss: 0.05003025724342399, val_loss: 0.7155924473541329 (92 / 100)\n",
            "train_acc: 0.9932885906040269, val_acc: 0.8313253012048193, train_loss: 0.03790939999455211, val_loss: 0.7337640299495444 (93 / 100)\n",
            "train_acc: 0.9932885906040269, val_acc: 0.8313253012048193, train_loss: 0.03827773125931283, val_loss: 0.7391263528850424 (94 / 100)\n",
            "train_acc: 0.9812080536912752, val_acc: 0.8313253012048193, train_loss: 0.0490526172910245, val_loss: 0.7119981409331043 (95 / 100)\n",
            "train_acc: 0.9758389261744966, val_acc: 0.8313253012048193, train_loss: 0.0639835816340821, val_loss: 0.6940807778253613 (96 / 100)\n",
            "train_acc: 0.9798657718120806, val_acc: 0.8313253012048193, train_loss: 0.06304473528773649, val_loss: 0.6839919799871473 (97 / 100)\n",
            "train_acc: 0.9825503355704698, val_acc: 0.8313253012048193, train_loss: 0.04892829870203535, val_loss: 0.6638847286251923 (98 / 100)\n",
            "train_acc: 0.9919463087248322, val_acc: 0.8313253012048193, train_loss: 0.03921023857586036, val_loss: 0.714991368802197 (99 / 100)\n",
            "train_acc: 0.9798657718120806, val_acc: 0.8192771084337349, train_loss: 0.04640796945595195, val_loss: 0.8067462068634579 (100 / 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwWAwBDn/C7AC2tAdhSwdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3hUVfrA8e+ZyaT3QjokNKkhkAAi\novQiiFIEXXWFVcq6IOiqILqIuD8FUVfWgoAgyiqgSFWkI4iiEIq0IBAIaaT3TMqU8/tjkiEhCQkh\noZ7P8/iYuXPm3jMJeefkvee8R0gpURRFUW59mhvdAUVRFKV+qICuKIpym1ABXVEU5TahArqiKMpt\nQgV0RVGU24TNjbqwt7e3DAkJuVGXVxRFuSUdPHgwXUrpU9VzNyygh4SEEBUVdaMuryiKcksSQlyo\n7jmVclEURblNqICuKIpym1ABXVEU5TZRYw5dCLEUGAKkSinbVfG8AOYDDwB6YIyU8lBdOmMwGEhI\nSKCoqKguL1eUBmNvb09QUBA6ne5Gd0VRqlWbm6LLgI+AL6t5fhDQovS/rsCC0v9ftYSEBFxcXAgJ\nCcHyOaEoN56UkoyMDBISEggNDb3R3VGUatWYcpFS7gEyr9DkIeBLafEb4C6E8K9LZ4qKivDy8lLB\nXLmpCCHw8vJSfzkqN736yKEHAvHlHieUHqsTFcyVm5H6d6ncCq7rPHQhxHhgPEDjxo2v56UVRVGu\nSlF0NHnbtlkf24aE4Prgg3X6cDfl5KB1c6vP7lWpPkboiUBwucdBpccqkVIuklJGSikjfXyqXOh0\nU1i3bh1CCE6dOnWju3LTmzVrFu+++y4AM2fOZPv27ZXa/PTTTwwZMuSK5zly5AibNm2yPt6wYQNz\n5syp387WwrJly0hKSrru172dmYuKkCUlN7obVZJSkv/zXgr2769wXH/4MBcef4L0TxaQvuBT0j9Z\nQNLL00j773+v7vwmE6nz53O6692kvDOPht5/oj5G6BuASUKIlVhuhuZIKS/Ww3lvmBUrVnDvvfey\nYsUK3njjjQa7jslkQqvVNtj5r7fZs2fX+bVHjhwhKiqKBx54AIChQ4cydOjQ+uparS1btox27doR\nEBBw3a99q5BGI2i1VxypFh47Ts7GDRQePkJRdDQaJyc8HnsUz8cfx+YqBnPGjAz0UQdx6dsHUe53\nRRoM5GzYQElsLIaLyZhycvD865M49+hRdZ+lRP/bb0iDAYcOHdC6uVH055+kzJmDft9vALg/Ohrf\nl16iOCaG+HHj0fp40/TL5eh8GyHNZpJff52MBZ8idDp8nn22xr6bcnJIfOklCvb8jF2LFmQuXYqw\nscHn+akNlsKrzbTFFUBPwFsIkQC8DugApJSfApuwTFk8i2Xa4tgG6el1kp+fz969e9m1axcPPvhg\nhYA+d+5c/ve//6HRaBg0aBBz5szh7NmzTJw4kbS0NLRaLd9++y3x8fG8++67fP/99wBMmjSJyMhI\nxowZQ0hICKNHj2bbtm28/PLL5OXlsWjRIkpKSmjevDnLly/H0dGRlJQUJk6cyLlz5wBYsGABmzdv\nxtPTk6lTpwLw6quv0qhRI6ZMmVLhPbz//vssXboUgGeeeYapU6cSGxvLoEGDuPfee/n1118JDAxk\n/fr1ODg4WF+Xk5NDWFgY58+fR6PRUFBQQKtWrTh37hzLli2rsp/ljRkzhiFDhjBy5Eg2b97M1KlT\ncXR05N5777W22b9/P1OmTKGoqAgHBwc+//xzQkNDmTlzJoWFhezdu5dXXnmFwsJCoqKi+Oijj4iN\njeVvf/sb6enp+Pj48Pnnn9O4cWPGjBmDq6srUVFRJCcn88477zBy5MgKfSooKGDUqFEkJCRgMpn4\n17/+xejRozl48CAvvPAC+fn5eHt7s2zZMn755ReioqJ4/PHHcXBwYN++fRW+P7eC4pgYzIVFOLRr\nW+/nLomNJeOLL8hZsxaNszMOHcNx7NgRt4cfxsbLy9rOkJJC3JgxSJMJh/bt8Ro71vLahYvIXLIU\nh8gIhNYSerSuLjh06IBDx47Yt2qFsLUFwFxSQtaXX5L+6ULM+fm4DBpIwNy5aGxtMRcUkDD1eQp+\n/hl0OnS+vkiTifjxE/CZMgWvCeMrBExjVhbJb8wmb/Nm6zHbkBBK4uLQurjgO2MGhuRkMj//nIJf\nfrWkR9zdabJsGTrfRgAIjQa/N95AGoyk//dDkBLvCRMQNhVDqCk3l8I//qDw8GFyNmzEkJKC3+sz\ncR89muRZb5CxaJHlA2HypHr/+UAtArqU8rEanpfAP+qtR6Xm7p/Lqcz6TXm08mzFtC7Trthm/fr1\nDBw4kJYtW+Ll5cXBgweJiIjgxx9/ZP369fz+++84OjqSmWmZ+PP4448zffp0hg0bRlFREWazmfj4\n+Ctew8vLi0OHLFP1MzIyGDduHACvvfYaS5YsYfLkyTz33HPcf//9rF27FpPJRH5+PgEBAQwfPpyp\nU6diNptZuXIl+y/7U/HgwYN8/vnn/P7770gp6dq1K/fffz8eHh6cOXOGFStWsHjxYkaNGsV3333H\nE088YX2tm5sb4eHh7N69m169evH9998zYMAAdDodw4cPr7KfVSkqKmLcuHHs3LmT5s2bM3r06Es/\ng1at+Pnnn7GxsWH79u3MmDGD7777jtmzZ1sDOFhGymUmT57MU089xVNPPcXSpUt57rnnWLduHQAX\nL15k7969nDp1iqFDh1YK6Js3byYgIIAffvgBsHxoGQwGJk+ezPr16/Hx8WHVqlW8+uqrLF26lI8+\n+oh3332XyMjIK/4MbxQpJfk//UTa++9jKijAtf8AXB8YhFlfSMbnSynYvQd0OhovXoTT3XfXeD5j\nVhb63/djSEzEkJyMOTcHxy5dcenXF62rK+bCQvJ37yFnwwbyd+1C2NjgOngwSIn+yGHyt+8gZ+P3\nhKz4Gk3ph1/Kv/8PaTTSdOMGbMvdKyuJjSXzyy8pPHHCeqz4XAy5m360PNBosGnUCJ2fH8bUVAxJ\nSTj37Il9m9akf7KA+Kxs/N+cTeI/X6To+HH8Zr+B+8iRCI0Gs17PxX/NJO2DDyg6cRzXIQ8CYMrK\nIu3DDzHl5uIzdSoO4eEUHjlM4ZE/cO7dG+/x49C6uwPg0qc3SdNfQeviQpMvlqHzrzhZT2g0+P/f\nv5FmE+kffkT2d9/h+de/4tKnDwV795K76Uf0UVEgJWg02LdpQ8A77+DYqSMAfrNeRxqNpH/8MVov\nTzz/8pe6/0Ooxg0rznWzWrFihXXE++ijj7JixQoiIiLYvn07Y8eOtY5KPT09ycvLIzExkWHDhgGW\nxSe1UT7AHT9+nNdee43s7Gzy8/MZMGAAADt37uTLLy1T/7VaLW5ubri5ueHl5cXhw4dJSUmhY8eO\neJUbGQHs3buXYcOG4eTkBMDw4cP5+eefGTp0KKGhoYSHhwMQERFBbGxslX1btWoVvXr1YuXKlTxb\n+qdldf2syqlTpwgNDaVFixYAPPHEEyxatAiwBNSnnnqKM2fOIITAYDDU+P3at28fa9asAeDJJ5/k\n5Zdftj738MMPo9FoaNOmDSkpKZVe2759e/75z38ybdo0hgwZQo8ePTh+/DjHjx+nX79+gCX15e9f\np5m21400GCg8doz0jz6i4Nd92IaGYt/yLjK/+orM0g8/racn3pMmkbdlMwmTJtPkq6+wv6slAKa8\nPAqPHgWzJYdrTE0hd/MWCvbtA6MRAI2jI8LBgZz1G0ieNQv7DmEUnYxG6vVovbzwmjC+Usokf88e\n4idM5OKrrxHw3rvk79xJ3rZt+LzwQoVgDpZRsd/MmZXemyElxZKaORWN8WIyhuRkdEFB+L05G+fu\n3a2vTZrxKjEDBiJ0OoI++hCX3r2t59A4OhLw7jzs27cjdd675G27dC/HrlUrGi9dgv1ddwHgdHfV\ny2QcIyJotukHpMmEpprfZaHVEjBnDq6DBpG5ZCmpc+aSOmeupY9Nm+L994k4dumKQ/t2aEp/B62v\n1Wjwf3M2Nj4+uPTpW+X5r9VNG9BrGkk3hMzMTHbu3MmxY8cQQmAymRBCMG/evKs6j42NDWaz2fr4\n8vnLTuV+0GPGjGHdunV06NCBZcuW8dNPP13x3M888wzLli0jOTmZv/3tb1fVLzs7O+vXWq2WwsLC\nSm2GDh3KjBkzyMzM5ODBg/Qu/aW52n5W51//+he9evVi7dq1xMbG0rNnzzqdp0z591TVDaeWLVty\n6NAhNm3axGuvvUafPn0YNmwYbdu2Zd++fdd07esha9U35G7cSOHx48iiIjRubvi++ioej45G6HSY\ncnLI27kLANcHBqGxs8N9xHBiRz9K/PjxBM3/gNwtW8n+5hvMBQUVzq0LCMBrzFO49OuHbWgoGhcX\nAIqOHSN3048U/PYbboMH4/rAIBw7d66UXgBwvu8+fJ5/nrT338c2JITsNWuwa9kSr7Fjav0edb6+\n6AYOwHVg9YMEt6FD0bq7k/7xJzR6+SUcIyIqtRFC4DVmDK6DHsCUnW05phHYhoZW2feqCJ0OUcNq\nYKHR4NKrFy69elF49Cj6qIM4db8Hu5Yta8yNC62WRs9PrVVf6uKmDeg3wurVq3nyySdZuHCh9dj9\n99/Pzz//TL9+/Zg9ezaPP/64NeXi6elJUFAQ69at4+GHH6a4uBiTyUSTJk04efIkxcXFFBYWsmPH\njgp55PLy8vLw9/fHYDDw1VdfERhomcLfp08fFixYwNSpU60pFzc3N4YNG8bMmTMxGAx8/fXXlc7X\no0cPxowZw/Tp05FSsnbtWpYvX17r74GzszOdO3dmypQpDBkyxHrTtrp+VqVVq1bExsYSExNDs2bN\nWLFihfW5nJwc62vLp1VcXFzIy8ur8nz33HMPK1eu5Mknn+Srr76iRzU3vqqSlJSEp6cnTzzxBO7u\n7nz22WdMnz6dtLQ09u3bR7du3TAYDJw+fZq2bdtesR/XW+GRIyS//jp2LVrgMXoUDh074nTPPWhd\nXa1ttG5uuA97uMLrdP7+BC9exIW/PE7so4+BVovroEG4Dx+GKE2LaBydsGvZosoA5BAWhkNYWK37\n6TXuGYpOniT9k09ACII++E+NQbEunO+7D+f77quxnc63kTX33dCu9nvV0FRAL2fFihVMm1bxL4MR\nI0awYsUKFixYwJEjR4iMjMTW1pYHHniAt956i+XLlzNhwgRmzpyJTqfj22+/pWnTpowaNYp27doR\nGhpKx44dq73mm2++SdeuXfHx8aFr167WYDJ//nzGjx/PkiVL0Gq1LFiwgG7dumFra0uvXr1wd3ev\ncoZMp06dGDNmDF26dAEsI/qOHTtWmV6pzujRo3nkkUcqjMKr62dV7O3tWbRoEYMHD8bR0ZEePXpY\n27/88ss89dRT/Pvf/2bw4MHW1/Tq1Ys5c+YQHh7OK6+8UuF8H374IWPHjmXevHnWm6K1dezYMV56\n6SU0Gg06nY4FCxZga2vL6tWree6558jJycFoNDJ16lTatm3LmDFjmDhx4g2/KSqlJOWdeWi9vQlZ\nuaLSn+81sb/rLoIXLyb/5z14jByJ7gofwNdKCEHA//2b+MxMHCI64VCa1lOuP9HQ8yKrExkZKS/f\n4CI6OprWrVvfkP7cKsxmM506deLbb7+15qiV6+N6/vvM3baNxMnP4TdrFh6Pjq75BcodQwhxUEpZ\n5V17VT73FnLy5EmaN29Onz59VDC/jUmDgbR338O2WTPcR4640d1RbiEq5XILadOmjXVeunL7yvrm\nG0ouXCDok09qfTNPUUCN0BXlpmJMTyf9o49x7NwZ5149b3R3lFuMCuiKcpOQZjNJL0/DrNfjN/Nf\nqsKjctVUQFeUm0TGkiUU/PorvjNmYKfukSh1oAK6otwE9IcPk/bBfFwGDsR91CM3ujvKLUoF9Crc\nyeVz161bx8mTJ6/6dbUpd5uUlFSp1sr1kJ2dzSeffHLdr3s5aTKRvWYtZr2+wnFTdjZJ/3wRnZ8f\n/rPfUKkWpc5UQK9C+fK5DclkMjXo+eviSgHdWFrzoypDhw5l+vTpVzx3QEAAq1evvqb+1cXNEtAL\nfv2VizNmkDB1qqUELZZStIkv/BNjWhqB779XYRWoolwtFdAvU1Y+d8mSJaxcubLCc3PnzqV9+/Z0\n6NDBGrzOnj1L37596dChA506dSImJqbShg6TJk2yLnMPCQlh2rRp1sVBixcvpnPnznTo0IERI0ag\nLx29paSkMGzYMDp06ECHDh349ddfmTlzJh988IH1vK+++irz58+v9B7ef/992rVrR7t27aztY2Nj\nad26NePGjaNt27b079+/Ui2XX3/9lQ0bNvDSSy8RHh5OTEwMPXv2ZOrUqURGRjJ//nw2btxI165d\n6dixI3379rUWxFq2bBmTJllKgo4ZM4bnnnuOe+65h6ZNm1qDeGxsLO3atbO2Hz58OAMHDqRFixYV\nCm4tWbKEli1b0qVLF8aNG2c9b3m7d+8mPDyc8PBwOnbsaF2JOm/ePDp37kxYWBivv/46ANOnTycm\nJobw8HBeeuml6n/4DazwyB8AFOz5meQ33kBKSep//kPBr7/i9/pMHDp0uGF9U24PN+0k1+S33qI4\nun5THnatW+E3Y8YV29zJ5XPvuecehg4daq1pXqakpISyVb1ZWVn89ttvCCH47LPPeOedd3jvvfcq\nvceaytqCZVOLw4cPY2dnx1133cXkyZPRarW8+eabHDp0CBcXF3r37k2HKgLdu+++y8cff0z37t3J\nz8/H3t6erVu3cubMGfbv34+UkqFDh7Jnzx7mzJnD8ePHOXLkyBV/Lg2t8OhR7Fq2xLlPbzIWfIox\nLZ38n37C4y+P4X4DUlHK7eemDeg3yp1ePrem/iYkJDB69GguXrxISUkJoaGhVb6mprK2YClA5la6\nz2KbNm24cOEC6enp3H///Xh6egLwyCOPcPr06Uqv7d69Oy+88AKPP/44w4cPJygoiK1bt7J161Zr\n7Zz8/HzOnDlzU+xfK6Wk6OhRXPr3w+e55zAmXSRn/XocIiLwrSFVpSi1ddMG9JpG0g1Blc+tWvn+\nTp48mRdeeIGhQ4fy008/MWvWrBqvVV29oMv7c6Uc/eWmT5/O4MGD2bRpE927d2fLli1IKXnllVeY\nMGFChbZXU5isoRguXMCUk4N9WBhCCPzfnI1Dp0649O9n3aVHUa5VrXLoQoiBQog/hRBnhRCVhhNC\niCZCiB1CiKNCiJ+EEEH139WGV1Y+98KFC8TGxhIfH09oaKi1fO7nn39uzXFnZmbi4uJiLZ8LUFxc\njF6vr1A+Nzs7mx07dlR7zcvL0pYpK58LlpunOTk5AAwbNozNmzdz4MCBKjeZ6NGjB+vWrUOv11NQ\nUMDatWuvqtxsTeVjy5e//eKLL2p93trq3Lkzu3fvJisrC6PRyHfffVdlu5iYGNq3b8+0adPo3Lkz\np06dYsCAASxdupT8/HwAEhMTSU1NvSlK4hYePQqAQ5glfSRsbfEYPQobD48b2S3lNlNjQBdCaIGP\ngUFAG+AxIUSby5q9C3wppQwDZgNv13dHr4cVK1ZY0ydlysrnDhw4kKFDhxIZGUl4eLh1p/vly5fz\n3//+l7CwMO655x6Sk5MJDg62ls8dNWpUrcrndu/enVatWlmPz58/n127dtG+fXsiIiKsM0/KyueO\nGjWqxvK5Xbt2tZbPra1HH32UefPm0bFjR2JiYio9P2vWLB555BEiIiLw9vau9XlrKzAwkBkzZtCl\nSxe6d+9OSEiINS1T3gcffEC7du0ICwtDp9MxaNAg+vfvz1/+8he6detG+/btGTlyJHl5eXh5edG9\ne3fatWt3w26KFv5xFI2jI3bNm92Q6yt3hhrL5wohugGzpJQDSh+/AiClfLtcmxPAQCllvLBMos2R\nUl5x/pUqn1s3d0L53Pz8fJydnTEajQwbNoy//e1vlT5ob4Rr+fd5/pFRaBwcaPJl/f9Vo9xZrrV8\nbiBQftpGQumx8v4Ahpd+PQxwEUJ4XdYGIcR4IUSUECIqLS2tFpdWyrtTyufOmjWL8PBw6wYhDz/8\ncM0vuk4MycnoDx68qteYi4spOnUKhw43z842yu2pvm6Kvgh8JIQYA+wBEoFKq2aklIuARWAZodfT\nte8Yd0r53LJ01s0o9f33yf3+B4I+/C8uffrU6jXF0dFgMGB/E21VptyeajNCTwSCyz0OKj1mJaVM\nklIOl1J2BF4tPZZdlw7dqB2UFOVKyv5dFh48BGYzif98kcJazmu//IaoojSU2gT0A0ALIUSoEMIW\neBTYUL6BEMJbCFF2rleApXXpjL29PRkZGSqoKzcVKSUZGRnYCoEhMRGv8eOxadSI+L8/S0kVUyKl\nlBTHxFz6EPjjKDZ+ftdt42LlzlVjykVKaRRCTAK2AFpgqZTyhBBiNhAlpdwA9ATeFkJILCmXf9Sl\nM0FBQSQkJKDy68rNxt7eHo+4eFIAlz69cR8xnNhHHyN+4t9p+v3GCjsLZa1YQcrsN3EZOBC/12dS\nePToTbUzvHL7qlUOXUq5Cdh02bGZ5b5eDVxz1SWdTlftykNFudFSVn2DsLXFvnVrhK0tfm/MIvG5\nKeTv3m3Np0spyV71DVpvb/J27EC/fz+mzEy10bNyXajiXIpSS4WHD2Pfvr11ZadL797YNGpE1qpV\n1jZFJ09S/OefeD/7d0JXf4tNI0uaxTEi4ob0WbmzqICuKLVgLi6m8ORJHDuGW48JGxvcR46g4Oe9\nlCRY5gnkfLcGYWuL2+DB2N91F6HfrCLk229xCA+v7tSKUm9UQFeUWig6cRIMBhwuW3VbViUx+7vV\nmIuKyPn+e1z69UNburpV2Nri0L7dde+vcme6aYtzKcqNlLNxI+bCQjxGjQIs6Rag0khbFxCA0309\nyFn9HbZNmmDOzcV95Ijr3l9FATVCV5RKpMlEypy5JL8xm6LSGjqFRw6ja9IYG69KC6DxGD0aY1oa\nqW/PQRcQgGPXrte7y4oCqICuKJUU/vEHpowMkJKLM19HGo3oDx/BMbzqImfO992Hja8vppwc3IYP\nR2jUr5VyY6h/eYpymbxt20Gnw2/W6xQdP07K3Hcwpafj0LHqG5vCxgb30aPAxga3m6jujHLnUTl0\nRSlHSknejh043X037o88Qt727WQtXw5Q6YZoed7jx+M2ZAi2QZfXrVOU60eN0BWlnOLTZzDExeHS\npw9CCPxmvo5wcEDj5IRd8+bVvk7Y2GB7DVvdfR39NckFyXV+vaKAGqErSgV527eBELj06Q2AbVAg\nAW+/jTEzA1HFhiL1Ib0wnbf3v01UShTv93y/Qa6h3BlUQFeUcvJ27MAhPBwbHx/rMdeBlbf6q0+5\nxbkAbLuwjTNZZ2jhcfvWulcalgroilKqJCGR4pPRNHrpxet63dySXOvXi44uYt79NW9KfjLjJLYa\nW5p7VJ8GKi+9MJ3N5zdjlpbNy70cvBgUOgiNaJis65HUIzjqHGnp0bLC8XPZ59ibuNf6uJl7M7oH\ndq/QJqsoi03nN2EyV9pSAUedIw83fxgbzbWFroMpB3G3c6eZe/VbAl7IvcDu+N3XdJ0y9jb2PBD6\nAM62zvVyvuqogK4opfJ3bAfApW/f63rdsoDexa8LW2K38Pfwv9PUrWm17fck7GHKril42Xuxafgm\nbLW2NV5j7v65bI7dXOHY4dTDvNr1VSy7RtafHRd28M/d/8RWa8unfT+lk28nAI6lHWP8tvHkG/Kt\nbW2EDT8M/4EA5wDrsf8c/A9rz6694jVGthxZ5/79eP5Hpv88HQcbBxb3W0x7n/aV2vyZ+SdPb32a\nnOKcOl/nchtiNrCw30KcdE71ds7LqYCuKKVyt23DrkULbJs0ub7XLQ3okzpOYsK2CSw+upi3e1S9\nz/ovib/w/K7n8XHw4WLBRdbHrOeRlo9c8fznss+xJXYLY9uOZVzYOAAWH1vM58c/Ryu0TO8yvd6C\n+q64Xby4+0XaercltziXv2//O4v6L0Kn0TFh+wTc7NxYOWQlnvaepBemM2LDCJYcW8K/uv0LgIS8\nBDbGbGT0XaOZ0mlKpfNP2DaBz459xkPNH0Kn0V11/7bGbuWVn18h3CecVH2q5fs9YDFtvdpa25zJ\nOsO4reOw19rzxUNf0Mjx2uvY/5r0K9P2TOPZ7c+yoO8CHHWO13zOqqiArtxxcjZuxL5tO+yaXirV\nXHz+PIVRB/F5/vnr3p+yHHoT1yaMvms0X578kokdJtLEteIHy28Xf2PKrik0dW/KZ/0/49ntz7Lk\n2BIebv6wNbhlFWWRU5xDiFuI9XWLjy3G3saese3G4mLrAsDznZ7HZDbx5ckvMZqN3BN4T6V+2Wvt\n6erftVJ640TGiSpn5KTp05h7YC6tvVrzad9PKTQWMnbzWCZum4hWo8VF58LSAUuto3EXWxeGNR/G\n2rNrGRc2Dj8nP5YcX4IQgnHtx1n7Wt7EDhP5x45/8MO5H3i4edVz/o+nHydFn1LpeHJBMu8eeJcw\nnzAW9F1ATnEOY7eMZfzW8UzvMh1HnSPFxmLmHpiLTqNj6YClNHat+8yl8gaEDEAimbZnGpN2TuLj\nPh/jYONQL+cuTwV05Y5S9Odpkl56GYeICJr8b7l1ZJr9zbdgY4P78GHXvU9lI3QXWxeeavsUq/5c\nxUu7X2Jx/8W42VmKfB1IPsDkHZMJdglmUb9FuNm5MaHDBP6x4x98H/M9w1oM42L+RcZuGUuaPo0P\ne3/IPYH3cCH3ApvOb+Kvbf6Kh72H9ZpCCF6MfBGTNPFV9Fd8c/qbKvvWr0k/3rnvHWtQ//b0t8ze\nN7va99LGqw2f9vsUF1sXXGxdWDJgCWM3j8UojSwZsKRCagXg6fZPs+bMGj4//jlj2o5h3dl1jGgx\nAl8n3yrP3yOwB609W7P46GKGNB1S6cPmq+ivmLN/TrX9C/MJ45M+n+Coc8RR52jt34y9M6xtvB28\n+WzAZ/UWzMsMDBmIyWxixt4ZfB39NU+3f7pezw8qoCt3mIwlnwFQePAg+v0HcOraBXNxMTlr11rq\nm5eb3XK95JXk4WDjgE6jw9vBm/fuf48pu6YwYdsEFvVfxJmsM/xjxz8IdA7ks/6fWQOzNbgdW0xX\n/648s/UZcotzaezamOd2PdsULMoAACAASURBVMfHfT5mY8xGdBodT7V9qtJ1hRBM6zyN0XeNpthU\nXOn5vYl7mX9oPjN+nsFbPd5iY8xGZu+bTY/AHkzuOLnKNE0zt2botJdSIX5Ofqx5aA1SyirTDAHO\nAQxtPpTVp1eTVmjZqezpdtUHOiEEEzpMYOquqfx4/kcebPag9blVp1YxZ/8c+jTuw4SwCZX6JxA0\ndW9aIVUT6BzIuofWEZcXZz0W5BzUYDcvBzcdjJ+THx18GmZ/2VoFdCHEQGA+li3oPpNSzrns+cbA\nF4B7aZvppbscKcpNw5CYSO4Pm3B/7FHyt+8gfcECnLp2IW/bdkzZ2Zbl+zdAbkkurrau1sc9gnrw\nn57/YepPU3l6y9PE5cbh6+jLZwM+w8vhUnEwIQQTO0xkyq4pjNhgqfC4qN8iglyCeHrr00zaMQmD\n2cBjrR7D28G7ymsLIQh1q3qXsFaerdAKLe8ffJ9kfTJHUo9wT8A9/KfXf7DT2tX6/dWUWnim3TOs\nP7uebRe2MaLFCPyd/a/YvldwL1p6tGTR0UXWWTRRKVHM2T+HnkE9mXffvAofKjVx1DnSyrNVrdtf\nqwjfhtvspMY5S0IILfAxMAhoAzwmhGhzWbPXgG+klB2xbCL9SX13VFGuVcayL0AIvMePx+uZp9H/\n9hv6Q4fIXrUKXXAwTt263ZB+5Rbn4mrnWuHY/cH38+7973I26yw+jj4sGbCkyqDcK7gXrTxbYZZm\nFvRdQHuf9njYe7C432ICnQOx0dgwtt3YOvdtbLuxTOk0hcOph+ni34X5veZfVTCvjWDXYAY3HYyN\nsKlVGkIjNEwIm0BsbiwjN45k5MaRzNk/hx6BPXiv53tXFcxvN6JsZ/JqGwjRDZglpRxQ+vgVACnl\n2+XaLATOSSnnlrZ/T0pZ+S5LOZGRkTIqKupa+68otWLMyuJs7z649u9PwNw5mAsLOdu3Hzbe3hT/\n+Sc+L7yA9/hxN6RvYzePxSzNfDHoi0rPxWTH4O3gbc2lVyWjMIMSU0mlkW1+ST7phekVbpDW1Yn0\nEzT3aF7vwbyM3qAnIT+h0rz16kgp2Ze0D71RD4Ct1pZu/t3uiGAuhDgopYys6rnapFwCgfhyjxOA\nyws+zwK2CiEmA05AlRN5hRDjgfEAja+h7oWiXK2sr79GFhbi9YxlBKhxcMDrb2NJnfdulTdDpZT1\nPj+7OrkluZVuFpa50sKXMuXTMOU52zrXWy64rXfbmhtdg6oWIV2JEKLKmTl3uvpaJvYYsExKGQQ8\nACwXovISNCnlIillpJQy0ucG3HxS7kzmoiKy/vcVzj17Ytfi0rJ6j0cfRevpiWv//th4V0xnTN01\nlRd3X3nF6JozaxiydghGs/Ga+pdXklchh64odVWbEXoiEFzucVDpsfKeBgYCSCn3CSHsAW8gtT46\nqSjXIveHTZiysvAcM6bCcY2TE6Hr1qJ1qrhy70jqEXbG78Rea4/BZKj2z/iTGSe5kHuBU5mnaOdd\n931DL78pqih1VZsR+gGghRAiVAhhi+Wm54bL2sQBfQCEEK0BeyCtPjuqKLVhyi+o8FhKSdZXX2HX\nojmOXbtUaq9r1AjNZQF94dGFABSZijiRcaLaa2UWZQKWuiB1ZTQbKTAUqICu1IsaA7qU0ghMArYA\n0Vhms5wQQswWQgwtbfZPYJwQ4g9gBTBG1nS3VVHqiZSSgn37iBs3ntORkWSvuVQHpOiPPyg6eZJ1\nYSVsj9te47mOpx9nb+JenmpjmbcdlVL9jfusoixLm+Ta3dxPLkhm1MZRxOdduiWVV5IHUGmWi6LU\nRa1y6FLKTVLKllLKZlLK/ys9NlNKuaH065NSyu5Syg5SynAp5daG7LSilDGmpxM78hHixv6NopMn\nsW3ejJT/+z8MiZasYObXX2N2tGdFk0Tei3oPg9lwxfMtPLoQNzs3/h7+d5q7N79isC4L6AdTD1ZZ\nGfByexL2EJ0ZzdG0o9Zj1oCuRuhKPVA7Fim3tKyVqyg6eRK/WbNovnMHwZ9+ClKS9OprGNPTyftx\nM6fvDqTEXktifiKbzlW/3i06I5qf4n/iidZP4KRzIsI3gsOph6u96ZlVnIWLrQt5JXmcyT5TY1/L\nPhzSC9Otx8qW/auArtQHFdCVW5Y0m8lZswanbt3weHQ0Gjs7bIOCaDR9GvrffiNu/HikwcDy1mkM\nDh1MK89WLD62uNrR9JLjS3DRufCX1n8BINIvEr1RT3RGdKW2ZmkmuzibXsG9gIpplz8z/2Tqrqno\nDfpLfZXSmr5J01+6vVRWmKuqQlSKcrVUQFduWfrff8eQlITbiOEVjrs/8ghOPXpQfDKakk6tOeOq\np39IfyaETeBC7oVKdcHLnEg/wb2B91pHy5G+lrUbVeXRc4pzMEszbbzaEOgcWKHN+wffZ0fcDnbF\n77Iei8uLs9YqKfs/qBG6Ur9UQFduWdnfrUHj6lppQwohBP7/fhOHiAj29PfDSedEt4Bu9G7cm+bu\nzVl0dFGVo/Ss4qwKi3S8HbwJcQ2pMqCX5c897DyI8I3gYMpBzNLM0bSj/Jr0K2DZUq5M2Qi+rA54\nGWtAVzdFlXqgArpySzLl5JC3dStuQ4agsau8HF3n60vQ8mV8Y3+M+4Pux05rZ6kB0mEC53LOVZrx\nUmIqocBQUKHELFgKKR1KOVTpA6BsyqKHvQeRvpFkF2cTkx3DwqMLcbdz56FmD7E3ca817RKVEoWX\nvRcRvhGk6i8tz1AjdKU+qYCu3JJyfvgBWVJSKd1SXlRKFNnF2fRv0t96rF/jfrjoXCrNXikfoMuL\n9Isk35DPn1l/VjieVWwZoXvaexLpZ0nNfHnyS/Yk7OHJNk/yUPOHKDYV83Piz9b8eYRvBD4OPpVG\n6LYaW+xt7OvwXVCUilRAV25JOd+twa5VK+zbXF7485JtsdtwsHGosAmxVqPF29GbjKKMCm2zi7MB\n8LTzrHC8LI9++eIha8rF3oMg5yB8HX1Zd3YdLrYuPNbqMTo16oSnvSfbLmwjMT+R5IJkS0B39CHf\nkE+hsRCw3BRVN0SV+qICunLLyd+9m6ITJ3AfMaLaAloms4ntcdu5L+i+SqNfL3svMgorBvTqRuh+\nTn4EOQdVP6K380AIYR2lP9n6SVxsXdBqtPRt3Jc9CXusOfVIv0h8HCw1jNL1llF6bknl0rmKUlcq\noCu3DHNhIclvvUX8hInYhoTgNvTBattui9tGZlEmg0IGVXrO097TGpDLlB9xXy7MJ4zozOhK7V10\nLtY6LwNDBtLcvbl1yiNA3yZ9KTQWWhcrNXdvbg3oZTNdVGEupT6pgK7cEgyJiZwfNpysL5fj8cQT\nhK75Dq1b1TXCzdLMwj8W0tStKT2De1Z63svBq1LKpSyge9p7Vmof6BxIqj61wgKjrKKsCsG/Z3BP\n1j60tkLd8ki/SNzt3EnVpxLRKAKN0ODtaKnqWBbQVWEupT6pgK7cErK++ZaS+Hgaf74Uv9deReNY\neX/KMjvjdnI2+yzjw8aj1WgrPe9l70VeSR4lphLrscyiTLRCW2U+O9A5EJM0VZidklmcibu9+xX7\nrNPo6N24N4A1JWMdoZcuLlI5dKU+qYCu3BL0B6Owb9u2xm3ipJQsPLqQENcQBoYMrLJN2Vzz8mmX\nrOIs3O3c0VQu42/dCSgx/1LV6KyirEo3UKsytNlQbDW21huz7nbu2Ghs1AhdaRAqoCs3PXNJCUVH\nj+EYUfPmursTdnMq8xTjwsZVOToHywgdqHBj9PIUSnmBzoEAXCy4WKv25UX4RvD747/T1K0pYFn0\n5O3gTXphOmZptuTQ1U1RpZ6ogK7c9IqOHUOWlJDf5srbFkop+fSPTwlyDuKB0AeqbVc2Qi+fR88q\nyqoyfw6WmS5waYQupSSruHYBHcBGU3EfGR8HH9L0aRQYCpBINUJX6o0K6MpNTx9lmQM+JnFOhUU5\nl/sl6RdOZJzgmfbPVAqi5VkDerkRemZRZrUB2k5rh4+DDxfzLSP0fEM+RrOx2g+Amvg4+JBWmKZW\niSr1TgV05aanPxhFfqAH2Q4mYnNiq2wjpWTBHwsIcApgaLOhVbYpUxaIK4zQi7PwsKt+xO3v7E9S\nfpKl7RWmONaGj6NltWhZpUUV0JX6ogK6clOTJhOFhw6T1MwyHbD8bj/l/XbxN46mHeXp9k9Xuwdo\nGQcbBxxtHK0jdKPZSE5xzhVH3IFOgSQVWAJ6+UVFdeHt4E12cbb1rw2VQ1fqS60CuhBioBDiTyHE\nWSHE9Cqe/48Q4kjpf6eFENn131XlTlR8+jTm/HzONLEE6bi8uCrbffrHp/g6+vJw84drdV4vh0ur\nRcuW/V9pGqK/sz8XCy5iluYrzlmvjbKpi+dzzgNqhK7Un+oTjaWEEFrgY6AfkAAcEEJskFKeLGsj\npXy+XPvJQMcG6KtyByrLnx8JKAYgLrdyQD+QfIBDqYd4pcsr2Gpta3VeL3sv60i7NimUAKcAjGYj\nafo0a2Gua0m5AJzLOQeozS2U+lObEXoX4KyU8pyUsgRYCTx0hfaPYdkoWlGumf7gQWz8/flTZwm+\nVaVcFv6xEB8HH0a0HFHr85ZfLWodcV9hXnmAcwBgmbpYXd2X2vJ2sKwWLQvoaoSu1JfaBPRAoPxv\nUULpsUqEEE2AUGBnNc+PF0JECSGi0tLSqmqiKFZSSvQHo9B1DKPIVIROoyMuLw4ppbVNdEY0vyf/\nzpi2Y7DTVq6LXp3yBboyi2sO0GUBPTE/kayiLBxsHHCwcajL27KmXGKyY9AKLU46pzqdR1EuV983\nRR8FVkspq9y0UUq5SEoZKaWM9PHxqedLK7eD3K1bSXhuCjnr11N08iSmtHSK2lkW5bT3bk+BoaDC\nCs9DqYcAGBha9arQ6ng5eJFdnI3RbKxVysXfybJa9GLBRcuiojreEAVL7l0jNOSWWJb9V1cxUlGu\nVo05dCARCC73OKj0WFUeBf5xrZ1S7kyGxEQuvjIDc0kJeVu3Wo9n3NUIzlrqoRxKPUR8Xrx1LvnJ\njJN42XtZR7215WXvhUSSVZRlDejudtXfFHXUOeJh50FifiKZxdXPWa8NrUaLl70XaYVpKt2i1Kva\njNAPAC2EEKFCCFssQXvD5Y2EEK0AD2Bf/XZRuR0Z09Ioibt0g1OazSS9+hpISbMfN9Fkxdd4/PVJ\n3IYNI8nLMoLt7NcZqJhHj86MprVX66se5Xo6XJqLnlmUiZud2xUXI4El7XIx/2Ktl/1fSVkeXd0Q\nVepTjQFdSmkEJgFbgGjgGynlCSHEbCFE+RUcjwIrZfkEp6JUQUpJ/N+fJeaBwaR9/DHSYCBrxQr0\nv/1Go+nTsA0KwrFjR/xmzCDg7bdIKUxDIzR08OmARmisUxeLjEWcyz5HG6/qdy2qTvl6LrVNoQQ4\nB5BUkHTFMgG1VTbTRY3QlfpUm5QLUspNwKbLjs287PGs+uuWcjsr+uMPio4fx65lS9I//Ij8HTsp\nPn8epx49cH/kkUrtU/QpeNt742DjgL+Tv3Xq4ums05ikiTaedQjo5eq5ZBXXLkAHOAWwJ2EPAnFN\nOXS4dGNULSpS6pNaKapcd5lffY3J0Z79s4YR+MEHGC5eROh0+L85u8rUSUpBCr5OvgA0dmlsTblE\nZ1h2EWrt1fqq+1A2Qs8szKx1CsXf2Z9iUzFFpqIaa6HXpCzlokboSn1SAV25rozp6eRt3szhCFc+\ni/kfrgMH0GzzjzRdvw6dn1+Vr0nRp+DrWBrQXRtbUy7RmdG427lbZ6BcDSedE3ZaO2sOvTYBvayM\nLtR9lWiZshG6yqEr9UkFdOW6yl69GmkwsLWTlhR9Cvkl+Wjd3ND5Vx+UU/SXRujBLsHkFOeQU5zD\nyYyTtPa8+huiYKlLXjbTJKc4p1YplPIfHNecclE5dKUBqICuXDfSaCRr5Socu3XjuJNlqmDZasnq\n5JfkU2AouDRCd7HURI/JjuFM9pk6pVvKeDl4EZsTi0maapdDL11cBHVfJVpG5dCVhqACunLd5O3c\niTE5GZtHHsQoLRsux2THXPE1Zft4lk+5AOyK34XRbLy2gG7vZf1AqU2AdrF1saZIrjXl0sy9GXf7\n302nRp2u6TyKUp4K6Mp1IUtKyFi0GJsAf7IjmlmP1zRCT9YnA1hTLmV57C2xWwBo69m2zn3ydPCk\n0FgI1H7EHeAUcFXtq+Ooc2Rx/8U0c29Wc2NFqSUV0JXrIvn/3qLo+HF8X55GcrGljo9Oo6sxoKcU\npACXRuj2Nvb4OvpyseAiLjoXglyC6tynspkuUPsRd4BzADYaG5x1znW+rqI0FBXQlQaXteobslet\nwmvcOFwHDrAG6U6NOtWYcknRW9o2cmxkPVaWdmnl1eqa6qCUzUWH2t/k7Orfla7+XVX9FeWmpAK6\n0qD0hw6T/O9/49SjBz5TpwCWIG2jsSHCL4Kk/CT0Bn21r0/Rp+Bp71mhznnZjdG6LCgqr/wIvbYp\nlMdbP86nfT+9pusqSkNRAV1pMMVnz5IwaRI6f38C352H0GqBS/PKm7s3RyKJzY2t9hwpBZfmoJcJ\ndrHUiruWG6JwaYTurHOu9cYYinIzUwFduWbSaCTptdfIWLIUc0kJAMXnznNhzFjQaghe+ClaNzdr\n+7Ig3czNckPwSmmX8nPQy4T5hGGrsSW8Ufg19btshH6tNzgV5WZRq1ouinIl2au/I2f1dwBkrVyJ\n19NPk/7JJyAlTb74ArvQ0ArtU/QptPNqR7BrMDbC5oo3RlP0KXRsVHFHw85+nfnlsV+wt7G/pn6X\njdBVQFduF2qErlwTU34BaR99hEOnTgQv+QyNvT3Js2YhDQYaf74Uu2YVp+VJKa21WXQaHU1cm1Q7\nQi80FpJTnIOfU+WSANcazMEyr9xG2Fxx6zlFuZWoEbpyTTKXLsWUno7vRx/iEB6O09o15G7Zgn3r\nNtg1Da3UPrs4mxJziTUv3tS9KaezTld57ssXFdU3jdAQ7Bp8TVMfFeVmogK6UmeGlFQyPv8cl4ED\ncQi35LOFjQ1ugwdX+5qyaYhlefFm7s3YEbeDYlNxpT1By6Y3lp+yWN+WDliKo41jg51fUa4nFdCV\nWjNmZhI/bjxaby8cO3ak6MRJpNFIoxeer/U5Ll8o1MytGWZpJjYnlrs876rYVl+xbUMoK2OrKLcD\nFdCVWsvduJGiEyewDQ0lbfceADz++iS2jRvX+hyXB+mm7pYNoM/lnKsU0M9kn0EgGnSErii3k1oF\ndCHEQGA+oAU+k1LOqaLNKGAWIIE/pJR/qcd+KjeB7PXrsW/XjtDV32LKyaHo1J84dLy6qYPJBclo\nhdY6Mg5xDUEjNJVmuuSV5LH6z9X0btwbR51KiShKbdQY0IUQWuBjoB+QABwQQmyQUp4s16YF8ArQ\nXUqZJYRQQ6rbTNGfpyk+GY3vq68CoHVzw6lrl6s+T4o+BW8Hb7QayyIjW60twS7BlWa6fB39NXmG\nPCaETbj2zivKHaI20xa7AGellOeklCXASuChy9qMAz6WUmYBSClT67ebyo2Ws2E9Uqtlovwfh1MP\n1/k8VS0UaurWlCOpR8gsygSgwFDA8ujl9Azqec2rQRXlTlKbgB4IxJd7nFB6rLyWQEshxC9CiN9K\nUzSVCCHGCyGihBBRaWlpdeuxct1Jk4mMdWs43EwQLRNZfXp1nc+Vqk+tdJNzTNsx5JXk8czWZ8gq\nymLlqZXkFOcwoYManSvK1aivhUU2QAugJ/AYsFgIUWkXXSnlIillpJQy0sfHp54urdSVNBgoiY2t\ndNyUnU3q/PkYkpIAOLb5K0RGNsc7e3Nf0H3situFwWS4+utJSXJBcqWA3sm3E//t/V/icuMYv208\nX5z4gu6B3Wnn3a5O70tR7lS1CeiJQHC5x0Glx8pLADZIKQ1SyvPAaSwBXrmJ/frWC5x+4AH08bEV\njmf+7ysyFnzKuaEPEbdiGVGfv4veXsOUSV8yquUo8gx5/J78u7W9wWRg0o5J7IjbccXr5RvyKTQW\nVrnys1tAN+b3mk9MdgxZxVlMDJtYL+9RUe4ktQnoB4AWQohQIYQt8Ciw4bI267CMzhFCeGNJwVx5\n5wLlhtr45zrEhu1ozZIL/1tqPS7NZnLWrMG+Qxj2rVtT8MZcuh034DSoP34ewXQL6IaTzoltF7Zd\nOte5jexO2M28A/MwmKsfuV8+B/1y3QO782nfT3m588vXXHhLUe5ENQZ0KaURmARsAaKBb6SUJ4QQ\ns4UQQ0ubbQEyhBAngV3AS1LKjIbqtHJ1pMmEubDQ+vjH8z+y/ot/4VEAWU5g3LAZabAEYv3vv2NI\nSsLzyb/S+Itl7B/VlnR3LU2esuSzbbW29AzuyY64HRjMBgxmA4uOLsLdzp3E/EQ2ndtUbT8uXyVa\nlS7+XXiyzZP18bYV5Y5Tqxy6lHKTlLKllLKZlPL/So/NlFJuKP1aSilfkFK2kVK2l1KubMhOK1cn\n9Z15xAwchDEri31J+3jl51cYdtIJra8vyx9wwCYrj7yffgIg+7s1aFxdcenXF4Tgy/ZZbJg7CIdW\nrazn69e4HznFOUQlR7Hp3CYS8xOZfc9s7vK4i8XHFmMym6rsx/VY+akodzJVbfE2J6Ukd8sWjCkp\npLz5b3bE7SA4z5amf+biMXIkhV3bkeumI3vVN5hycsjbuhW3IUPQ2NmRkJdAqj6VSN/ICufsHtgd\nBxsHtsRuYfGxxbTybEXP4J5M7DCRC7kX2By7ucq+pBSkIBD4OKgb4orSEFRAv80V//knxuRk7Nu0\nIXfTJpz3HuWBE7YgBO6PjKR1o7bsCIOCX34hfeEiZEkJbiOGAxCVEgVApF/FgG5vY899Qfex9uxa\nLuReYELYBIQQ9G7cm+buzVl0dFGVo/QUfQpeDl7otLqGf+OKcgdStVxuU4dTD5NVlEXYT2cACPrk\nYxImP8e9X59EaDU4338/Oj8/2hS04b0wM8N+EWQuXYpdq1bYt7Hs1RmVEoWHnQdN3ZpWOn+/Jv3Y\nEruF5u7N6d24N2ApRzuhwwRe2v0SL+5+EXf7ijNX9yXtU+kWRWlAKqDfpj458gkXci+w6Ccv7Nu2\nRefnR8Cct8l7aAg6own3UY8A0NqzNRmugoLIljjtj8Z9xAjrjvZRyVFE+EZUucN9j8AehPuEM6HD\nBDTi0h96/Rr3456AeziSdqTKft0fdH8DvFtFUUAF9NtWfF48hekpFP6RgPezzwIgQoJZNEDD6JQm\ntLrvPgBC3UKx19pzuH8IfYt0uD04BICk/CSSCpL4a9u/Vnl+R50jyx9YXum4VqNlYb+FDfOmFEW5\nIpVDv8UdTTvKueyKU/5LTCVcLLhI+xgjSIlzT8uoOFWfyu4wDemvj0NoLcWxtBotd3nexS/emYR+\nswqtuyVNYs2fX3ZDVFGUm5cK6Le4aXum8c6BdyocS8xPxCzNdDorMXu4Yd+2LVD9tMHWnq05lXkK\nszRbjx1MOYirrSstPNSCX0W5VaiAfgvTG/Qk5CdwNvtshePxefFozJIO5yV5kS0QGsuP2bpS87KF\nPW282lBgKCAuN856LCo5ik6+nSrkxxVFubmp39Zb2Pnc84Bl5J1fkm89Hp8Xz10J4FwE8e0vlaav\nboTexssyqyU6MxqwpGbi8uJUukVRbjEqoN/CyufOy+/4E5dzgV7RWoxaiG5qaz2eqk/FSeeEs61z\nhfM0dW+KTqMjOiMag9nA27+/DcDd/nc38DtQFKU+qYB+CysfxMu+NhcVcdcnW+l5qIRjkV7EmS/V\nnU/Rp1Q5D1yn0dHSoyVH048yfc90tsdtZ1rnaZX2+FQU5eampi3ewmKyYwhxDSEpP4lz2ecoSUgg\n8bkptD6ZwsEHW/LHAy1IyrLuFEhKQdUBHSxpl29PfwvAi5Ev8kSbJ67Le1AUpf6oEfot7FzOOVp4\ntOBuvT8hH24kZtADlMTFMe8RHZmP9cHfNYCLBRets1eS9cnVVjps790egKmdpvJU26eu23tQFKX+\nqBH6LciUnU3Ogd+5d8N57k8vwP1MMsU6gcfoxykeNYAD+8byoEswJaYSjGYjafo0vBy8SC9Mr3aE\nPqTZEFp7taaVZ6sqn1cU5eanAnoDM0tzvUz9M5qNGGPjyFr2BTnr1iFLShisAVMLW84/2o03/Q6w\n4+nnOZJqWXIf7BKM3qgH4GLBRSQSszRXO0LXaXQqmCvKLU4F9Aa0/cJ2Zu2bxYaHN+Bp71nn8+Sm\nJ7FxwhA6nShE2NriNmwYpyJ9+GfKAlYM/y+6vDjyfzpAbG4scXmWueSNXRuTV5IHWBYaldVjUcWx\nFOX2pXLoDehgykFyinPYGbez1q/RHzyIISXF+rjo1Clihg+n/alCtvfyoNnOHfi/MYuTwWCy1RLi\nFkIzt2aA5SZpXF4c9lp7fBx88HfyBywj9Jq2f1MU5dZXqxG6EGIgMB/QAp9JKedc9vwYYB6XNo/+\nSEr5WT3285YUkx0DwLYL2xjZcmSN7YvPnefC40+AEDhEdMKxYycyl39Joa2B/z7lyAnfPAZps2mO\nNzHZMQS7BGOntSPYNRgbYcO5nHPE58YT7BqMEAJHnSMedh4k5Sdhp7UDVEBXlNtZjSN0IYQW+BgY\nBLQBHhNCtKmi6SopZXjpf7d0MH/l51f44sQX13yemBxLQN9/cT85xTk1ttdHHQDA869PYs7JIWPx\nYnJCfXh5jOCZ0XMRCOvmzOeyz1nrlOs0Opq4NrGO0Bu7NLae09/Zn6T8JFIKUrDT2uFm53bN70tR\nlJtTbVIuXYCzUspzUsoSYCXwUMN268YxSzPbLmxjT8KeazpPXkkeqfpU+jXph1Ea2RW/q8bXFB48\nhNbTk0bTp9N040aCd21l+qgSWjW/m75N+tKxUUe2XtiKwWzgQu4Fmrk3s762qXtTzmSdIT4vvkJA\nD3QOJKkgybqoqKra5oqi3B5qE9ADgfhyjxNKj11uhBDiqBBitRAiuKoTCSHGCyGihBBRaWlpVTW5\n4VL1qRSbiq03F+uqnfMvNAAAGLlJREFUbOXmg00fJMApwDqyvhL94cM4dOpoDbrrs/eQXpLJxLCJ\nAPQP6c/Z7LPsSdiDURor7CTUzL0ZCfkJGMwGgl0vffv9nfy5mH+R5ILq56ArinJ7qK+bohuBECll\nGLANqDJfIaVcJKWMlFJG+vjcnBsFx+dZPruSC5IpMhbV+TxldVaauzenX5N+/Jr0q3XWSVWMaWkY\n4uL43jmG4RuGM3zDcOYfmk+kb6R1T88+jfsAsPAPywYS5UfoZTdGgQoj9ADnAIpMRZzOOq3y54py\nm6tNQE8Eyo+4g7h08xMAKWWGlLK49OFnQET9dO/6K19CNjE/8QotrywmOwY7rR0BzgH0bdIXo9nI\nT/E/Vdtef+gwANtc43HWOdPYpTH3Bt7Ly51ftrbxc/IjzCeM6MxoBIJQt1Drc03dL43WKwR0pwDL\n+Y16FdAV5TZXm1kuB4AWQohQLIH8UeAv5RsIIfyllBdLHw4Fouu1l9dR+VRLXG5chVHw1TiXc44Q\n1xC0Gi1hPmE0cvz/9u48Pq7qOuD474xG62izZCHLWr3v2IA3grHxiiHBzieUYAolTUhNkxKchhJI\nF9OGkIUkJKUhBIclaRpCydKUELMI2wTqYLBsbGNZgI0XSV5k7ZKtXTr9Y56GkTWyRmjzjM7389HH\nM/ddzTuXK46u7nvv3ovIP5bPdROuC1i/cfduOqLcHB6j/GbhRiaOmhiw3qrcVewr38fY+LHEumN9\n5XmJebjEhVvcXaZWxsaP9b22KRdjwluvI3RVbQPuAF7Cm6ifVdVCEfm6iKxxqt0pIoUishe4E/jr\nwQp4sJXUl/geAurPPPrh2sNMSMijsbAQl7hYmbuS7ce3c7b1bMD6Dbt3UzchnfYIISshq8fPXZG7\nAqDbL5qoiChyEnLISsjq8mRql4RuI3RjwlpQc+iqullVJ6vqBFV9wCnbqKrPOa+/pqozVHW2qi5V\n1XcHM+jBVFxXzIzUGSREJfjm0/uqobWB42eOc9W2Ko5e/xdU/PSnrMxdSUtHC6+Xvt6tfkdDA01F\nRZSO85Ael06MO6bHz86Mz+SWabewZsKabsfWTV3HDZNv6FKWEJVAQlQCYCN0Y8KdPfrvR1Upri9m\n3ph5VDVVdZlP74sjdUdAlazXDyKRkZR//yFy3V9ltGc0Lx97mdXjVnep37jvHWhro3BsBzmJOT18\n6ofumX9PwPKbp90csHysZyzvtbxnI3Rjwpw9+u+nsqmSxrZGshOyyUnIOe+Uy/6K/T0+0n+45jBT\nSsF9soIx/3ofCatXU/6dB/nC/jEUF/yJmn27aT58xFe/8e3dALw1urbLBc2BMjZ+LG6Xu1/ryRhj\nLnw2QvfTOSLPScyhsqmSl469RGt7K5ERkQB0tLQgIkhkJN9681scqTvC9uzt3R7W+aDmA5buB4mN\nIXH1apLWrOF4exuznn2FWcDJx70j6YRrVjNm40Yadr9N5MTxlEgx1ycEvIW/X67MupLoiGjb8NmY\nMGcJ3U/nnHlOQg6VjZV0aAcnzp4gNzEXgBN3f5XG3buJ+c5G9lXsA6D0TCnZ5yTho+Xvc+u7SuLV\nq3B5PABkPvQQdTve4J+33M2UlCncKPOp2LSJhp0FdDQ0oKsWAcVBTbn01Q2Tb+g2t26MCT82ZPNT\nXF9MhESQEZ/hS6ydo/aOs2c5s3UrbRUV1K3fwPK3vbsAFVV2v0MzbkchsU0dJK39cIUEiYwk6crF\npK66hl+kvkfiF/+Gcb9+FndqKtrQQOVk7/z2YEy5GGNGBkvofkrqShgbP5ZIV6Rv1N05j352xw60\ntZXMH/yAoxPjuf3FDr74R+X943u7fEZzezMzCsppSvEQt2BBt3Osyl1FQ1sDfz7+Z2KmTmXcr58l\n+7Gf8O6lowHOe8uiMcacjyV0P/4rFabGpBLnjvNNw5x59U+4PB4aFs7ga2vPcuxT81n8Tjtz7/4l\nZ996y/cZR47sYfYHSvOKBUhERLdzzMuYR2JUom9tF4mKIn7JEkoajpMak4on0jMELTXGhCObQ3eo\nKsV1xcwa790sWUTIScyhuK4YVeXMa6/hueIKtp58DXUJk796H89P+R4XP/Yaxbd+Bs+VV9JRX0/b\nsQ+IUEhZ+6mA54l0RbIsZxlbjm3pcsG1uG5w5s+NMSOHjdAdtc211LfWd0mq2QnZlNSX0Pzuu7SV\nlRG/ZAmvFL/ChKQJjE8ez0ULFvMPn4Xom66ntbQUiY7mxIyLePJqN3lzruzxXCtzV1LfWs8bJ9/w\nlRXXF3e7uGqMMX1hCd3h24vT76JkTkIOpWdKqdvmXcu8Zf4MdpXtYmXeSgCmpU6jOUo48tmlTHhh\nMzk/e4rvr26lYc1ioiKiejzXwoyFxEfG88qxVwBobGvkdMNpuyBqjOkXS+iOzoTuv5Z4TmIObR1t\n1Gx7hZiZM3m1YS8d2sHKXG9CnzxqMi5x+e50OVB5gBNnT/iO9yQqIoqrsq9ia8lWWjtaKa0v9Z3P\nGGM+KkvojpK6EgQhK/7Du0yyE7JJaFDa9r+L64p5/KzwZ+Ql5jEpeRIAse5YxieNp6jKm9BfPvYy\nbnGzNHtpr+dbmbuS2uZadp7aGfCvA2OM6StL6I7i+mIyPBldpkpyEnKY84Eiqnw78hUqGiu4/4r7\nuzwZOj11OkWVRagq+cfyWZCxIKh9Oz829mPEuePIP5ZPSZ33Thq7ZdEY0x+W0B0l9SXdLkqmxaWx\n8KBQ44HdydX8ePmPmXPRnC51pqVMo7yxnO0ntlNSX9LrdEunGHcMS7KWsLV4K0fqjpAcnWwbOBtj\n+sUSOt5bFo/VHesyfw5Q/4fnmfdeO6/PjuRHK37s2wrO37TUaQA8vPthXOJiaU7v0y2dVuSuoKqp\nivyj+TbdYozpN0voeJe7rWmuYWbqTF9ZY2EhJ/9lIx1zpvOJb/0X88bMC/i9U1OmIghFVUXMS5/X\npxUNF2UuIiYihvrW+m6/TIwxpq8soQMFpwoAfCPwtqoqSr/0JSJSUpjyyCamp1/c4/d6Ij2+xbuC\nnW7pFBcZx5VZ3vvVbYRujOmvoBK6iKwWkfdE5JCI3HueeteLiIpI97mJYVLXUsen//Bp9pze02Od\ngrIC0mLTyEnIQdvbOf6Vu2ivqCTr4Ydxp6b2eo5pqdMQhOW5y/scX+cvAXuoyBjTX70mdBGJAB4B\nrgGmAzeJyPQA9RKADcCbAx1kfxypPUJRVRE/3P3DgMdVlV2ndnFZ+mWICBWPPUbDjh2MuW8jsbNm\nBvyec31+1ue5/4r7GR07us/xLc9ZzoZLN7AsZ1mfv9cYY/wFM0KfDxxS1cOq2gI8A6wNUO9+4DtA\n0wDG12/VTdUA7Crbxc5TO7sdL6kv4XTjaeamz6WhoICKHz1C4nXXkfSpwGuxBDJ51GTWTgz0n6R3\nURFRfH7W521RLmNMvwWT0DMB/92SS50yHxG5FMhW1T+e74NEZL2IFIhIQXl5eZ+D/Sg6E3qsO5bH\n9j3W7fiusl0AXBY7meP/cDeR2VmMue++brsQGWPMha7fF0VFxAU8BNzVW11V3aSqc1V1blpaWn9P\nHZSqpioAPjfzc7x58k3ePv12l+MFZQWkxKQQ/b2naKusJPOhh4iIt9GyMSb0BJPQjwP+V+yynLJO\nCcBM4FUROQosBJ67UC6MVjdVExMRw63TbyUlJoXH9nYdpRecKmApUzmTn8/o228ndsaMYYrUGGP6\nJ5iEvhOYJCLjRCQKWAc813lQVWtVdbSq5qlqHrADWKOqBYMR8Kslr/LlbV+mQzuCql/dXM2omFHE\nRcZx6/Rb2X5iu+82xRNnTnDi7AkW7+8Al4vkG2zfTWNM6Oo1oatqG3AH8BJQBDyrqoUi8nURWTPY\nAZ6rvqWeLcVb2F+xP6j6VU1VjIoZBcC6qevIjM9kw7YNFFUWUVBWgKiS8fp7eC6/nMj0iwYzdGOM\nGVRBzaGr6mZVnayqE1T1Aadso6o+F6DuVYM1OgdYkr0Et8vt28KtN9VN1b6E7on08MTVTxAXGcf6\n/PX8/tDvuexkHJwqJ+mTH+0uFWOMuVCE3JOiiVGJLMxYSP6xfFS11/rVTdWkRH/4OH5mfCZPrnqS\nqIgodp7ayXXvJ+CKiyNhed8fCjLGmAtJyCV0gFW5qzh+5jgHqg70Wre6uZrkmOQuZdmJ2Tx59ZPM\nip/MlL1VJFx9Na64uMEK1xhjhkRIJvSl2UuJkAjfFm49aWxrpLGtMeCCWbmJuTwa/TlcDU0krbXp\nFmNM6AvJhJ4ck8z8MfN5+ejL55126XyoaFT0KF9Z49691G/dRv3WbVT/6le4MzKImx94JUVjjAkl\n7uEO4KNakbuC+3fcz/vV7zMlZUrAOr6E7lwUbTpwgKM3rutSZ/QXv4i4QvL3mjHGdBGyCX1ZzjIe\nePMB8o/l95jQO58S7ZxyqXz8CVweD9mP/xSJjEJcQvSkSUMWszHGDKaQHZqOjh3NZemXnff2xerm\nD0foLSUl1L34IsnrbiTukkuInTmDmOnTkcjIoQrZGGMGVcgmdIBl2cs4XHuYU2dPBTzuP+VS9dRT\nEBFByq2fGcoQjTFmyIR0Qs9LygPg5NmTAY9XNVXhdrmJrWuh5re/I2nNdfY0qDEmbIV0Qk+PSweg\n7GxZwOM1zTWMih5FzdNPoy0tpN5221CGZ4wxQyq0E7rHSegNgRN6VVMVF7mSqPrl08QvX0b0+PFD\nGZ4xxgypkE7oCZEJxLpjzzuHfslhpaO2lpRb/mqIozPGmKEVsrctAogI6XHpPY7Qq5uqmX6ghYik\nJOLmXjbE0RljzNAK6YQO3mmXnhJ6bUMV2fsbiV9+NeIO+aYaY8x5hfSUC3gvjAa6KNra3krG0Xqi\nzzQTv3TpMERmjDFDKywSekVjBW0dbV3Kq5uruexgBx3uCDyLFg1TdMYYM3RCPqGP8YyhXdupbKzs\nUl7dVM3cg0rL7ElExMcPU3TGGDN0gkroIrJaRN4TkUMicm+A438rIu+IyB4R+T8RmT7woQbmuxf9\nnHn0mvcLyawCWTR/qEIxxphh1WtCF5EI4BHgGmA6cFOAhP20qs5S1TnAg8BDAx5pD3q6F7319TcA\nSLD5c2PMCBHMCH0+cEhVD6tqC/AM0GVHCFWt83vrAXrfG26A9PS0aNQbezmSDil5gVdiNMaYcBNM\nQs8ESvzelzplXYjI34nIB3hH6HcG+iARWS8iBSJSUF5e/lHi7SY5OpkoVxSnG077ytqqqogvKmX3\nRBdJ0UkDch5jjLnQDdhFUVV9RFUnAPcA/9xDnU2qOldV56alpQ3IeUWEdE86pxpOdZ6Dsm98AxXY\nPycZl4T8dV9jjAlKMNnuOJDt9z7LKevJM8An+xNUX/nfi177299St/kFCj4xkcbs0UMZhjHGDKtg\nEvpOYJKIjBORKGAd8Jx/BRHx3/bn48DBgQuxd51PizYfOsSpbzxA3OUL2bYk2bf1nDHGjAS9JnRV\nbQPuAF4CioBnVbVQRL4uImucaneISKGI7AG+AgzpLhLpcelU15ZR+vdfweXxkPngg1S11vi2njPG\nmJEgqAVOVHUzsPmcso1+rzcMcFx9kh6XzuX7Wmg5eJDsx36COy2N6qZqRkXbCN0YM3KExRXDdE86\nHytSNHssnsWLae9op7a51qZcjDEjSlgk9DFNMcwoVs4uno2IUNNcg6KW0I0xI0pYJPSE7ftxKZQu\nyAM+3Bza5tCNMSNJWCT0ji3/R+looXi09wHVdyreASDDkzGcYRljzJAK+YTeWnaaxl272HdxAmVn\ny2jraOPxdx5nWso0ZqfNHu7wjDFmyIR8Qq9/6UVQpXh+DmUNZbxw5AWK64u5ffbtiMhwh2eMMUMm\n5BN63eYXiJ46lchxuZw4c4JN+zYxedRklmbbKovGmJElpBN664kTNO7ZQ+Lq1aTHpVN6ppSjdUe5\n/eLbbQ0XY8yIE7JZT9vbOfXAN8HlIvHaa3zL6E5ImsCK3BXDHJ0xxgy9kEzoqkrZA9/kzJYtpP/j\nPxKVk0NmvHdF3/UXr7fRuTFmRArq0f8LTdWTT1L99NOk3PY5Um65GYDF2Yt5dMWjXDH2imGOzhhj\nhkfIJfTa5//I6e9+j8Rrr+Wiu+7ylUe6IlmUuWgYIzPGmOEVcnMT7rQ04pcvJ+Pb30JcIRe+McYM\nmpAboXsWzMezYP5wh2GMMRccG+IaY0yYsIRujDFhwhK6McaEiaASuoisFpH3ROSQiNwb4PhXROSA\niOwTkS0ikjvwoRpjjDmfXhO6iEQAjwDXANOBm0Rk+jnV3gbmqurFwG+ABwc6UGOMMecXzAh9PnBI\nVQ+ragvwDLDWv4KqblPVBuftDiBrYMM0xhjTm2ASeiZQ4ve+1CnryW3AC4EOiMh6ESkQkYLy8vLg\nozTGGNOrAb0oKiK3AHOB7wY6rqqbVHWuqs5NS0sbyFMbY8yIF8yDRceBbL/3WU5ZFyKyAvgnYImq\nNvf2obt27aoQkWPBBnqO0UDFR/zeUDYS2z0S2wwjs90jsc3Q93b3eNOJqOp5v1NE3MD7wHK8iXwn\n8JeqWuhX5xK8F0NXq+rBPgT2kYhIgarOHezzXGhGYrtHYpthZLZ7JLYZBrbdvU65qGobcAfwElAE\nPKuqhSLydRFZ41T7LhAP/FpE9ojIcwMRnDHGmOAFtZaLqm4GNp9TttHvte0oYYwxwyxUnxTdNNwB\nDJOR2O6R2GYYme0eiW2GAWx3r3PoxhhjQkOojtCNMcacwxK6McaEiZBL6L0tFBYORCRbRLY5C54V\nisgGpzxFRPJF5KDz76jhjnWgiUiEiLwtIs8778eJyJtOf/+3iEQNd4wDTUSSReQ3IvKuiBSJyOUj\npK//3vn53i8ivxKRmHDrbxF5UkROi8h+v7KAfSteDztt3ycil/b1fCGV0INcKCwctAF3qep0YCHw\nd0477wW2qOokYIvzPtxswHt7bKfvAD9Q1YlANd6lJcLNvwMvqupUYDbe9od1X4tIJnAn3kX9ZgIR\nwDrCr79/Bqw+p6ynvr0GmOR8rQce7evJQiqhE8RCYeFAVU+q6m7ndT3e/8Ez8bb15061nwOfHJ4I\nB4eIZAEfBx533guwDO9DaxCebU4CFgNPAKhqi6rWEOZ97XADsc7Di3HAScKsv1X1NaDqnOKe+nYt\n8J/qtQNIFpGMvpwv1BJ6XxcKC3kikgdcArwJpKvqSefQKSB9mMIaLD8Evgp0OO9TgRrn4TYIz/4e\nB5QDTzlTTY+LiIcw72tVPQ58DyjGm8hrgV2Ef39Dz33b7/wWagl9RBGReOC3wJdVtc7/mHrvNw2b\ne05F5BPAaVXdNdyxDDE3cCnwqKpeApzlnOmVcOtrAGfeeC3eX2hjAQ/dpybC3kD3bagl9KAWCgsH\nIhKJN5n/UlV/5xSXdf4J5vx7erjiGwRXAGtE5CjeqbRleOeWk50/ySE8+7sUKFXVN533v8Gb4MO5\nrwFWAEdUtVxVW4Hf4f0ZCPf+hp77tt/5LdQS+k5gknMlPArvRZSwWzfGmTt+AihS1Yf8Dj0HfMZ5\n/Rngf4c6tsGiql9T1SxVzcPbr1tV9WZgG/AXTrWwajOAqp4CSkRkilO0HDhAGPe1oxhYKCJxzs97\nZ7vDur8dPfXtc8Ctzt0uC4Fav6mZ4KhqSH0B1+Jd/fED4J+GO55BauMivH+G7QP2OF/X4p1T3gIc\nBF4BUoY71kFq/1XA887r8cBbwCHg10D0cMc3CO2dAxQ4/f17YNRI6Gvg34B3gf3AL4DocOtv4Fd4\nrxG04v1r7Lae+hYQvHfxfQC8g/cOoD6dzx79N8aYMBFqUy7GGGN6YAndGGPChCV0Y4wJE5bQjTEm\nTFhCN8aYMGEJ3YQdEWl39rbt/Bqwha1EJM9/5TxjLiRB7SlqTIhpVNU5wx2EMUPNRuhmxBCRoyLy\noIi8IyJvichEpzxPRLY6a1BvEZEcpzxdRP5HRPY6Xx9zPipCRH7qrOX9sojEOvXvdNaw3ycizwxT\nM80IZgndhKPYc6ZcbvQ7Vquqs4Af4V3dEeA/gJ+r6sXAL4GHnfKHgT+p6my866sUOuWTgEdUdQZQ\nA1zvlN8LXOJ8zt8OVuOM6Yk9KWrCjoicUdX4AOVHgWWqethZ/OyUqqaKSAWQoaqtTvlJVR0tIuVA\nlqo2+31GHpCv3s0JEJF7gEhV/YaIvAicwfv4/u9V9cwgN9WYLmyEbkYa7eF1XzT7vW7nw2tRH8e7\nFselwE6/VQONGRKW0M1Ic6Pfv284r/+Md4VHgJuB153XW4AvgG+v06SePlREXEC2qm4D7gGSgG5/\nJRgzmGwEYcJRrIjs8Xv/oqp23ro4SkT24R1l3+SUfQnvjkF349096LNO+QZgk4jchnck/gW8K+cF\nEgH8l5P0BXhYvVvJGTNkbA7djBjOHPpcVa0Y7liMGQw25WKMMWHCRujGGBMmbIRujDFhwhK6McaE\nCUvoxhgTJiyhG2NMmLCEbowxYeL/AbCyZlI7wKYaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0BsMBoPDMUJvMBgMDuf/AX7dhQCyJdhvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "val accuracy 0.8433734939759037\n",
            "val loss 0.489986171833722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1kgj76dvQxIJ"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_dKdDvgnQw7d",
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f053788a-0b11-45b6-bd38-c0f21716cf41"
      },
      "source": [
        "from torchvision import datasets\n",
        "# Serve per il mapping fra label sul file con le labels per canzone di test e l'indice della label che la rete considera.\n",
        "LABELS_FROM_FILE = {'angry' : 0, 'calming' : 1, 'happy' : 2, 'normal' : 3, 'sad' : 4}\n",
        "NUM_CLASSES = 5\n",
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset that includes image file paths. Extends\n",
        "    torchvision.datasets.ImageFolder\n",
        "    \"\"\"\n",
        "\n",
        "    # override the __getitem__ method. this is the method that dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path = self.imgs[index][0]\n",
        "        # make a new tuple that includes original and the path\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path\n",
        "\n",
        "def test_network_with_songs_data_return(net, test_dataset, batch_size):\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    net.train(False)\n",
        "    net = net.to(DEVICE)\n",
        "\n",
        "    test_songs_data = dict()\n",
        "    for images, labels, paths in test_dataloader:\n",
        "      torch.cuda.empty_cache()\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      idx = 0;\n",
        "      for pred in preds:\n",
        "\n",
        "        image_name = paths[idx].split(\"/\")[-1]\n",
        "        song_idx = image_name.split(\"_\")[0]\n",
        "      \n",
        "        if song_idx not in test_songs_data:\n",
        "          test_songs_data[song_idx] = dict()\n",
        "          test_songs_data[song_idx][\"preds\"] = np.zeros(NUM_CLASSES, dtype=int)\n",
        "          # test_songs_data[song_idx][\"outputs\"] = []\n",
        "\n",
        "        test_songs_data[song_idx][\"preds\"][pred] += 1\n",
        "        # test_songs_data[song_idx][\"outputs\"].append(outputs[idx])\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "      del labels\n",
        "      del images\n",
        "      del outputs\n",
        "\n",
        "    return test_songs_data\n",
        "\n",
        "def read_songs_labels(path):\n",
        "  f = open(path,\"r\")\n",
        "  lines = f.readlines()\n",
        "\n",
        "  song_labels = dict()\n",
        "\n",
        "  for line in lines:\n",
        "    names = line.split(\":\")\n",
        "\n",
        "    if names[0] not in song_labels:\n",
        "      song_labels[names[0]] = []\n",
        "\n",
        "    labels = names[1].replace(\" \", \"\").split(\",\")\n",
        "    labels[-1] = labels[-1][:-2]\n",
        "\n",
        "    for label in labels:\n",
        "      song_labels[names[0]].append(LABELS_FROM_FILE[label])\n",
        "  \n",
        "  return song_labels\n",
        "\n",
        "def major_voting_analyze(songs_data, songs_labels):\n",
        "\n",
        "  ordered_keys = sorted(songs_labels.keys())\n",
        "  print(ordered_keys)\n",
        "  prediction = dict()\n",
        "  avg_outputs = dict()\n",
        "  corrects = 0\n",
        "\n",
        "  for key in songs_data.keys():\n",
        "    num_slices = 0\n",
        "    for value in songs_data[key][\"preds\"]:\n",
        "      num_slices += value\n",
        "\n",
        "\n",
        "    for value in songs_data[key][\"preds\"]:\n",
        "      value = (float) (value/num_slices)\n",
        "      # if value > 0.5:\n",
        "      #   prediction[key] = idx_pred\n",
        "\n",
        "    max = 0\n",
        "    idx_pred = 0\n",
        "    idx_max = 0\n",
        "    for value in songs_data[key][\"preds\"]:\n",
        "      if value > max:\n",
        "        max = value\n",
        "        idx_max = idx_pred\n",
        "      idx_pred += 1\n",
        "\n",
        "    prediction[key] = idx_max\n",
        "\n",
        "    # for output in songs_data[key][\"outputs\"]:\n",
        "    #   if sum_outputs is None:\n",
        "    #     sum_outputs = output\n",
        "    #   else:\n",
        "    #     sum_outputs += output\n",
        "\n",
        "    # avg_ouputs = sum_outputs/num_slices\n",
        "\n",
        "  idx = 0\n",
        "  keys = sorted(prediction.keys(), key=int)\n",
        "  print(list(keys))\n",
        "  for song_idx in keys:\n",
        "    if prediction[song_idx] in songs_labels[ordered_keys[idx]]:\n",
        "      corrects += 1\n",
        "    print(\"Prediction for song {} - {}: {}; labels: {}\".format(song_idx, ordered_keys[idx], prediction[song_idx], list(songs_labels[ordered_keys[idx]])))\n",
        "    idx += 1\n",
        "\n",
        "  test_accuracy = (float) (corrects / idx)\n",
        "  print(\"Test accuracy: {}\".format(test_accuracy))\n",
        "\n",
        "def get_test_dataset(test_data_dir):\n",
        "    eval_transform = transforms.Compose([\n",
        "          #transforms.Resize(224),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor()\n",
        "          ])\n",
        "    \n",
        "    if not os.path.isdir('./AIML_project'):\n",
        "        !git clone https://github.com/anphetamina/AIML_project.git\n",
        "    \n",
        "    test_dataset = ImageFolderWithPaths(test_data_dir, transform=eval_transform)\n",
        "\n",
        "    return test_dataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "TEST_DATA_DIR = 'AIML_project/CAL500_test_sliced_spectrograms'\n",
        "test_dataset = get_test_dataset(TEST_DATA_DIR)\n",
        "print('test set {}'.format(len(test_dataset)))\n",
        "\n",
        "# net extracted by training\n",
        "songs_data = test_network_with_songs_data_return(best_net, test_dataset, BATCH_SIZE)\n",
        "\n",
        "print(songs_data)\n",
        "\n",
        "songs_labels = read_songs_labels(\"AIML_project/songs_filtered_with_labels.txt\")\n",
        "\n",
        "print(songs_labels)\n",
        "\n",
        "major_voting_analyze(songs_data, songs_labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test set 19229\n",
            "{'100': {'preds': array([ 0,  0, 49,  0,  0])}, '101': {'preds': array([ 1,  0, 60,  0,  0])}, '102': {'preds': array([ 0,  0, 38,  0,  0])}, '103': {'preds': array([ 0,  0, 42,  0,  0])}, '104': {'preds': array([ 0,  0, 36,  0,  0])}, '105': {'preds': array([ 0,  2, 85,  1,  6])}, '106': {'preds': array([ 0,  0, 46,  0,  0])}, '107': {'preds': array([ 0,  0, 24,  0,  0])}, '108': {'preds': array([ 0,  0, 64,  0,  0])}, '109': {'preds': array([ 0,  0, 54,  1,  0])}, '10': {'preds': array([ 0,  0, 66,  0,  4])}, '110': {'preds': array([ 0,  0, 29,  0,  0])}, '111': {'preds': array([ 0,  0, 28,  0,  0])}, '112': {'preds': array([ 0,  0, 42,  0,  0])}, '113': {'preds': array([ 1, 10, 54,  1, 26])}, '114': {'preds': array([ 0,  0, 55,  0,  0])}, '115': {'preds': array([ 0,  0, 30,  0,  0])}, '116': {'preds': array([ 0,  0, 49,  0,  0])}, '117': {'preds': array([ 0,  0, 42,  1,  0])}, '118': {'preds': array([ 0,  0, 52,  0,  0])}, '119': {'preds': array([ 1,  0, 38,  0,  0])}, '11': {'preds': array([ 0,  0, 51,  0,  0])}, '120': {'preds': array([ 0,  0, 11,  0,  0])}, '121': {'preds': array([ 0,  0, 38,  0,  0])}, '122': {'preds': array([ 0,  0, 59,  0,  0])}, '123': {'preds': array([ 0,  0, 25,  0,  0])}, '124': {'preds': array([ 0,  0, 42,  0,  0])}, '125': {'preds': array([ 0,  0, 61,  0,  0])}, '126': {'preds': array([ 0,  0, 28,  0,  0])}, '127': {'preds': array([ 7,  0, 24,  0, 13])}, '128': {'preds': array([ 0,  0, 41,  0,  0])}, '129': {'preds': array([ 0, 10, 20,  1,  4])}, '12': {'preds': array([ 1,  0, 36,  0,  0])}, '130': {'preds': array([ 0,  0, 53,  0,  0])}, '131': {'preds': array([ 0,  0, 26,  0,  0])}, '132': {'preds': array([ 0,  0, 17,  0,  0])}, '133': {'preds': array([ 0,  0, 27,  0,  0])}, '134': {'preds': array([  0,   0, 125,   2,   2])}, '135': {'preds': array([ 0,  0, 38,  0,  0])}, '136': {'preds': array([  1,   0, 116,   0,   0])}, '137': {'preds': array([ 0,  0, 60,  0,  0])}, '138': {'preds': array([ 4,  0, 63,  0,  0])}, '139': {'preds': array([ 0,  0, 38,  0,  0])}, '13': {'preds': array([ 0,  0, 59,  0,  0])}, '140': {'preds': array([ 0,  0, 49,  0,  0])}, '141': {'preds': array([ 0,  0, 47,  0,  0])}, '142': {'preds': array([ 0,  0, 28,  0,  0])}, '143': {'preds': array([ 1,  0, 50,  0,  0])}, '144': {'preds': array([ 5,  0, 28,  0,  0])}, '145': {'preds': array([ 0,  0, 22,  0,  0])}, '146': {'preds': array([ 0,  0, 48,  0,  0])}, '147': {'preds': array([ 1,  0, 28,  0,  0])}, '148': {'preds': array([  2,   0, 107,   2,   0])}, '149': {'preds': array([ 0,  0, 52,  0,  0])}, '14': {'preds': array([ 0,  0, 30,  0,  0])}, '150': {'preds': array([  1,   0, 103,   0,   0])}, '151': {'preds': array([ 0,  0, 21,  0,  0])}, '152': {'preds': array([ 0,  0, 20,  0,  0])}, '153': {'preds': array([ 0,  0, 33,  0,  0])}, '154': {'preds': array([ 0,  0, 32,  0,  0])}, '155': {'preds': array([ 0,  0, 44,  0,  0])}, '156': {'preds': array([ 2,  0, 52,  0,  0])}, '157': {'preds': array([ 0,  0, 42,  0,  0])}, '158': {'preds': array([ 4,  0, 42,  0,  1])}, '159': {'preds': array([ 0,  0, 48,  0,  0])}, '15': {'preds': array([ 3,  0, 46,  0,  0])}, '160': {'preds': array([ 0,  0, 54,  0,  0])}, '161': {'preds': array([ 0,  0, 27,  0,  0])}, '162': {'preds': array([ 0,  0, 38,  0,  0])}, '163': {'preds': array([ 0,  0, 30,  0,  0])}, '164': {'preds': array([ 0,  0, 32,  0,  0])}, '165': {'preds': array([ 0,  0, 78,  0,  0])}, '166': {'preds': array([ 0,  0, 32,  1,  0])}, '167': {'preds': array([ 0,  0, 16,  0,  0])}, '168': {'preds': array([ 0,  0, 36,  0,  0])}, '169': {'preds': array([ 0,  0, 40,  0,  0])}, '16': {'preds': array([ 0,  0, 46,  0,  0])}, '170': {'preds': array([ 0,  0, 39,  0,  0])}, '171': {'preds': array([ 0,  0, 29,  0,  0])}, '172': {'preds': array([ 0,  0, 51,  0,  0])}, '173': {'preds': array([ 0,  0, 21,  0,  0])}, '174': {'preds': array([ 0,  0, 47,  0,  0])}, '175': {'preds': array([ 0,  0, 58,  0,  0])}, '176': {'preds': array([ 4,  0, 49,  0,  0])}, '177': {'preds': array([ 0,  1, 32,  0,  2])}, '178': {'preds': array([ 0,  0, 47,  0,  0])}, '179': {'preds': array([ 0,  0, 19,  0,  0])}, '17': {'preds': array([ 0,  0, 48,  0,  0])}, '180': {'preds': array([ 0,  0, 18,  0,  0])}, '181': {'preds': array([ 2,  0, 41,  0,  1])}, '182': {'preds': array([ 0,  0, 21,  0,  0])}, '183': {'preds': array([ 0,  0, 41,  0,  0])}, '184': {'preds': array([ 0,  0, 57,  0,  0])}, '185': {'preds': array([ 0,  0, 44,  0,  0])}, '186': {'preds': array([ 1,  0, 15,  0,  1])}, '187': {'preds': array([ 0,  0, 40,  0,  0])}, '188': {'preds': array([ 0,  0, 16,  0,  0])}, '189': {'preds': array([ 0,  0, 44,  0,  0])}, '18': {'preds': array([ 0,  0, 33,  0,  0])}, '190': {'preds': array([ 0,  0, 42,  0,  0])}, '191': {'preds': array([ 0,  0, 39,  0,  0])}, '192': {'preds': array([ 0,  0, 29,  0,  0])}, '193': {'preds': array([ 0,  0, 59,  0,  0])}, '194': {'preds': array([ 0,  0, 43,  0,  0])}, '195': {'preds': array([  1,   0, 117,   0,   0])}, '196': {'preds': array([ 0,  0, 36,  0,  0])}, '197': {'preds': array([ 0,  0, 29,  0,  0])}, '198': {'preds': array([ 0,  0, 64,  0,  0])}, '199': {'preds': array([ 0,  0, 38,  0,  0])}, '19': {'preds': array([ 0,  0, 45,  0,  0])}, '1': {'preds': array([ 0,  0, 62,  2,  1])}, '200': {'preds': array([ 5,  0, 76,  0,  0])}, '201': {'preds': array([ 0,  0, 73,  1,  1])}, '202': {'preds': array([ 0,  0, 55,  0,  0])}, '204': {'preds': array([ 0,  0, 29,  0,  0])}, '205': {'preds': array([ 1,  0, 46,  0,  0])}, '207': {'preds': array([ 0,  0, 32,  0,  0])}, '208': {'preds': array([ 0,  0, 35,  0,  0])}, '209': {'preds': array([ 0,  0, 96,  0,  1])}, '20': {'preds': array([ 0,  0, 41,  0,  0])}, '210': {'preds': array([ 1,  0, 43,  0,  0])}, '211': {'preds': array([ 0,  0, 21,  0,  0])}, '212': {'preds': array([ 0,  0, 38,  0,  0])}, '213': {'preds': array([ 0,  0, 48,  0,  0])}, '214': {'preds': array([  1,   0, 215,   2,   6])}, '215': {'preds': array([ 0,  0, 53,  0,  0])}, '216': {'preds': array([ 1,  0, 27,  0,  0])}, '217': {'preds': array([ 0,  0, 12,  0,  0])}, '218': {'preds': array([ 0,  0, 45,  0,  0])}, '219': {'preds': array([ 5,  0, 32,  0,  0])}, '21': {'preds': array([ 0,  0, 29,  1,  0])}, '220': {'preds': array([ 0,  0, 34,  0,  0])}, '221': {'preds': array([ 0,  0, 56,  0,  0])}, '222': {'preds': array([ 0,  0, 22,  0,  0])}, '223': {'preds': array([ 0,  0, 51,  0,  0])}, '224': {'preds': array([ 3,  0, 36,  0,  0])}, '225': {'preds': array([ 0,  0, 76,  0,  0])}, '226': {'preds': array([ 0,  0, 30,  0,  0])}, '227': {'preds': array([ 0,  0, 41,  0,  0])}, '228': {'preds': array([ 0,  0, 31,  0,  0])}, '229': {'preds': array([  1,   2, 146,   1,   2])}, '22': {'preds': array([ 1,  0, 18,  0,  0])}, '230': {'preds': array([ 0,  0, 30,  0,  0])}, '231': {'preds': array([ 0,  0, 20,  0,  0])}, '232': {'preds': array([ 1,  0, 40,  0,  1])}, '233': {'preds': array([ 0,  0, 38,  0,  0])}, '234': {'preds': array([ 0,  7, 55,  0,  4])}, '235': {'preds': array([ 0,  0, 27,  0,  0])}, '236': {'preds': array([ 0,  0, 57,  0,  0])}, '237': {'preds': array([ 0,  0, 24,  0,  0])}, '238': {'preds': array([ 0,  0, 45,  1,  0])}, '239': {'preds': array([ 0,  0, 30,  0,  0])}, '23': {'preds': array([ 2,  0, 64,  3,  1])}, '240': {'preds': array([ 0,  0, 87,  0,  0])}, '241': {'preds': array([ 0,  0, 55,  0,  0])}, '242': {'preds': array([ 0,  0, 44,  0,  0])}, '243': {'preds': array([ 0,  0, 26,  0,  0])}, '244': {'preds': array([ 0,  0, 65,  0,  0])}, '245': {'preds': array([ 0,  0, 19,  0,  0])}, '246': {'preds': array([ 0,  0, 13,  0,  0])}, '247': {'preds': array([ 0,  0, 21,  0,  0])}, '248': {'preds': array([ 0,  0, 43,  0,  0])}, '249': {'preds': array([ 0,  0, 27,  0,  0])}, '24': {'preds': array([ 0,  0, 26,  0,  0])}, '250': {'preds': array([ 1,  0, 58,  0,  0])}, '251': {'preds': array([ 0,  0, 57,  0,  0])}, '252': {'preds': array([ 1,  0, 48,  0,  0])}, '253': {'preds': array([ 0,  0, 35,  0,  0])}, '254': {'preds': array([ 0,  0, 58,  0,  0])}, '255': {'preds': array([ 3,  8, 64,  0,  2])}, '256': {'preds': array([ 1,  1, 67,  0,  0])}, '257': {'preds': array([ 0,  0, 44,  0,  0])}, '258': {'preds': array([ 0,  0, 18,  0,  0])}, '259': {'preds': array([ 0,  0, 26,  0,  0])}, '25': {'preds': array([ 1,  0, 63,  0,  0])}, '260': {'preds': array([ 0,  0, 78,  2,  0])}, '261': {'preds': array([ 0,  0, 38,  0,  0])}, '262': {'preds': array([ 0,  0, 58,  0,  0])}, '263': {'preds': array([ 0,  0, 31,  0,  0])}, '264': {'preds': array([ 0,  0, 79,  0,  0])}, '265': {'preds': array([ 0,  0, 25,  0,  0])}, '266': {'preds': array([ 0,  0, 34,  0,  3])}, '267': {'preds': array([ 0,  0, 30,  0,  0])}, '268': {'preds': array([ 1,  0, 49,  0,  0])}, '269': {'preds': array([ 0,  0, 42,  1,  0])}, '26': {'preds': array([ 0,  0, 91,  0,  0])}, '270': {'preds': array([0, 0, 6, 0, 0])}, '271': {'preds': array([ 0,  0, 46,  0,  0])}, '272': {'preds': array([ 0,  0, 81,  1,  0])}, '273': {'preds': array([ 0,  0, 45,  0,  0])}, '274': {'preds': array([ 1,  0, 47,  0,  0])}, '275': {'preds': array([ 0,  0, 45,  0,  0])}, '276': {'preds': array([ 0,  0, 59,  0,  0])}, '277': {'preds': array([ 0,  0, 52,  0,  0])}, '278': {'preds': array([ 0,  0, 43,  0,  0])}, '279': {'preds': array([ 0,  0, 30,  1,  0])}, '27': {'preds': array([ 0,  0, 27,  0,  0])}, '280': {'preds': array([ 0,  0, 55,  0,  0])}, '281': {'preds': array([ 1,  0, 82,  0,  0])}, '282': {'preds': array([ 0,  0, 24,  0,  0])}, '283': {'preds': array([ 0,  0, 45,  0,  0])}, '284': {'preds': array([ 0,  0, 31,  0,  0])}, '285': {'preds': array([ 0,  0, 26,  0, 14])}, '286': {'preds': array([ 0,  2, 55,  0,  3])}, '287': {'preds': array([ 0,  0, 57,  0,  0])}, '288': {'preds': array([  2,  37, 289,   0,   9])}, '289': {'preds': array([0, 0, 7, 0, 0])}, '28': {'preds': array([ 0,  0, 78,  0,  0])}, '290': {'preds': array([ 1,  0, 48,  0,  0])}, '291': {'preds': array([10,  0, 35,  0,  0])}, '292': {'preds': array([ 0,  0, 45,  0,  0])}, '293': {'preds': array([ 0,  0, 32,  0,  0])}, '294': {'preds': array([13,  0, 48,  0,  0])}, '295': {'preds': array([ 3,  0, 91,  1,  0])}, '296': {'preds': array([ 0,  0, 37,  0,  0])}, '297': {'preds': array([ 0,  1, 72,  1,  1])}, '298': {'preds': array([ 3,  0, 56,  0,  1])}, '299': {'preds': array([ 0,  0, 65,  0,  1])}, '29': {'preds': array([ 0,  0, 49,  0,  0])}, '2': {'preds': array([ 1,  0, 55,  0,  0])}, '300': {'preds': array([64,  0, 24,  0,  0])}, '301': {'preds': array([ 1,  0, 24,  0,  0])}, '302': {'preds': array([ 0,  0, 27,  0,  0])}, '303': {'preds': array([ 0,  0, 55,  0,  0])}, '304': {'preds': array([ 0,  0, 34,  0,  0])}, '305': {'preds': array([ 3,  0, 26,  0,  0])}, '306': {'preds': array([ 0,  0, 16,  0,  0])}, '307': {'preds': array([ 0,  0, 24,  0,  0])}, '308': {'preds': array([ 0,  0, 32,  0,  0])}, '309': {'preds': array([ 1,  0, 45,  0,  0])}, '30': {'preds': array([ 0,  0, 25,  0,  0])}, '310': {'preds': array([ 0,  0, 52,  0,  0])}, '311': {'preds': array([ 0,  0, 55,  0,  0])}, '312': {'preds': array([ 0,  0, 49,  0,  0])}, '313': {'preds': array([ 0,  0, 71,  0,  0])}, '314': {'preds': array([ 0,  0, 54,  0,  0])}, '315': {'preds': array([0, 1, 4, 0, 0])}, '316': {'preds': array([ 0,  3, 60,  0,  0])}, '317': {'preds': array([ 0,  0, 46,  0,  0])}, '318': {'preds': array([ 0,  0, 41,  0,  0])}, '319': {'preds': array([ 0,  0, 33,  0,  0])}, '31': {'preds': array([ 0,  0, 29,  0,  0])}, '320': {'preds': array([ 0,  0, 40,  0,  0])}, '321': {'preds': array([ 0,  0, 69,  1,  0])}, '322': {'preds': array([ 0,  1, 63,  4,  0])}, '323': {'preds': array([ 0,  0, 42,  0,  0])}, '324': {'preds': array([ 0,  2, 80,  1,  1])}, '325': {'preds': array([ 0,  0, 46,  0,  1])}, '326': {'preds': array([ 0,  0, 45,  0,  0])}, '327': {'preds': array([ 0,  0, 50,  0,  0])}, '328': {'preds': array([ 0,  0, 83,  0,  0])}, '329': {'preds': array([ 0,  0, 54,  8,  0])}, '32': {'preds': array([  0,   0, 261,   1,   1])}, '330': {'preds': array([ 0,  0, 46,  0,  0])}, '331': {'preds': array([ 0,  0, 35,  0,  0])}, '332': {'preds': array([ 0,  0, 73,  0,  0])}, '333': {'preds': array([ 0,  0, 27,  0,  0])}, '334': {'preds': array([ 0,  0, 45,  0,  2])}, '335': {'preds': array([ 0,  0, 68,  0,  0])}, '336': {'preds': array([ 2,  0, 54,  7,  0])}, '337': {'preds': array([ 0,  0, 71,  3,  0])}, '338': {'preds': array([ 0,  0, 44,  0,  0])}, '339': {'preds': array([ 0,  0, 26,  0,  0])}, '33': {'preds': array([ 0,  0, 29,  0,  0])}, '340': {'preds': array([ 0,  0, 29,  0,  0])}, '341': {'preds': array([ 1,  0, 35,  0,  0])}, '342': {'preds': array([ 0,  0, 62,  0,  0])}, '343': {'preds': array([ 0,  0, 63,  0,  0])}, '344': {'preds': array([ 0,  0, 83,  0,  0])}, '345': {'preds': array([ 0,  0, 44,  0,  0])}, '346': {'preds': array([ 0,  0, 48,  0,  0])}, '347': {'preds': array([ 0,  0, 53,  0,  0])}, '348': {'preds': array([ 0,  0, 37,  0,  0])}, '349': {'preds': array([ 0,  0, 54,  0,  0])}, '34': {'preds': array([ 3,  0, 19,  0,  0])}, '350': {'preds': array([ 0,  0, 39,  0,  0])}, '351': {'preds': array([ 0,  0, 27,  0,  0])}, '352': {'preds': array([ 0,  0, 60,  1,  0])}, '353': {'preds': array([ 0,  0, 57,  0,  0])}, '354': {'preds': array([ 0,  0, 48,  0,  0])}, '355': {'preds': array([ 0,  0, 34,  0,  0])}, '356': {'preds': array([ 0,  0, 43,  0,  0])}, '357': {'preds': array([ 0,  0, 28,  0,  0])}, '358': {'preds': array([ 0,  0, 35,  0,  0])}, '359': {'preds': array([ 0,  0, 13,  0,  0])}, '35': {'preds': array([ 3,  0, 24,  0,  0])}, '360': {'preds': array([ 0,  0, 40,  0,  0])}, '361': {'preds': array([ 0,  0, 39,  0,  0])}, '362': {'preds': array([ 0,  0, 21,  0,  0])}, '363': {'preds': array([ 0,  0, 39,  0,  0])}, '364': {'preds': array([ 0,  0, 46,  0,  0])}, '365': {'preds': array([ 0,  1, 75,  0,  0])}, '366': {'preds': array([ 0,  0, 27,  0,  0])}, '367': {'preds': array([ 0,  0, 31,  0,  0])}, '368': {'preds': array([ 0,  0, 52,  0,  0])}, '369': {'preds': array([ 0,  0, 43,  0,  0])}, '36': {'preds': array([ 0,  0, 76,  0,  0])}, '370': {'preds': array([ 0,  0, 85,  0,  0])}, '371': {'preds': array([ 0, 64, 88,  2, 24])}, '372': {'preds': array([ 0,  0, 27,  0,  0])}, '373': {'preds': array([ 0,  0, 20,  0,  0])}, '374': {'preds': array([ 0,  0, 39,  0,  0])}, '375': {'preds': array([ 0,  0, 34,  0,  0])}, '376': {'preds': array([ 0,  0, 28,  0,  0])}, '377': {'preds': array([ 0,  0, 60,  0,  0])}, '378': {'preds': array([ 0,  0, 43,  0,  0])}, '379': {'preds': array([ 0,  0, 16,  1,  1])}, '37': {'preds': array([ 0,  0, 27,  0,  0])}, '380': {'preds': array([ 0,  0, 44,  0,  0])}, '381': {'preds': array([ 0,  0, 49,  0,  0])}, '382': {'preds': array([ 0,  0, 23,  0,  0])}, '383': {'preds': array([ 0,  0, 22,  0,  0])}, '384': {'preds': array([ 0,  0, 56,  0,  0])}, '385': {'preds': array([ 1,  0, 33,  0,  0])}, '386': {'preds': array([ 0,  0, 42,  0,  0])}, '387': {'preds': array([ 0,  0, 42,  0,  0])}, '388': {'preds': array([ 0,  0, 41,  0,  0])}, '389': {'preds': array([ 1,  0, 51,  0,  0])}, '38': {'preds': array([ 0,  0, 43,  0,  0])}, '390': {'preds': array([ 1,  0, 40,  0,  0])}, '391': {'preds': array([ 0,  0, 83,  0,  0])}, '392': {'preds': array([ 0,  2, 98, 11,  5])}, '393': {'preds': array([ 0,  0, 52,  0,  0])}, '394': {'preds': array([ 0,  1, 59,  0,  2])}, '395': {'preds': array([ 0,  0, 24,  0,  0])}, '396': {'preds': array([ 0,  0, 85,  0,  0])}, '397': {'preds': array([ 0,  0, 63,  0,  0])}, '398': {'preds': array([ 2,  0, 13,  2,  0])}, '399': {'preds': array([ 0,  0, 36,  0,  0])}, '39': {'preds': array([ 0,  0, 29,  0,  0])}, '3': {'preds': array([ 0,  0, 37,  0,  0])}, '400': {'preds': array([32,  0, 36,  0,  0])}, '401': {'preds': array([ 0,  3, 50,  2,  0])}, '402': {'preds': array([ 0,  0, 64,  0,  0])}, '403': {'preds': array([ 0,  0, 31,  0,  0])}, '404': {'preds': array([ 7,  0, 62,  0,  5])}, '405': {'preds': array([ 7,  0, 32,  0,  0])}, '406': {'preds': array([ 0,  0, 48,  0,  0])}, '407': {'preds': array([ 0,  0, 50,  0,  0])}, '408': {'preds': array([ 0,  0, 34,  0,  0])}, '409': {'preds': array([ 1,  0, 62,  0,  0])}, '40': {'preds': array([ 1,  0, 38,  0,  0])}, '410': {'preds': array([ 0,  0, 25,  0,  0])}, '41': {'preds': array([ 0,  0, 43,  0,  0])}, '42': {'preds': array([ 0,  0, 38,  0,  0])}, '43': {'preds': array([ 0,  0, 38,  0,  0])}, '44': {'preds': array([ 0,  0, 35,  0,  0])}, '45': {'preds': array([ 0,  0, 43,  0,  0])}, '46': {'preds': array([ 0,  0, 29,  0,  0])}, '47': {'preds': array([ 0,  0, 18,  0,  0])}, '48': {'preds': array([ 0,  0, 44,  0,  1])}, '49': {'preds': array([ 0,  0, 37,  0,  0])}, '4': {'preds': array([ 0,  0, 38,  1,  0])}, '50': {'preds': array([ 1,  0, 66,  0,  0])}, '51': {'preds': array([ 0,  0, 56,  0,  0])}, '52': {'preds': array([ 0,  0, 46,  1,  0])}, '53': {'preds': array([ 1,  0, 54,  0,  0])}, '54': {'preds': array([ 0,  0, 28,  0,  0])}, '55': {'preds': array([ 1,  0, 31,  0,  0])}, '56': {'preds': array([ 0,  0, 27,  0,  0])}, '57': {'preds': array([ 0,  0, 57,  0,  0])}, '58': {'preds': array([ 0,  0, 40,  0,  0])}, '59': {'preds': array([ 0,  0, 43,  0,  0])}, '5': {'preds': array([ 0,  0, 25,  0,  0])}, '60': {'preds': array([ 0,  0, 74,  1,  0])}, '61': {'preds': array([ 0,  0, 20,  0,  0])}, '62': {'preds': array([ 0,  0, 79,  0,  0])}, '63': {'preds': array([ 0,  0, 30,  1,  0])}, '64': {'preds': array([ 0,  0, 52,  0,  0])}, '65': {'preds': array([ 0,  0, 43,  0,  0])}, '66': {'preds': array([ 0,  0, 32,  0,  0])}, '67': {'preds': array([ 1,  0, 22,  0,  0])}, '68': {'preds': array([ 3,  0, 26,  0,  0])}, '69': {'preds': array([ 0,  0, 60,  1,  0])}, '6': {'preds': array([ 1,  0, 34,  0,  0])}, '70': {'preds': array([ 1,  0, 56,  0,  0])}, '71': {'preds': array([ 0,  0, 19,  0,  0])}, '72': {'preds': array([ 0,  0, 37,  0,  0])}, '73': {'preds': array([ 0,  0, 44,  0,  0])}, '74': {'preds': array([ 0,  0, 47,  0,  0])}, '75': {'preds': array([ 2,  0, 86,  0,  1])}, '76': {'preds': array([ 0,  0, 38,  0,  0])}, '77': {'preds': array([ 0,  0, 36,  0,  0])}, '78': {'preds': array([ 0,  4, 54,  0,  0])}, '79': {'preds': array([ 0,  0, 55,  0,  0])}, '7': {'preds': array([ 0,  0, 47,  0,  0])}, '80': {'preds': array([ 0,  0, 23,  0,  0])}, '81': {'preds': array([ 0,  0, 14,  0,  0])}, '82': {'preds': array([ 0,  0, 45,  0,  0])}, '83': {'preds': array([ 3,  0, 36,  0,  0])}, '84': {'preds': array([ 0,  0, 45,  0,  0])}, '85': {'preds': array([ 0,  0, 22,  0,  0])}, '86': {'preds': array([ 0,  0, 96,  0,  0])}, '87': {'preds': array([ 0,  0, 24,  0,  0])}, '88': {'preds': array([ 0,  0, 26,  0,  0])}, '89': {'preds': array([ 1,  0, 64,  0,  0])}, '8': {'preds': array([ 0,  0, 50,  0,  0])}, '90': {'preds': array([ 0,  0, 34,  0,  0])}, '91': {'preds': array([ 1,  0, 65,  0,  0])}, '92': {'preds': array([ 0,  0, 52,  0,  0])}, '93': {'preds': array([ 0,  0, 32,  0,  0])}, '94': {'preds': array([ 0,  0, 35,  0,  0])}, '95': {'preds': array([ 2,  0, 18,  0,  0])}, '96': {'preds': array([12,  0, 13,  2,  8])}, '97': {'preds': array([ 0,  0, 33,  0,  0])}, '98': {'preds': array([ 0,  0, 45,  0,  2])}, '99': {'preds': array([ 0,  0, 16,  0,  0])}, '9': {'preds': array([ 0,  0, 19,  0,  0])}}\n",
            "{'a_tribe_called_quest-bonita_applebum': [1, 3], 'aaron_neville-tell_it_like_it_is': [1, 3], 'aerosmith-dude_looks_like_a_lady': [2, 3], 'alicia_keys-fallin': [3], 'american_music_club-jesus_hands': [1, 3], 'andrews_sisters-boogie_woogie_bugle_boy': [2, 3], 'animals-im_crying': [3], 'anita_baker-caught_up_in_the_rapture': [1, 2, 3], 'antonio_carlos_jobim-wave': [2, 3], 'arlo_guthrie-alices_restaurant_massacree': [1, 2], 'b.b._king-sweet_little_angel': [1, 3], 'band-king_harvest_has_surely_come': [2], 'barry_white-cant_get_enough_of_your_love_babe': [3], 'bay_city_rollers-saturday_night': [2, 3], 'beach_boys-i_get_around': [3], 'beautiful_south-one_last_love_song': [1, 3, 4], 'belle_and_sebastian-like_dylan_in_the_movies': [1], 'blood_sweat_and_tears-sometimes_in_winter': [1, 3], 'bo_diddley-you_cant_judge_a_book_by_its_cover': [2, 3], 'boogie_down_productions-the_bridge_is_over': [0], 'boston-more_than_a_feeling': [3], 'burnshee_thornside-goodbye_on_a_beautiful_day': [3], 'charles_mingus-mood_indigo': [1, 4], 'contours-do_you_love_me': [2, 3], 'counting_crows-speedway': [1, 3, 4], 'crosby_stills_nash_and_young-teach_your_children': [1], 'dionne_warwick-walk_on_by': [1, 3, 4], 'donovan-catch_the_wind': [3], 'eric_clapton-wonderful_tonight': [1, 3], 'franz_ferdinand-come_on_home': [2, 3], 'gloria_gaynor-i_will_survive': [2, 3], 'james_taylor-fire_and_rain': [1, 3], 'kourosh_zolani-peaceful_planet': [1, 3], 'men_at_work-who_can_it_be_now': [2, 3], 'natalie_imbruglia-torn': [3, 4], 'neutral_milk_hotel-where_youll_find_me_now': [1], 'pink_floyd-echoes': [1, 3], 'randy_newman-sail_away': [1, 3], 'ronettes-walking_in_the_rain': [2, 3], 'sly_and_the_family_stone-just_like_a_baby': [1, 3], 'smithereens-behind_the_wall_of_sleep': [3], 'smokey_robinson_and_the_miracles-ooo_baby_baby': [1, 3], 'solace-laz_7_8': [3], 'sonny_rollins-strode_rode': [2, 3], 'soul_ii_soul-keep_on_movin': [1], 'spinecar-stay': [0], 'strawbs-new_world': [0], 'sundays-heres_where_the_story_ends': [1, 2], 'syreeta-what_love_has_joined_together': [1, 2, 3], 'temptations-since_i_lost_my_baby': [1, 3], 'thursday_group-like_white_on_rice': [3, 4], 'white_stripes-hotel_yorba': [2, 3], 'who-bargain': [3], '2pac-trapped': [0], 'ac_dc-dirty_deeds_done_dirt_cheap': [0, 3], 'aphex_twin-come_to_daddy': [0], 'bomb_the_bass-bug_powder_dust': [0], 'cheap_trick-dream_police': [2], 'hybris-hate': [0], 'skitzo-last_depression': [0, 3], 'soulprint-crawlspace': [0, 3], 'van_halen-aint_talkin_bout_love': [3], 'beck-where_its_at': [2], 'duran_duran-come_undone': [1, 3, 4], 'u2-hold_me_thrill_me_kiss_me_kill_me': [3], 'big_star-in_the_street': [2], 'cat_power-he_war': [4], 'specials-gangsters': [2, 3], 'urge_overkill-sister_havana': [3], 'ivilion-d_b_l': [0], 'moby-porcelain': [1, 3], 'mr_gelatine-knysnamushrooms': [3], 'led_zeppelin-immigrant_song': [0, 3], 'bee_gees-stayin_alive': [2, 3], 'britney_spears-im_a_slave_for_you': [2, 3], 'erasure-chains_of_love': [2, 3], 'jamiroquai-little_l': [2, 3], 'madonna-ray_of_light': [2, 3], 'police-every_little_thing_she_does_is_magic': [2, 3], 'propellerheads-take_california': [2], 'steppenwolf-born_to_be_wild': [2, 3], '10cc-for_you_and_i': [1, 3], 'ashford_and_simpson-solid': [2, 3], 'zombies-beechwood_park': [1, 2, 3], 'alice_cooper-elected': [3], 'cardigans-lovefool': [2, 3], 'young_mc-bust_a_move': [3], 'pet_shop_boys-being_boring': [1], '5th_dimension-one_less_bell_to_answer': [1, 3, 4], 'adam_ant-wonderful': [3], 'al_green-sha-la-la_make_me_happy': [1, 2, 3], 'ani_difranco-crime_for_crime': [0, 3], 'apples_in_stereo-glowworm': [3], 'aretha_franklin-dont_play_that_song': [3], 'art_tatum-willow_weep_for_me': [1, 2], 'beatles-strawberry_fields_forever': [1], 'black_crowes-thorn_in_my_pride': [3], 'blue_yster_cult-burnin_for_you': [2, 3], 'bob_dylan-ill_be_your_baby_tonight': [1, 3], 'bob_marley_and_the_wailers-three_little_birds': [1, 2, 3], 'bobby_womack-womans_gotta_have_it': [1, 2, 3], 'bon_jovi-livin_on_a_prayer': [3], 'buddy_holly-peggy_sue': [3], 'built_to_spill-i_would_hurt_a_fly': [3, 4], 'buzzcocks-everybodys_happy_nowadays': [2, 3], 'cab_calloway-minnie_the_moocher': [3], 'carole_king-youve_got_a_friend': [1, 2, 3], 'chantal_kreviazuk-surrounded': [3, 4], 'charlie_parker-ornithology': [1, 2, 3], 'cilla_black-alfie': [1, 3], 'coldplay-clocks': [1, 3], 'cowboy_junkies-postcard_blues': [1, 4], 'cranberries-linger': [1, 3], 'david_bowie-song_for_bob_dylan': [2], 'diana_ross_and_the_supremes-where_did_our_love_go': [1, 2, 3], 'doors-touch_me': [2, 3], 'electric_frankenstein-teenage_shutdown': [0, 3], 'elton_john-tiny_dancer': [1, 3], 'evanescence-my_immortal': [3, 4], 'falik-bliss': [1, 3], 'fiona_apple-love_ridden': [1, 3, 4], 'foo_fighters-big_me': [1, 3], 'guns_n_roses-november_rain': [3], 'james_brown-give_it_up_or_turnit_a_loose': [2], 'jamie_janover-event_horizon': [1, 3], 'junior_murvin-police_and_thieves': [3], 'leonard_cohen-suzanne': [1], 'los_lobos-corrido_1': [2], 'love-you_set_the_scene': [3], 'macy_gray-i_try': [1, 4], 'michael_jackson-billie_jean': [3], 'r.e.m.-camera': [1, 3], 'scott_hill-silk_road': [1], 'spiritualized-stop_your_crying': [1, 3], 'stereolab-cybeles_reverie': [1, 2, 3], 't._rex-children_of_the_revolution': [0], 'talking_heads-and_she_was': [3], 'throwing_muses-hate_my_way': [0], 'tim_hardin-dont_make_promises': [1, 3], 'tom_paul-little_part_of_me': [3, 4], 'tom_waits-time': [1, 4], 'troggs-wild_thing': [2, 3], 'wicked_allstars-happy': [1, 2, 3], 'wilco-kingpin': [3], 'yeah_yeah_yeahs-maps': [1, 3], 'black_sabbath-black_sabbath': [0, 4], 'll_cool_j-mama_said_knock_you_out': [0], 'muddy_waters-mannish_boy': [0, 2, 3], 'sex_pistols-pretty_vacant': [0], 'kokoon-order': [3], 'version-universal_humans': [1], 'xtc-love_at_first_sight': [2], 'alanis_morissette-thank_u': [3, 4], 'arthur_yoria-at_least_you_ve_been_told': [3], 'no_doubt-artificial_sweetener': [0, 3], 'roots_of_rebellion-legend': [3], 'third_eye_blind-semi-charmed_life': [3], 'introspekt-tbd': [3], 'psychetropic-dead_slow_day': [1], 'sir_mix-a-lot-baby_got_back': [0], 'cc_music_factory-gonna_make_you_sweat_everybody_dance_now': [2], 'daft_punk-da_funk': [3], 'dj_jazzy_jeff_and_the_fresh_prince-summertime': [2, 3], 'jackson_5-abc': [2, 3], 'rocket_city_riot-mine_tonite': [3], 'brian_eno-here_come_the_warm_jets': [2], 'massive_attack-risingson': [3], 'chuck_berry-roll_over_beethoven': [2, 3], 'mr_epic-ruff_and_tumble': [1, 3], 'muddy_waters-im_ready': [1], 'chad_and_jeremy-before_and_after': [1, 4], 'mrdc-leaving': [1], 'antiguru-peering': [1, 2, 3], 'drop_trio-slapjack': [1, 2], 'tilopa-kyo_rei': [1, 3], 'billy_bragg-jeane': [4], 'cream-tales_of_brave_ulysses': [4], 'alice_in_chains-no_excuses': [4], 'allman_brothers_band-melissa': [1, 3], 'barry_manilow-mandy': [1, 3], 'billie_holiday-god_bless_the_child': [1, 3], 'blur-country_house': [2, 3], 'brenton_wood-lovey_dovey_kind_of_love': [1, 2, 3], 'buena_vista_social_club-el_cuarto_de_tula': [2, 3], 'cheryl_ann_fulton-marsh_of_rhuddlan': [1, 2, 3], 'chet_baker-these_foolish_things': [1, 3], 'chumbawamba-tubthumping': [2, 3], 'count_basie-lester_leaps_in': [1, 2, 3], 'creedence_clearwater_revival-travelin_band': [2, 3], 'crosby_stills_and_nash-guinnevere': [1, 3], 'django_reinhardt-brazil': [1, 2], 'duke_ellington_and_his_orchestra-caravan': [2, 3], 'eagles-tequila_sunrise': [1, 4], 'elvis_costello-less_than_zero': [3], 'elvis_presley-heartbreak_hotel': [3, 4], 'etherine-never_leave': [1, 3], 'four_stones-brilliant_day_eine_kleine_mix': [3], 'frank_sinatra-fly_me_to_the_moon': [1, 2, 3], 'genesis-cuckoo_cocoon': [1], 'howlin_wolf-moanin_at_midnight': [4], 'interpol-stella_was_a_diver_and_she_was_always_down': [4], 'jan_and_dean-surf_city': [1, 2, 3], 'lisa_debenedictis-fruitless': [3], 'marvelettes-please_mr._postman': [4], 'mary_wells-my_guy': [2], 'neil_young-razor_love': [1, 3], 'norah_jones-dont_know_why': [1, 3], 'otis_redding-mr._pitiful': [3], 'psychedelic_furs-love_my_way': [3], 'radiohead-karma_police': [1, 4], 'robert_johnson-sweet_home_chicago': [3], 'rolling_stones-little_by_little': [3], 'rubn_gonzlez-cumbanchero': [2, 3], 'rufus_wainwright-cigarettes_and_chocolate_milk': [1, 3], 'sarah_mclachlan-possession': [1, 3], 'sebadoh-soul_and_fire': [3, 4], 'shane_jackman-set_fire_to_the_city': [1, 3, 4], 'shins-new_slang': [1, 3, 4], 'stan_getz-corcovado_quiet_nights_of_quiet_stars': [1, 3], 'stone_roses-i_wanna_be_adored': [3], 'very_large_array-psychedelic_baby': [3], 'war-all_day_music': [1, 2], 'black_flag-six_pack': [0, 3], 'bush-comedown': [0], 'dr._dre-nuthin_but_a_g_thang': [3], 'faith_no_more-epic': [0], 'jackalopes-rotgut': [3], 'modest_mouse-what_people_are_made_of': [0, 3], 'nine_inch_nails-head_like_a_hole': [0], 'nirvana-aneurysm': [0, 3], 'nelly-country_grammar': [2], 'zapp-dance_floor': [2, 3], 'birthday_party-mr._clarinet': [0, 4], 'elliott_smith-baby_britain': [1, 2, 3], 'germs-lexicon_devil': [3], 'teenage_fanclub-the_concept': [3], 'weezer-buddy_holly': [2, 3], 'aerobic_jonquil-sweat_machine': [1, 3], 'kenji_williams-i_m_alive': [1], 'saros-prelude': [3], 'adverts-gary_gilmores_eyes': [0], 'dire_straits-money_for_nothing': [2], 'doobie_brothers-china_grove': [2, 3], 'gin_blossoms-hey_jealousy': [3, 4], 'jefferson_airplane-somebody_to_love': [3], 'rick_james-super_freak': [2], 'squarepusher-a_journey_to_reedham_(7am_mix)': [2], 'sylvester-you_make_me_feel_mighty_real': [2, 3], 'todd_rundgren-bang_the_drum_all_day': [2, 3], 'cars-good_times_roll': [2, 3], 'christina_aguilera-genie_in_a_bottle': [2, 3], 'ramones-i_just_want_to_have_something_to_do': [2, 3], 'gene_clark-the_true_one': [1, 3, 4], 'go-gos-vacation': [2, 3], 'dido-here_with_me': [1, 3], 'starship-nothings_gonna_stop_us_now': [3], 'booker_t._and_the_mgs-time_is_tight': [2, 3], 'anup-life_glides': [2], 'atomic_opera-watergrave': [3], 'backstreet_boys-as_long_as_you_love_me': [1, 3], 'badly_drawn_boy-all_possibilities': [2, 3], 'bonnie_tyler-total_eclipse_of_the_heart': [3, 4], 'bread-if': [1, 3], 'buffalo_springfield-for_what_its_worth': [1], 'carly_simon-youre_so_vain': [3], 'carpenters-rainy_days_and_mondays': [1, 3, 4], 'catherine_wheel-black_metallic': [1, 3], 'charlie_rich-behind_closed_doors': [1, 2, 3], 'chi-lites-stoned_out_of_my_mind': [1, 2, 3], 'chicago-if_you_leave_me_now': [1, 3, 4], 'chris_juergensen-prospects': [1, 3], 'church-under_the_milky_way': [3], 'curandero-aras': [3], 'cure-just_like_heaven': [3], 'curtis_mayfield-move_on_up': [2, 3], 'dave_matthews_band-ants_marching': [1, 2, 3], 'drevo-our_watcher_show_us_the_way': [1], 'faith_hill-lets_make_love': [3], 'george_harrison-all_things_must_pass': [1, 3], 'glenn_miller-in_the_mood': [1, 2, 3], 'hall_and_oates-private_eyes': [3], 'jay_kishor-raga_malgunji_jor': [1, 3], 'jeff_buckley-last_goodbye': [3], 'john_coltrane-giant_steps': [2], 'live-lightning_crashes': [4], 'lynyrd_skynyrd-sweet_home_alabama': [3], 'madness-baggy_trousers': [2], 'mazzy_star-fade_into_you': [1, 3, 4], 'myles_cochran-getting_stronger': [1, 3], 'paul_mccartney-ebony_and_ivory': [1, 2, 3], 'portishead-all_mine': [1, 4], 'pretenders-day_after_day': [2, 3], 'queen-we_will_rock_you': [2], 'santana-love_of_my_life': [3], 'stevie_wonder-for_once_in_my_life': [2, 3], 'syd_barrett-effervescing_elephant': [2], 'tim_buckley-morning_glory': [1, 4], 'tommy_james_and_the_shondells-i_think_were_alone_now': [2, 3], 'velvet_underground-new_age': [3], 'wes_montgomery-bumpin': [1, 3], 'yakshi-chandra': [1], 'captain_beefheart_and_the_magic_band-safe_as_milk': [0], 'dead_kennedys-chemical_warfare': [0], 'green_day-longview': [3], 'mc_hammer-u_cant_touch_this': [2, 3], 'pantera-becoming': [0], 'superchunk-slack_motherfucker': [0, 3], 'utopia_banished-by_mourning': [0, 4], 'dj_markitos-sunset_138_bpm_remix': [3], 'somadrone-coda': [0, 4], 'bjrk-army_of_me': [0], 'darkness-i_believe_in_a_thing_called_love': [3], 'liz_phair-supernova': [0, 3], 'morrissey-everyday_is_like_sunday': [1], 'processor-nibtal_7': [0], 'john_cale-pablo_picasso': [0, 4], 'napoleon_blown_aparts-higher_education': [0, 3], 'altered_images-dont_talk_to_me_about_love': [2], 'barbara_leoni-don_t_rain_on_my_parade': [2, 3], 'bobby_brown-my_prerogative': [2], 'boo_radleys-wake_up_boo': [2], 'def_leppard-pour_some_sugar_on_me': [3], 'vapors-turning_japanese': [2], 'whitney_houston-how_will_i_know': [3], 'mott_the_hoople-roll_away_the_stone': [2], 'shuggie_otis-sweet_thang': [1, 3], 'tom_petty_and_the_heartbreakers-dont_come_around_here_no_more': [1, 3], 'michael_masley-advice_from_the_angel_of_thresholds': [1], 'bruce_springsteen-badlands': [2, 3], 'abba-s.o.s.': [2, 3], 'aimee_mann-wise_up': [1, 3], 'association-windy': [2, 3], 'ben_folds_five-brick': [1, 3, 4], 'billy_joel-we_didnt_start_the_fire': [2], 'bob_seger-turn_the_page': [1, 3, 4], 'bobby_fuller_four-i_fought_the_law': [3], 'cake-perhaps,_perhaps,_perhaps': [4], 'carl_perkins-matchbox': [3], 'chic-le_freak': [2, 3], 'duncan_sheik-barely_breathing': [3], 'earth_wind_and_fire-september': [2, 3], 'everly_brothers-take_a_message_to_mary': [1, 3, 4], 'flying_burrito_brothers-break_my_mind': [3], 'gram_parsons-1000_wedding': [3], 'ike_and_tina_turner-river_deep_mountain_high': [3], 'jacques_brel-les_vieux': [1], 'jade_leary-going_in': [3], 'janes_addiction-been_caught_stealing': [2], 'jerry_lee_lewis-great_balls_of_fire': [2, 3], 'jewel-enter_from_the_east': [1, 4], 'jimi_hendrix-highway_chile': [3], 'john_lee_hooker-boom_boom': [1, 2], 'johnny_cash-the_man_comes_around': [1], 'kool_and_the_gang-funky_stuff': [2], 'lambert_hendricks_and_ross-gimme_that_wine': [2, 3], 'lou_reed-walk_on_the_wild_side': [1], 'louis_armstrong-hotter_than_that': [2], 'manassas-bound_to_fall': [2, 3], 'marvin_gaye-whats_going_on': [1, 3], 'miles_davis-blue_in_green': [1, 3, 4], 'monkees-a_little_bit_me_a_little_bit_you': [2, 3], 'mose_allison-monsters_of_the_id': [4], 'neil_young_and_crazy_horse-western_hero': [1, 3, 4], 'ojays-livin_for_the_weekend': [2, 3], 'panacea-dragaicuta': [4], 'pearl_jam-yellow_ledbetter': [3, 4], 'sade-smooth_operator': [1, 3], 'screaming_trees-nearly_lost_you': [3], 'sheryl_crow-i_shall_believe': [1, 3], 'shira_kammen-music_of_waters': [1, 3], 'simon_and_garfunkel-the_only_living_boy_in_new_york': [1, 3, 4], 'smashing_pumpkins-rocket': [3], 'smokey_robinson-cruisin': [1, 3], 'spice_girls-stop': [2, 3], 'sting-big_lie_small_world': [1, 3], 'style_council-headstart_for_happiness': [2, 3], 'van_morrison-and_it_stoned_me': [1, 3], 'young_rascals-baby_lets_wait': [3, 4], 'pizzle-what_s_wrong_with_my_footm': [0, 3], 'rage_against_the_machine-maggies_farm': [0, 3], 'traffic-pearly_queen': [0], 'air-sexy_boy': [1], 'emma_s_mini-lost': [3], 'garbage-hammering_in_my_head': [0], 'primus-jerry_was_a_race_car_driver': [0], 'west_exit-nocturne': [3], 'clash-lost_in_the_supermarket': [3], 'oasis-supersonic': [3], 'red_hot_chili_peppers-give_it_away': [2], 'they_might_be_giants-i_should_be_allowed_to_think': [1, 3], 'uncle_tupelo-the_long_cut': [1, 3], 'cargo_cult-garden': [0], 'busta_rhymes-woo_hah_got_you_all_in_check': [0, 3], 'buffalo_springfield-mr._soul': [3], 'janet_jackson-miss_you_much': [2, 3], 'kiss-deuce': [3], 'shakira-the_one': [3], 'spencer_davis_group-gimme_some_lovin': [2, 3], 'squeeze-pulling_mussels_from_the_shell': [2], 'stevie_ray_vaughan-pride_and_joy': [2, 3], 'turtles-elenore': [2], 'arthur_alexander-you_dont_care': [3], 'cyndi_lauper-money_changes_everything': [3], 'dennis_brown-tribulation': [1, 3], 'fleetwood_mac-say_you_love_me': [1, 3], 'mamas_and_the_papas-words_of_love': [4], 'sweet-fox_on_the_run': [2, 3], 'tom_petty-i_wont_back_down': [3], 'yes-leave_it': [3], 'tears_for_fears-everybody_wants_to_rule_the_world': [1, 2, 3], 'monoide-golden_key': [1, 2, 3], 'touchinggrace-wild_spring_apples': [1, 2, 3]}\n",
            "['10cc-for_you_and_i', '2pac-trapped', '5th_dimension-one_less_bell_to_answer', 'a_tribe_called_quest-bonita_applebum', 'aaron_neville-tell_it_like_it_is', 'abba-s.o.s.', 'ac_dc-dirty_deeds_done_dirt_cheap', 'adam_ant-wonderful', 'adverts-gary_gilmores_eyes', 'aerobic_jonquil-sweat_machine', 'aerosmith-dude_looks_like_a_lady', 'aimee_mann-wise_up', 'air-sexy_boy', 'al_green-sha-la-la_make_me_happy', 'alanis_morissette-thank_u', 'alice_cooper-elected', 'alice_in_chains-no_excuses', 'alicia_keys-fallin', 'allman_brothers_band-melissa', 'altered_images-dont_talk_to_me_about_love', 'american_music_club-jesus_hands', 'andrews_sisters-boogie_woogie_bugle_boy', 'ani_difranco-crime_for_crime', 'animals-im_crying', 'anita_baker-caught_up_in_the_rapture', 'antiguru-peering', 'antonio_carlos_jobim-wave', 'anup-life_glides', 'aphex_twin-come_to_daddy', 'apples_in_stereo-glowworm', 'aretha_franklin-dont_play_that_song', 'arlo_guthrie-alices_restaurant_massacree', 'art_tatum-willow_weep_for_me', 'arthur_alexander-you_dont_care', 'arthur_yoria-at_least_you_ve_been_told', 'ashford_and_simpson-solid', 'association-windy', 'atomic_opera-watergrave', 'b.b._king-sweet_little_angel', 'backstreet_boys-as_long_as_you_love_me', 'badly_drawn_boy-all_possibilities', 'band-king_harvest_has_surely_come', 'barbara_leoni-don_t_rain_on_my_parade', 'barry_manilow-mandy', 'barry_white-cant_get_enough_of_your_love_babe', 'bay_city_rollers-saturday_night', 'beach_boys-i_get_around', 'beatles-strawberry_fields_forever', 'beautiful_south-one_last_love_song', 'beck-where_its_at', 'bee_gees-stayin_alive', 'belle_and_sebastian-like_dylan_in_the_movies', 'ben_folds_five-brick', 'big_star-in_the_street', 'billie_holiday-god_bless_the_child', 'billy_bragg-jeane', 'billy_joel-we_didnt_start_the_fire', 'birthday_party-mr._clarinet', 'bjrk-army_of_me', 'black_crowes-thorn_in_my_pride', 'black_flag-six_pack', 'black_sabbath-black_sabbath', 'blood_sweat_and_tears-sometimes_in_winter', 'blue_yster_cult-burnin_for_you', 'blur-country_house', 'bo_diddley-you_cant_judge_a_book_by_its_cover', 'bob_dylan-ill_be_your_baby_tonight', 'bob_marley_and_the_wailers-three_little_birds', 'bob_seger-turn_the_page', 'bobby_brown-my_prerogative', 'bobby_fuller_four-i_fought_the_law', 'bobby_womack-womans_gotta_have_it', 'bomb_the_bass-bug_powder_dust', 'bon_jovi-livin_on_a_prayer', 'bonnie_tyler-total_eclipse_of_the_heart', 'boo_radleys-wake_up_boo', 'boogie_down_productions-the_bridge_is_over', 'booker_t._and_the_mgs-time_is_tight', 'boston-more_than_a_feeling', 'bread-if', 'brenton_wood-lovey_dovey_kind_of_love', 'brian_eno-here_come_the_warm_jets', 'britney_spears-im_a_slave_for_you', 'bruce_springsteen-badlands', 'buddy_holly-peggy_sue', 'buena_vista_social_club-el_cuarto_de_tula', 'buffalo_springfield-for_what_its_worth', 'buffalo_springfield-mr._soul', 'built_to_spill-i_would_hurt_a_fly', 'burnshee_thornside-goodbye_on_a_beautiful_day', 'bush-comedown', 'busta_rhymes-woo_hah_got_you_all_in_check', 'buzzcocks-everybodys_happy_nowadays', 'cab_calloway-minnie_the_moocher', 'cake-perhaps,_perhaps,_perhaps', 'captain_beefheart_and_the_magic_band-safe_as_milk', 'cardigans-lovefool', 'cargo_cult-garden', 'carl_perkins-matchbox', 'carly_simon-youre_so_vain', 'carole_king-youve_got_a_friend', 'carpenters-rainy_days_and_mondays', 'cars-good_times_roll', 'cat_power-he_war', 'catherine_wheel-black_metallic', 'cc_music_factory-gonna_make_you_sweat_everybody_dance_now', 'chad_and_jeremy-before_and_after', 'chantal_kreviazuk-surrounded', 'charles_mingus-mood_indigo', 'charlie_parker-ornithology', 'charlie_rich-behind_closed_doors', 'cheap_trick-dream_police', 'cheryl_ann_fulton-marsh_of_rhuddlan', 'chet_baker-these_foolish_things', 'chi-lites-stoned_out_of_my_mind', 'chic-le_freak', 'chicago-if_you_leave_me_now', 'chris_juergensen-prospects', 'christina_aguilera-genie_in_a_bottle', 'chuck_berry-roll_over_beethoven', 'chumbawamba-tubthumping', 'church-under_the_milky_way', 'cilla_black-alfie', 'clash-lost_in_the_supermarket', 'coldplay-clocks', 'contours-do_you_love_me', 'count_basie-lester_leaps_in', 'counting_crows-speedway', 'cowboy_junkies-postcard_blues', 'cranberries-linger', 'cream-tales_of_brave_ulysses', 'creedence_clearwater_revival-travelin_band', 'crosby_stills_and_nash-guinnevere', 'crosby_stills_nash_and_young-teach_your_children', 'curandero-aras', 'cure-just_like_heaven', 'curtis_mayfield-move_on_up', 'cyndi_lauper-money_changes_everything', 'daft_punk-da_funk', 'darkness-i_believe_in_a_thing_called_love', 'dave_matthews_band-ants_marching', 'david_bowie-song_for_bob_dylan', 'dead_kennedys-chemical_warfare', 'def_leppard-pour_some_sugar_on_me', 'dennis_brown-tribulation', 'diana_ross_and_the_supremes-where_did_our_love_go', 'dido-here_with_me', 'dionne_warwick-walk_on_by', 'dire_straits-money_for_nothing', 'dj_jazzy_jeff_and_the_fresh_prince-summertime', 'dj_markitos-sunset_138_bpm_remix', 'django_reinhardt-brazil', 'donovan-catch_the_wind', 'doobie_brothers-china_grove', 'doors-touch_me', 'dr._dre-nuthin_but_a_g_thang', 'drevo-our_watcher_show_us_the_way', 'drop_trio-slapjack', 'duke_ellington_and_his_orchestra-caravan', 'duncan_sheik-barely_breathing', 'duran_duran-come_undone', 'eagles-tequila_sunrise', 'earth_wind_and_fire-september', 'electric_frankenstein-teenage_shutdown', 'elliott_smith-baby_britain', 'elton_john-tiny_dancer', 'elvis_costello-less_than_zero', 'elvis_presley-heartbreak_hotel', 'emma_s_mini-lost', 'erasure-chains_of_love', 'eric_clapton-wonderful_tonight', 'etherine-never_leave', 'evanescence-my_immortal', 'everly_brothers-take_a_message_to_mary', 'faith_hill-lets_make_love', 'faith_no_more-epic', 'falik-bliss', 'fiona_apple-love_ridden', 'fleetwood_mac-say_you_love_me', 'flying_burrito_brothers-break_my_mind', 'foo_fighters-big_me', 'four_stones-brilliant_day_eine_kleine_mix', 'frank_sinatra-fly_me_to_the_moon', 'franz_ferdinand-come_on_home', 'garbage-hammering_in_my_head', 'gene_clark-the_true_one', 'genesis-cuckoo_cocoon', 'george_harrison-all_things_must_pass', 'germs-lexicon_devil', 'gin_blossoms-hey_jealousy', 'glenn_miller-in_the_mood', 'gloria_gaynor-i_will_survive', 'go-gos-vacation', 'gram_parsons-1000_wedding', 'green_day-longview', 'guns_n_roses-november_rain', 'hall_and_oates-private_eyes', 'howlin_wolf-moanin_at_midnight', 'hybris-hate', 'ike_and_tina_turner-river_deep_mountain_high', 'interpol-stella_was_a_diver_and_she_was_always_down', 'introspekt-tbd', 'ivilion-d_b_l', 'jackalopes-rotgut', 'jackson_5-abc', 'jacques_brel-les_vieux', 'jade_leary-going_in', 'james_brown-give_it_up_or_turnit_a_loose', 'james_taylor-fire_and_rain', 'jamie_janover-event_horizon', 'jamiroquai-little_l', 'jan_and_dean-surf_city', 'janes_addiction-been_caught_stealing', 'janet_jackson-miss_you_much', 'jay_kishor-raga_malgunji_jor', 'jeff_buckley-last_goodbye', 'jefferson_airplane-somebody_to_love', 'jerry_lee_lewis-great_balls_of_fire', 'jewel-enter_from_the_east', 'jimi_hendrix-highway_chile', 'john_cale-pablo_picasso', 'john_coltrane-giant_steps', 'john_lee_hooker-boom_boom', 'johnny_cash-the_man_comes_around', 'junior_murvin-police_and_thieves', 'kenji_williams-i_m_alive', 'kiss-deuce', 'kokoon-order', 'kool_and_the_gang-funky_stuff', 'kourosh_zolani-peaceful_planet', 'lambert_hendricks_and_ross-gimme_that_wine', 'led_zeppelin-immigrant_song', 'leonard_cohen-suzanne', 'lisa_debenedictis-fruitless', 'live-lightning_crashes', 'liz_phair-supernova', 'll_cool_j-mama_said_knock_you_out', 'los_lobos-corrido_1', 'lou_reed-walk_on_the_wild_side', 'louis_armstrong-hotter_than_that', 'love-you_set_the_scene', 'lynyrd_skynyrd-sweet_home_alabama', 'macy_gray-i_try', 'madness-baggy_trousers', 'madonna-ray_of_light', 'mamas_and_the_papas-words_of_love', 'manassas-bound_to_fall', 'marvelettes-please_mr._postman', 'marvin_gaye-whats_going_on', 'mary_wells-my_guy', 'massive_attack-risingson', 'mazzy_star-fade_into_you', 'mc_hammer-u_cant_touch_this', 'men_at_work-who_can_it_be_now', 'michael_jackson-billie_jean', 'michael_masley-advice_from_the_angel_of_thresholds', 'miles_davis-blue_in_green', 'moby-porcelain', 'modest_mouse-what_people_are_made_of', 'monkees-a_little_bit_me_a_little_bit_you', 'monoide-golden_key', 'morrissey-everyday_is_like_sunday', 'mose_allison-monsters_of_the_id', 'mott_the_hoople-roll_away_the_stone', 'mr_epic-ruff_and_tumble', 'mr_gelatine-knysnamushrooms', 'mrdc-leaving', 'muddy_waters-im_ready', 'muddy_waters-mannish_boy', 'myles_cochran-getting_stronger', 'napoleon_blown_aparts-higher_education', 'natalie_imbruglia-torn', 'neil_young-razor_love', 'neil_young_and_crazy_horse-western_hero', 'nelly-country_grammar', 'neutral_milk_hotel-where_youll_find_me_now', 'nine_inch_nails-head_like_a_hole', 'nirvana-aneurysm', 'no_doubt-artificial_sweetener', 'norah_jones-dont_know_why', 'oasis-supersonic', 'ojays-livin_for_the_weekend', 'otis_redding-mr._pitiful', 'panacea-dragaicuta', 'pantera-becoming', 'paul_mccartney-ebony_and_ivory', 'pearl_jam-yellow_ledbetter', 'pet_shop_boys-being_boring', 'pink_floyd-echoes', 'pizzle-what_s_wrong_with_my_footm', 'police-every_little_thing_she_does_is_magic', 'portishead-all_mine', 'pretenders-day_after_day', 'primus-jerry_was_a_race_car_driver', 'processor-nibtal_7', 'propellerheads-take_california', 'psychedelic_furs-love_my_way', 'psychetropic-dead_slow_day', 'queen-we_will_rock_you', 'r.e.m.-camera', 'radiohead-karma_police', 'rage_against_the_machine-maggies_farm', 'ramones-i_just_want_to_have_something_to_do', 'randy_newman-sail_away', 'red_hot_chili_peppers-give_it_away', 'rick_james-super_freak', 'robert_johnson-sweet_home_chicago', 'rocket_city_riot-mine_tonite', 'rolling_stones-little_by_little', 'ronettes-walking_in_the_rain', 'roots_of_rebellion-legend', 'rubn_gonzlez-cumbanchero', 'rufus_wainwright-cigarettes_and_chocolate_milk', 'sade-smooth_operator', 'santana-love_of_my_life', 'sarah_mclachlan-possession', 'saros-prelude', 'scott_hill-silk_road', 'screaming_trees-nearly_lost_you', 'sebadoh-soul_and_fire', 'sex_pistols-pretty_vacant', 'shakira-the_one', 'shane_jackman-set_fire_to_the_city', 'sheryl_crow-i_shall_believe', 'shins-new_slang', 'shira_kammen-music_of_waters', 'shuggie_otis-sweet_thang', 'simon_and_garfunkel-the_only_living_boy_in_new_york', 'sir_mix-a-lot-baby_got_back', 'skitzo-last_depression', 'sly_and_the_family_stone-just_like_a_baby', 'smashing_pumpkins-rocket', 'smithereens-behind_the_wall_of_sleep', 'smokey_robinson-cruisin', 'smokey_robinson_and_the_miracles-ooo_baby_baby', 'solace-laz_7_8', 'somadrone-coda', 'sonny_rollins-strode_rode', 'soul_ii_soul-keep_on_movin', 'soulprint-crawlspace', 'specials-gangsters', 'spencer_davis_group-gimme_some_lovin', 'spice_girls-stop', 'spinecar-stay', 'spiritualized-stop_your_crying', 'squarepusher-a_journey_to_reedham_(7am_mix)', 'squeeze-pulling_mussels_from_the_shell', 'stan_getz-corcovado_quiet_nights_of_quiet_stars', 'starship-nothings_gonna_stop_us_now', 'steppenwolf-born_to_be_wild', 'stereolab-cybeles_reverie', 'stevie_ray_vaughan-pride_and_joy', 'stevie_wonder-for_once_in_my_life', 'sting-big_lie_small_world', 'stone_roses-i_wanna_be_adored', 'strawbs-new_world', 'style_council-headstart_for_happiness', 'sundays-heres_where_the_story_ends', 'superchunk-slack_motherfucker', 'sweet-fox_on_the_run', 'syd_barrett-effervescing_elephant', 'sylvester-you_make_me_feel_mighty_real', 'syreeta-what_love_has_joined_together', 't._rex-children_of_the_revolution', 'talking_heads-and_she_was', 'tears_for_fears-everybody_wants_to_rule_the_world', 'teenage_fanclub-the_concept', 'temptations-since_i_lost_my_baby', 'they_might_be_giants-i_should_be_allowed_to_think', 'third_eye_blind-semi-charmed_life', 'throwing_muses-hate_my_way', 'thursday_group-like_white_on_rice', 'tilopa-kyo_rei', 'tim_buckley-morning_glory', 'tim_hardin-dont_make_promises', 'todd_rundgren-bang_the_drum_all_day', 'tom_paul-little_part_of_me', 'tom_petty-i_wont_back_down', 'tom_petty_and_the_heartbreakers-dont_come_around_here_no_more', 'tom_waits-time', 'tommy_james_and_the_shondells-i_think_were_alone_now', 'touchinggrace-wild_spring_apples', 'traffic-pearly_queen', 'troggs-wild_thing', 'turtles-elenore', 'u2-hold_me_thrill_me_kiss_me_kill_me', 'uncle_tupelo-the_long_cut', 'urge_overkill-sister_havana', 'utopia_banished-by_mourning', 'van_halen-aint_talkin_bout_love', 'van_morrison-and_it_stoned_me', 'vapors-turning_japanese', 'velvet_underground-new_age', 'version-universal_humans', 'very_large_array-psychedelic_baby', 'war-all_day_music', 'weezer-buddy_holly', 'wes_montgomery-bumpin', 'west_exit-nocturne', 'white_stripes-hotel_yorba', 'whitney_houston-how_will_i_know', 'who-bargain', 'wicked_allstars-happy', 'wilco-kingpin', 'xtc-love_at_first_sight', 'yakshi-chandra', 'yeah_yeah_yeahs-maps', 'yes-leave_it', 'young_mc-bust_a_move', 'young_rascals-baby_lets_wait', 'zapp-dance_floor', 'zombies-beechwood_park']\n",
            "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '204', '205', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410']\n",
            "Prediction for song 1 - 10cc-for_you_and_i: 2; labels: [1, 3]\n",
            "Prediction for song 2 - 2pac-trapped: 2; labels: [0]\n",
            "Prediction for song 3 - 5th_dimension-one_less_bell_to_answer: 2; labels: [1, 3, 4]\n",
            "Prediction for song 4 - a_tribe_called_quest-bonita_applebum: 2; labels: [1, 3]\n",
            "Prediction for song 5 - aaron_neville-tell_it_like_it_is: 2; labels: [1, 3]\n",
            "Prediction for song 6 - abba-s.o.s.: 2; labels: [2, 3]\n",
            "Prediction for song 7 - ac_dc-dirty_deeds_done_dirt_cheap: 2; labels: [0, 3]\n",
            "Prediction for song 8 - adam_ant-wonderful: 2; labels: [3]\n",
            "Prediction for song 9 - adverts-gary_gilmores_eyes: 2; labels: [0]\n",
            "Prediction for song 10 - aerobic_jonquil-sweat_machine: 2; labels: [1, 3]\n",
            "Prediction for song 11 - aerosmith-dude_looks_like_a_lady: 2; labels: [2, 3]\n",
            "Prediction for song 12 - aimee_mann-wise_up: 2; labels: [1, 3]\n",
            "Prediction for song 13 - air-sexy_boy: 2; labels: [1]\n",
            "Prediction for song 14 - al_green-sha-la-la_make_me_happy: 2; labels: [1, 2, 3]\n",
            "Prediction for song 15 - alanis_morissette-thank_u: 2; labels: [3, 4]\n",
            "Prediction for song 16 - alice_cooper-elected: 2; labels: [3]\n",
            "Prediction for song 17 - alice_in_chains-no_excuses: 2; labels: [4]\n",
            "Prediction for song 18 - alicia_keys-fallin: 2; labels: [3]\n",
            "Prediction for song 19 - allman_brothers_band-melissa: 2; labels: [1, 3]\n",
            "Prediction for song 20 - altered_images-dont_talk_to_me_about_love: 2; labels: [2]\n",
            "Prediction for song 21 - american_music_club-jesus_hands: 2; labels: [1, 3]\n",
            "Prediction for song 22 - andrews_sisters-boogie_woogie_bugle_boy: 2; labels: [2, 3]\n",
            "Prediction for song 23 - ani_difranco-crime_for_crime: 2; labels: [0, 3]\n",
            "Prediction for song 24 - animals-im_crying: 2; labels: [3]\n",
            "Prediction for song 25 - anita_baker-caught_up_in_the_rapture: 2; labels: [1, 2, 3]\n",
            "Prediction for song 26 - antiguru-peering: 2; labels: [1, 2, 3]\n",
            "Prediction for song 27 - antonio_carlos_jobim-wave: 2; labels: [2, 3]\n",
            "Prediction for song 28 - anup-life_glides: 2; labels: [2]\n",
            "Prediction for song 29 - aphex_twin-come_to_daddy: 2; labels: [0]\n",
            "Prediction for song 30 - apples_in_stereo-glowworm: 2; labels: [3]\n",
            "Prediction for song 31 - aretha_franklin-dont_play_that_song: 2; labels: [3]\n",
            "Prediction for song 32 - arlo_guthrie-alices_restaurant_massacree: 2; labels: [1, 2]\n",
            "Prediction for song 33 - art_tatum-willow_weep_for_me: 2; labels: [1, 2]\n",
            "Prediction for song 34 - arthur_alexander-you_dont_care: 2; labels: [3]\n",
            "Prediction for song 35 - arthur_yoria-at_least_you_ve_been_told: 2; labels: [3]\n",
            "Prediction for song 36 - ashford_and_simpson-solid: 2; labels: [2, 3]\n",
            "Prediction for song 37 - association-windy: 2; labels: [2, 3]\n",
            "Prediction for song 38 - atomic_opera-watergrave: 2; labels: [3]\n",
            "Prediction for song 39 - b.b._king-sweet_little_angel: 2; labels: [1, 3]\n",
            "Prediction for song 40 - backstreet_boys-as_long_as_you_love_me: 2; labels: [1, 3]\n",
            "Prediction for song 41 - badly_drawn_boy-all_possibilities: 2; labels: [2, 3]\n",
            "Prediction for song 42 - band-king_harvest_has_surely_come: 2; labels: [2]\n",
            "Prediction for song 43 - barbara_leoni-don_t_rain_on_my_parade: 2; labels: [2, 3]\n",
            "Prediction for song 44 - barry_manilow-mandy: 2; labels: [1, 3]\n",
            "Prediction for song 45 - barry_white-cant_get_enough_of_your_love_babe: 2; labels: [3]\n",
            "Prediction for song 46 - bay_city_rollers-saturday_night: 2; labels: [2, 3]\n",
            "Prediction for song 47 - beach_boys-i_get_around: 2; labels: [3]\n",
            "Prediction for song 48 - beatles-strawberry_fields_forever: 2; labels: [1]\n",
            "Prediction for song 49 - beautiful_south-one_last_love_song: 2; labels: [1, 3, 4]\n",
            "Prediction for song 50 - beck-where_its_at: 2; labels: [2]\n",
            "Prediction for song 51 - bee_gees-stayin_alive: 2; labels: [2, 3]\n",
            "Prediction for song 52 - belle_and_sebastian-like_dylan_in_the_movies: 2; labels: [1]\n",
            "Prediction for song 53 - ben_folds_five-brick: 2; labels: [1, 3, 4]\n",
            "Prediction for song 54 - big_star-in_the_street: 2; labels: [2]\n",
            "Prediction for song 55 - billie_holiday-god_bless_the_child: 2; labels: [1, 3]\n",
            "Prediction for song 56 - billy_bragg-jeane: 2; labels: [4]\n",
            "Prediction for song 57 - billy_joel-we_didnt_start_the_fire: 2; labels: [2]\n",
            "Prediction for song 58 - birthday_party-mr._clarinet: 2; labels: [0, 4]\n",
            "Prediction for song 59 - bjrk-army_of_me: 2; labels: [0]\n",
            "Prediction for song 60 - black_crowes-thorn_in_my_pride: 2; labels: [3]\n",
            "Prediction for song 61 - black_flag-six_pack: 2; labels: [0, 3]\n",
            "Prediction for song 62 - black_sabbath-black_sabbath: 2; labels: [0, 4]\n",
            "Prediction for song 63 - blood_sweat_and_tears-sometimes_in_winter: 2; labels: [1, 3]\n",
            "Prediction for song 64 - blue_yster_cult-burnin_for_you: 2; labels: [2, 3]\n",
            "Prediction for song 65 - blur-country_house: 2; labels: [2, 3]\n",
            "Prediction for song 66 - bo_diddley-you_cant_judge_a_book_by_its_cover: 2; labels: [2, 3]\n",
            "Prediction for song 67 - bob_dylan-ill_be_your_baby_tonight: 2; labels: [1, 3]\n",
            "Prediction for song 68 - bob_marley_and_the_wailers-three_little_birds: 2; labels: [1, 2, 3]\n",
            "Prediction for song 69 - bob_seger-turn_the_page: 2; labels: [1, 3, 4]\n",
            "Prediction for song 70 - bobby_brown-my_prerogative: 2; labels: [2]\n",
            "Prediction for song 71 - bobby_fuller_four-i_fought_the_law: 2; labels: [3]\n",
            "Prediction for song 72 - bobby_womack-womans_gotta_have_it: 2; labels: [1, 2, 3]\n",
            "Prediction for song 73 - bomb_the_bass-bug_powder_dust: 2; labels: [0]\n",
            "Prediction for song 74 - bon_jovi-livin_on_a_prayer: 2; labels: [3]\n",
            "Prediction for song 75 - bonnie_tyler-total_eclipse_of_the_heart: 2; labels: [3, 4]\n",
            "Prediction for song 76 - boo_radleys-wake_up_boo: 2; labels: [2]\n",
            "Prediction for song 77 - boogie_down_productions-the_bridge_is_over: 2; labels: [0]\n",
            "Prediction for song 78 - booker_t._and_the_mgs-time_is_tight: 2; labels: [2, 3]\n",
            "Prediction for song 79 - boston-more_than_a_feeling: 2; labels: [3]\n",
            "Prediction for song 80 - bread-if: 2; labels: [1, 3]\n",
            "Prediction for song 81 - brenton_wood-lovey_dovey_kind_of_love: 2; labels: [1, 2, 3]\n",
            "Prediction for song 82 - brian_eno-here_come_the_warm_jets: 2; labels: [2]\n",
            "Prediction for song 83 - britney_spears-im_a_slave_for_you: 2; labels: [2, 3]\n",
            "Prediction for song 84 - bruce_springsteen-badlands: 2; labels: [2, 3]\n",
            "Prediction for song 85 - buddy_holly-peggy_sue: 2; labels: [3]\n",
            "Prediction for song 86 - buena_vista_social_club-el_cuarto_de_tula: 2; labels: [2, 3]\n",
            "Prediction for song 87 - buffalo_springfield-for_what_its_worth: 2; labels: [1]\n",
            "Prediction for song 88 - buffalo_springfield-mr._soul: 2; labels: [3]\n",
            "Prediction for song 89 - built_to_spill-i_would_hurt_a_fly: 2; labels: [3, 4]\n",
            "Prediction for song 90 - burnshee_thornside-goodbye_on_a_beautiful_day: 2; labels: [3]\n",
            "Prediction for song 91 - bush-comedown: 2; labels: [0]\n",
            "Prediction for song 92 - busta_rhymes-woo_hah_got_you_all_in_check: 2; labels: [0, 3]\n",
            "Prediction for song 93 - buzzcocks-everybodys_happy_nowadays: 2; labels: [2, 3]\n",
            "Prediction for song 94 - cab_calloway-minnie_the_moocher: 2; labels: [3]\n",
            "Prediction for song 95 - cake-perhaps,_perhaps,_perhaps: 2; labels: [4]\n",
            "Prediction for song 96 - captain_beefheart_and_the_magic_band-safe_as_milk: 2; labels: [0]\n",
            "Prediction for song 97 - cardigans-lovefool: 2; labels: [2, 3]\n",
            "Prediction for song 98 - cargo_cult-garden: 2; labels: [0]\n",
            "Prediction for song 99 - carl_perkins-matchbox: 2; labels: [3]\n",
            "Prediction for song 100 - carly_simon-youre_so_vain: 2; labels: [3]\n",
            "Prediction for song 101 - carole_king-youve_got_a_friend: 2; labels: [1, 2, 3]\n",
            "Prediction for song 102 - carpenters-rainy_days_and_mondays: 2; labels: [1, 3, 4]\n",
            "Prediction for song 103 - cars-good_times_roll: 2; labels: [2, 3]\n",
            "Prediction for song 104 - cat_power-he_war: 2; labels: [4]\n",
            "Prediction for song 105 - catherine_wheel-black_metallic: 2; labels: [1, 3]\n",
            "Prediction for song 106 - cc_music_factory-gonna_make_you_sweat_everybody_dance_now: 2; labels: [2]\n",
            "Prediction for song 107 - chad_and_jeremy-before_and_after: 2; labels: [1, 4]\n",
            "Prediction for song 108 - chantal_kreviazuk-surrounded: 2; labels: [3, 4]\n",
            "Prediction for song 109 - charles_mingus-mood_indigo: 2; labels: [1, 4]\n",
            "Prediction for song 110 - charlie_parker-ornithology: 2; labels: [1, 2, 3]\n",
            "Prediction for song 111 - charlie_rich-behind_closed_doors: 2; labels: [1, 2, 3]\n",
            "Prediction for song 112 - cheap_trick-dream_police: 2; labels: [2]\n",
            "Prediction for song 113 - cheryl_ann_fulton-marsh_of_rhuddlan: 2; labels: [1, 2, 3]\n",
            "Prediction for song 114 - chet_baker-these_foolish_things: 2; labels: [1, 3]\n",
            "Prediction for song 115 - chi-lites-stoned_out_of_my_mind: 2; labels: [1, 2, 3]\n",
            "Prediction for song 116 - chic-le_freak: 2; labels: [2, 3]\n",
            "Prediction for song 117 - chicago-if_you_leave_me_now: 2; labels: [1, 3, 4]\n",
            "Prediction for song 118 - chris_juergensen-prospects: 2; labels: [1, 3]\n",
            "Prediction for song 119 - christina_aguilera-genie_in_a_bottle: 2; labels: [2, 3]\n",
            "Prediction for song 120 - chuck_berry-roll_over_beethoven: 2; labels: [2, 3]\n",
            "Prediction for song 121 - chumbawamba-tubthumping: 2; labels: [2, 3]\n",
            "Prediction for song 122 - church-under_the_milky_way: 2; labels: [3]\n",
            "Prediction for song 123 - cilla_black-alfie: 2; labels: [1, 3]\n",
            "Prediction for song 124 - clash-lost_in_the_supermarket: 2; labels: [3]\n",
            "Prediction for song 125 - coldplay-clocks: 2; labels: [1, 3]\n",
            "Prediction for song 126 - contours-do_you_love_me: 2; labels: [2, 3]\n",
            "Prediction for song 127 - count_basie-lester_leaps_in: 2; labels: [1, 2, 3]\n",
            "Prediction for song 128 - counting_crows-speedway: 2; labels: [1, 3, 4]\n",
            "Prediction for song 129 - cowboy_junkies-postcard_blues: 2; labels: [1, 4]\n",
            "Prediction for song 130 - cranberries-linger: 2; labels: [1, 3]\n",
            "Prediction for song 131 - cream-tales_of_brave_ulysses: 2; labels: [4]\n",
            "Prediction for song 132 - creedence_clearwater_revival-travelin_band: 2; labels: [2, 3]\n",
            "Prediction for song 133 - crosby_stills_and_nash-guinnevere: 2; labels: [1, 3]\n",
            "Prediction for song 134 - crosby_stills_nash_and_young-teach_your_children: 2; labels: [1]\n",
            "Prediction for song 135 - curandero-aras: 2; labels: [3]\n",
            "Prediction for song 136 - cure-just_like_heaven: 2; labels: [3]\n",
            "Prediction for song 137 - curtis_mayfield-move_on_up: 2; labels: [2, 3]\n",
            "Prediction for song 138 - cyndi_lauper-money_changes_everything: 2; labels: [3]\n",
            "Prediction for song 139 - daft_punk-da_funk: 2; labels: [3]\n",
            "Prediction for song 140 - darkness-i_believe_in_a_thing_called_love: 2; labels: [3]\n",
            "Prediction for song 141 - dave_matthews_band-ants_marching: 2; labels: [1, 2, 3]\n",
            "Prediction for song 142 - david_bowie-song_for_bob_dylan: 2; labels: [2]\n",
            "Prediction for song 143 - dead_kennedys-chemical_warfare: 2; labels: [0]\n",
            "Prediction for song 144 - def_leppard-pour_some_sugar_on_me: 2; labels: [3]\n",
            "Prediction for song 145 - dennis_brown-tribulation: 2; labels: [1, 3]\n",
            "Prediction for song 146 - diana_ross_and_the_supremes-where_did_our_love_go: 2; labels: [1, 2, 3]\n",
            "Prediction for song 147 - dido-here_with_me: 2; labels: [1, 3]\n",
            "Prediction for song 148 - dionne_warwick-walk_on_by: 2; labels: [1, 3, 4]\n",
            "Prediction for song 149 - dire_straits-money_for_nothing: 2; labels: [2]\n",
            "Prediction for song 150 - dj_jazzy_jeff_and_the_fresh_prince-summertime: 2; labels: [2, 3]\n",
            "Prediction for song 151 - dj_markitos-sunset_138_bpm_remix: 2; labels: [3]\n",
            "Prediction for song 152 - django_reinhardt-brazil: 2; labels: [1, 2]\n",
            "Prediction for song 153 - donovan-catch_the_wind: 2; labels: [3]\n",
            "Prediction for song 154 - doobie_brothers-china_grove: 2; labels: [2, 3]\n",
            "Prediction for song 155 - doors-touch_me: 2; labels: [2, 3]\n",
            "Prediction for song 156 - dr._dre-nuthin_but_a_g_thang: 2; labels: [3]\n",
            "Prediction for song 157 - drevo-our_watcher_show_us_the_way: 2; labels: [1]\n",
            "Prediction for song 158 - drop_trio-slapjack: 2; labels: [1, 2]\n",
            "Prediction for song 159 - duke_ellington_and_his_orchestra-caravan: 2; labels: [2, 3]\n",
            "Prediction for song 160 - duncan_sheik-barely_breathing: 2; labels: [3]\n",
            "Prediction for song 161 - duran_duran-come_undone: 2; labels: [1, 3, 4]\n",
            "Prediction for song 162 - eagles-tequila_sunrise: 2; labels: [1, 4]\n",
            "Prediction for song 163 - earth_wind_and_fire-september: 2; labels: [2, 3]\n",
            "Prediction for song 164 - electric_frankenstein-teenage_shutdown: 2; labels: [0, 3]\n",
            "Prediction for song 165 - elliott_smith-baby_britain: 2; labels: [1, 2, 3]\n",
            "Prediction for song 166 - elton_john-tiny_dancer: 2; labels: [1, 3]\n",
            "Prediction for song 167 - elvis_costello-less_than_zero: 2; labels: [3]\n",
            "Prediction for song 168 - elvis_presley-heartbreak_hotel: 2; labels: [3, 4]\n",
            "Prediction for song 169 - emma_s_mini-lost: 2; labels: [3]\n",
            "Prediction for song 170 - erasure-chains_of_love: 2; labels: [2, 3]\n",
            "Prediction for song 171 - eric_clapton-wonderful_tonight: 2; labels: [1, 3]\n",
            "Prediction for song 172 - etherine-never_leave: 2; labels: [1, 3]\n",
            "Prediction for song 173 - evanescence-my_immortal: 2; labels: [3, 4]\n",
            "Prediction for song 174 - everly_brothers-take_a_message_to_mary: 2; labels: [1, 3, 4]\n",
            "Prediction for song 175 - faith_hill-lets_make_love: 2; labels: [3]\n",
            "Prediction for song 176 - faith_no_more-epic: 2; labels: [0]\n",
            "Prediction for song 177 - falik-bliss: 2; labels: [1, 3]\n",
            "Prediction for song 178 - fiona_apple-love_ridden: 2; labels: [1, 3, 4]\n",
            "Prediction for song 179 - fleetwood_mac-say_you_love_me: 2; labels: [1, 3]\n",
            "Prediction for song 180 - flying_burrito_brothers-break_my_mind: 2; labels: [3]\n",
            "Prediction for song 181 - foo_fighters-big_me: 2; labels: [1, 3]\n",
            "Prediction for song 182 - four_stones-brilliant_day_eine_kleine_mix: 2; labels: [3]\n",
            "Prediction for song 183 - frank_sinatra-fly_me_to_the_moon: 2; labels: [1, 2, 3]\n",
            "Prediction for song 184 - franz_ferdinand-come_on_home: 2; labels: [2, 3]\n",
            "Prediction for song 185 - garbage-hammering_in_my_head: 2; labels: [0]\n",
            "Prediction for song 186 - gene_clark-the_true_one: 2; labels: [1, 3, 4]\n",
            "Prediction for song 187 - genesis-cuckoo_cocoon: 2; labels: [1]\n",
            "Prediction for song 188 - george_harrison-all_things_must_pass: 2; labels: [1, 3]\n",
            "Prediction for song 189 - germs-lexicon_devil: 2; labels: [3]\n",
            "Prediction for song 190 - gin_blossoms-hey_jealousy: 2; labels: [3, 4]\n",
            "Prediction for song 191 - glenn_miller-in_the_mood: 2; labels: [1, 2, 3]\n",
            "Prediction for song 192 - gloria_gaynor-i_will_survive: 2; labels: [2, 3]\n",
            "Prediction for song 193 - go-gos-vacation: 2; labels: [2, 3]\n",
            "Prediction for song 194 - gram_parsons-1000_wedding: 2; labels: [3]\n",
            "Prediction for song 195 - green_day-longview: 2; labels: [3]\n",
            "Prediction for song 196 - guns_n_roses-november_rain: 2; labels: [3]\n",
            "Prediction for song 197 - hall_and_oates-private_eyes: 2; labels: [3]\n",
            "Prediction for song 198 - howlin_wolf-moanin_at_midnight: 2; labels: [4]\n",
            "Prediction for song 199 - hybris-hate: 2; labels: [0]\n",
            "Prediction for song 200 - ike_and_tina_turner-river_deep_mountain_high: 2; labels: [3]\n",
            "Prediction for song 201 - interpol-stella_was_a_diver_and_she_was_always_down: 2; labels: [4]\n",
            "Prediction for song 202 - introspekt-tbd: 2; labels: [3]\n",
            "Prediction for song 204 - ivilion-d_b_l: 2; labels: [0]\n",
            "Prediction for song 205 - jackalopes-rotgut: 2; labels: [3]\n",
            "Prediction for song 207 - jackson_5-abc: 2; labels: [2, 3]\n",
            "Prediction for song 208 - jacques_brel-les_vieux: 2; labels: [1]\n",
            "Prediction for song 209 - jade_leary-going_in: 2; labels: [3]\n",
            "Prediction for song 210 - james_brown-give_it_up_or_turnit_a_loose: 2; labels: [2]\n",
            "Prediction for song 211 - james_taylor-fire_and_rain: 2; labels: [1, 3]\n",
            "Prediction for song 212 - jamie_janover-event_horizon: 2; labels: [1, 3]\n",
            "Prediction for song 213 - jamiroquai-little_l: 2; labels: [2, 3]\n",
            "Prediction for song 214 - jan_and_dean-surf_city: 2; labels: [1, 2, 3]\n",
            "Prediction for song 215 - janes_addiction-been_caught_stealing: 2; labels: [2]\n",
            "Prediction for song 216 - janet_jackson-miss_you_much: 2; labels: [2, 3]\n",
            "Prediction for song 217 - jay_kishor-raga_malgunji_jor: 2; labels: [1, 3]\n",
            "Prediction for song 218 - jeff_buckley-last_goodbye: 2; labels: [3]\n",
            "Prediction for song 219 - jefferson_airplane-somebody_to_love: 2; labels: [3]\n",
            "Prediction for song 220 - jerry_lee_lewis-great_balls_of_fire: 2; labels: [2, 3]\n",
            "Prediction for song 221 - jewel-enter_from_the_east: 2; labels: [1, 4]\n",
            "Prediction for song 222 - jimi_hendrix-highway_chile: 2; labels: [3]\n",
            "Prediction for song 223 - john_cale-pablo_picasso: 2; labels: [0, 4]\n",
            "Prediction for song 224 - john_coltrane-giant_steps: 2; labels: [2]\n",
            "Prediction for song 225 - john_lee_hooker-boom_boom: 2; labels: [1, 2]\n",
            "Prediction for song 226 - johnny_cash-the_man_comes_around: 2; labels: [1]\n",
            "Prediction for song 227 - junior_murvin-police_and_thieves: 2; labels: [3]\n",
            "Prediction for song 228 - kenji_williams-i_m_alive: 2; labels: [1]\n",
            "Prediction for song 229 - kiss-deuce: 2; labels: [3]\n",
            "Prediction for song 230 - kokoon-order: 2; labels: [3]\n",
            "Prediction for song 231 - kool_and_the_gang-funky_stuff: 2; labels: [2]\n",
            "Prediction for song 232 - kourosh_zolani-peaceful_planet: 2; labels: [1, 3]\n",
            "Prediction for song 233 - lambert_hendricks_and_ross-gimme_that_wine: 2; labels: [2, 3]\n",
            "Prediction for song 234 - led_zeppelin-immigrant_song: 2; labels: [0, 3]\n",
            "Prediction for song 235 - leonard_cohen-suzanne: 2; labels: [1]\n",
            "Prediction for song 236 - lisa_debenedictis-fruitless: 2; labels: [3]\n",
            "Prediction for song 237 - live-lightning_crashes: 2; labels: [4]\n",
            "Prediction for song 238 - liz_phair-supernova: 2; labels: [0, 3]\n",
            "Prediction for song 239 - ll_cool_j-mama_said_knock_you_out: 2; labels: [0]\n",
            "Prediction for song 240 - los_lobos-corrido_1: 2; labels: [2]\n",
            "Prediction for song 241 - lou_reed-walk_on_the_wild_side: 2; labels: [1]\n",
            "Prediction for song 242 - louis_armstrong-hotter_than_that: 2; labels: [2]\n",
            "Prediction for song 243 - love-you_set_the_scene: 2; labels: [3]\n",
            "Prediction for song 244 - lynyrd_skynyrd-sweet_home_alabama: 2; labels: [3]\n",
            "Prediction for song 245 - macy_gray-i_try: 2; labels: [1, 4]\n",
            "Prediction for song 246 - madness-baggy_trousers: 2; labels: [2]\n",
            "Prediction for song 247 - madonna-ray_of_light: 2; labels: [2, 3]\n",
            "Prediction for song 248 - mamas_and_the_papas-words_of_love: 2; labels: [4]\n",
            "Prediction for song 249 - manassas-bound_to_fall: 2; labels: [2, 3]\n",
            "Prediction for song 250 - marvelettes-please_mr._postman: 2; labels: [4]\n",
            "Prediction for song 251 - marvin_gaye-whats_going_on: 2; labels: [1, 3]\n",
            "Prediction for song 252 - mary_wells-my_guy: 2; labels: [2]\n",
            "Prediction for song 253 - massive_attack-risingson: 2; labels: [3]\n",
            "Prediction for song 254 - mazzy_star-fade_into_you: 2; labels: [1, 3, 4]\n",
            "Prediction for song 255 - mc_hammer-u_cant_touch_this: 2; labels: [2, 3]\n",
            "Prediction for song 256 - men_at_work-who_can_it_be_now: 2; labels: [2, 3]\n",
            "Prediction for song 257 - michael_jackson-billie_jean: 2; labels: [3]\n",
            "Prediction for song 258 - michael_masley-advice_from_the_angel_of_thresholds: 2; labels: [1]\n",
            "Prediction for song 259 - miles_davis-blue_in_green: 2; labels: [1, 3, 4]\n",
            "Prediction for song 260 - moby-porcelain: 2; labels: [1, 3]\n",
            "Prediction for song 261 - modest_mouse-what_people_are_made_of: 2; labels: [0, 3]\n",
            "Prediction for song 262 - monkees-a_little_bit_me_a_little_bit_you: 2; labels: [2, 3]\n",
            "Prediction for song 263 - monoide-golden_key: 2; labels: [1, 2, 3]\n",
            "Prediction for song 264 - morrissey-everyday_is_like_sunday: 2; labels: [1]\n",
            "Prediction for song 265 - mose_allison-monsters_of_the_id: 2; labels: [4]\n",
            "Prediction for song 266 - mott_the_hoople-roll_away_the_stone: 2; labels: [2]\n",
            "Prediction for song 267 - mr_epic-ruff_and_tumble: 2; labels: [1, 3]\n",
            "Prediction for song 268 - mr_gelatine-knysnamushrooms: 2; labels: [3]\n",
            "Prediction for song 269 - mrdc-leaving: 2; labels: [1]\n",
            "Prediction for song 270 - muddy_waters-im_ready: 2; labels: [1]\n",
            "Prediction for song 271 - muddy_waters-mannish_boy: 2; labels: [0, 2, 3]\n",
            "Prediction for song 272 - myles_cochran-getting_stronger: 2; labels: [1, 3]\n",
            "Prediction for song 273 - napoleon_blown_aparts-higher_education: 2; labels: [0, 3]\n",
            "Prediction for song 274 - natalie_imbruglia-torn: 2; labels: [3, 4]\n",
            "Prediction for song 275 - neil_young-razor_love: 2; labels: [1, 3]\n",
            "Prediction for song 276 - neil_young_and_crazy_horse-western_hero: 2; labels: [1, 3, 4]\n",
            "Prediction for song 277 - nelly-country_grammar: 2; labels: [2]\n",
            "Prediction for song 278 - neutral_milk_hotel-where_youll_find_me_now: 2; labels: [1]\n",
            "Prediction for song 279 - nine_inch_nails-head_like_a_hole: 2; labels: [0]\n",
            "Prediction for song 280 - nirvana-aneurysm: 2; labels: [0, 3]\n",
            "Prediction for song 281 - no_doubt-artificial_sweetener: 2; labels: [0, 3]\n",
            "Prediction for song 282 - norah_jones-dont_know_why: 2; labels: [1, 3]\n",
            "Prediction for song 283 - oasis-supersonic: 2; labels: [3]\n",
            "Prediction for song 284 - ojays-livin_for_the_weekend: 2; labels: [2, 3]\n",
            "Prediction for song 285 - otis_redding-mr._pitiful: 2; labels: [3]\n",
            "Prediction for song 286 - panacea-dragaicuta: 2; labels: [4]\n",
            "Prediction for song 287 - pantera-becoming: 2; labels: [0]\n",
            "Prediction for song 288 - paul_mccartney-ebony_and_ivory: 2; labels: [1, 2, 3]\n",
            "Prediction for song 289 - pearl_jam-yellow_ledbetter: 2; labels: [3, 4]\n",
            "Prediction for song 290 - pet_shop_boys-being_boring: 2; labels: [1]\n",
            "Prediction for song 291 - pink_floyd-echoes: 2; labels: [1, 3]\n",
            "Prediction for song 292 - pizzle-what_s_wrong_with_my_footm: 2; labels: [0, 3]\n",
            "Prediction for song 293 - police-every_little_thing_she_does_is_magic: 2; labels: [2, 3]\n",
            "Prediction for song 294 - portishead-all_mine: 2; labels: [1, 4]\n",
            "Prediction for song 295 - pretenders-day_after_day: 2; labels: [2, 3]\n",
            "Prediction for song 296 - primus-jerry_was_a_race_car_driver: 2; labels: [0]\n",
            "Prediction for song 297 - processor-nibtal_7: 2; labels: [0]\n",
            "Prediction for song 298 - propellerheads-take_california: 2; labels: [2]\n",
            "Prediction for song 299 - psychedelic_furs-love_my_way: 2; labels: [3]\n",
            "Prediction for song 300 - psychetropic-dead_slow_day: 0; labels: [1]\n",
            "Prediction for song 301 - queen-we_will_rock_you: 2; labels: [2]\n",
            "Prediction for song 302 - r.e.m.-camera: 2; labels: [1, 3]\n",
            "Prediction for song 303 - radiohead-karma_police: 2; labels: [1, 4]\n",
            "Prediction for song 304 - rage_against_the_machine-maggies_farm: 2; labels: [0, 3]\n",
            "Prediction for song 305 - ramones-i_just_want_to_have_something_to_do: 2; labels: [2, 3]\n",
            "Prediction for song 306 - randy_newman-sail_away: 2; labels: [1, 3]\n",
            "Prediction for song 307 - red_hot_chili_peppers-give_it_away: 2; labels: [2]\n",
            "Prediction for song 308 - rick_james-super_freak: 2; labels: [2]\n",
            "Prediction for song 309 - robert_johnson-sweet_home_chicago: 2; labels: [3]\n",
            "Prediction for song 310 - rocket_city_riot-mine_tonite: 2; labels: [3]\n",
            "Prediction for song 311 - rolling_stones-little_by_little: 2; labels: [3]\n",
            "Prediction for song 312 - ronettes-walking_in_the_rain: 2; labels: [2, 3]\n",
            "Prediction for song 313 - roots_of_rebellion-legend: 2; labels: [3]\n",
            "Prediction for song 314 - rubn_gonzlez-cumbanchero: 2; labels: [2, 3]\n",
            "Prediction for song 315 - rufus_wainwright-cigarettes_and_chocolate_milk: 2; labels: [1, 3]\n",
            "Prediction for song 316 - sade-smooth_operator: 2; labels: [1, 3]\n",
            "Prediction for song 317 - santana-love_of_my_life: 2; labels: [3]\n",
            "Prediction for song 318 - sarah_mclachlan-possession: 2; labels: [1, 3]\n",
            "Prediction for song 319 - saros-prelude: 2; labels: [3]\n",
            "Prediction for song 320 - scott_hill-silk_road: 2; labels: [1]\n",
            "Prediction for song 321 - screaming_trees-nearly_lost_you: 2; labels: [3]\n",
            "Prediction for song 322 - sebadoh-soul_and_fire: 2; labels: [3, 4]\n",
            "Prediction for song 323 - sex_pistols-pretty_vacant: 2; labels: [0]\n",
            "Prediction for song 324 - shakira-the_one: 2; labels: [3]\n",
            "Prediction for song 325 - shane_jackman-set_fire_to_the_city: 2; labels: [1, 3, 4]\n",
            "Prediction for song 326 - sheryl_crow-i_shall_believe: 2; labels: [1, 3]\n",
            "Prediction for song 327 - shins-new_slang: 2; labels: [1, 3, 4]\n",
            "Prediction for song 328 - shira_kammen-music_of_waters: 2; labels: [1, 3]\n",
            "Prediction for song 329 - shuggie_otis-sweet_thang: 2; labels: [1, 3]\n",
            "Prediction for song 330 - simon_and_garfunkel-the_only_living_boy_in_new_york: 2; labels: [1, 3, 4]\n",
            "Prediction for song 331 - sir_mix-a-lot-baby_got_back: 2; labels: [0]\n",
            "Prediction for song 332 - skitzo-last_depression: 2; labels: [0, 3]\n",
            "Prediction for song 333 - sly_and_the_family_stone-just_like_a_baby: 2; labels: [1, 3]\n",
            "Prediction for song 334 - smashing_pumpkins-rocket: 2; labels: [3]\n",
            "Prediction for song 335 - smithereens-behind_the_wall_of_sleep: 2; labels: [3]\n",
            "Prediction for song 336 - smokey_robinson-cruisin: 2; labels: [1, 3]\n",
            "Prediction for song 337 - smokey_robinson_and_the_miracles-ooo_baby_baby: 2; labels: [1, 3]\n",
            "Prediction for song 338 - solace-laz_7_8: 2; labels: [3]\n",
            "Prediction for song 339 - somadrone-coda: 2; labels: [0, 4]\n",
            "Prediction for song 340 - sonny_rollins-strode_rode: 2; labels: [2, 3]\n",
            "Prediction for song 341 - soul_ii_soul-keep_on_movin: 2; labels: [1]\n",
            "Prediction for song 342 - soulprint-crawlspace: 2; labels: [0, 3]\n",
            "Prediction for song 343 - specials-gangsters: 2; labels: [2, 3]\n",
            "Prediction for song 344 - spencer_davis_group-gimme_some_lovin: 2; labels: [2, 3]\n",
            "Prediction for song 345 - spice_girls-stop: 2; labels: [2, 3]\n",
            "Prediction for song 346 - spinecar-stay: 2; labels: [0]\n",
            "Prediction for song 347 - spiritualized-stop_your_crying: 2; labels: [1, 3]\n",
            "Prediction for song 348 - squarepusher-a_journey_to_reedham_(7am_mix): 2; labels: [2]\n",
            "Prediction for song 349 - squeeze-pulling_mussels_from_the_shell: 2; labels: [2]\n",
            "Prediction for song 350 - stan_getz-corcovado_quiet_nights_of_quiet_stars: 2; labels: [1, 3]\n",
            "Prediction for song 351 - starship-nothings_gonna_stop_us_now: 2; labels: [3]\n",
            "Prediction for song 352 - steppenwolf-born_to_be_wild: 2; labels: [2, 3]\n",
            "Prediction for song 353 - stereolab-cybeles_reverie: 2; labels: [1, 2, 3]\n",
            "Prediction for song 354 - stevie_ray_vaughan-pride_and_joy: 2; labels: [2, 3]\n",
            "Prediction for song 355 - stevie_wonder-for_once_in_my_life: 2; labels: [2, 3]\n",
            "Prediction for song 356 - sting-big_lie_small_world: 2; labels: [1, 3]\n",
            "Prediction for song 357 - stone_roses-i_wanna_be_adored: 2; labels: [3]\n",
            "Prediction for song 358 - strawbs-new_world: 2; labels: [0]\n",
            "Prediction for song 359 - style_council-headstart_for_happiness: 2; labels: [2, 3]\n",
            "Prediction for song 360 - sundays-heres_where_the_story_ends: 2; labels: [1, 2]\n",
            "Prediction for song 361 - superchunk-slack_motherfucker: 2; labels: [0, 3]\n",
            "Prediction for song 362 - sweet-fox_on_the_run: 2; labels: [2, 3]\n",
            "Prediction for song 363 - syd_barrett-effervescing_elephant: 2; labels: [2]\n",
            "Prediction for song 364 - sylvester-you_make_me_feel_mighty_real: 2; labels: [2, 3]\n",
            "Prediction for song 365 - syreeta-what_love_has_joined_together: 2; labels: [1, 2, 3]\n",
            "Prediction for song 366 - t._rex-children_of_the_revolution: 2; labels: [0]\n",
            "Prediction for song 367 - talking_heads-and_she_was: 2; labels: [3]\n",
            "Prediction for song 368 - tears_for_fears-everybody_wants_to_rule_the_world: 2; labels: [1, 2, 3]\n",
            "Prediction for song 369 - teenage_fanclub-the_concept: 2; labels: [3]\n",
            "Prediction for song 370 - temptations-since_i_lost_my_baby: 2; labels: [1, 3]\n",
            "Prediction for song 371 - they_might_be_giants-i_should_be_allowed_to_think: 2; labels: [1, 3]\n",
            "Prediction for song 372 - third_eye_blind-semi-charmed_life: 2; labels: [3]\n",
            "Prediction for song 373 - throwing_muses-hate_my_way: 2; labels: [0]\n",
            "Prediction for song 374 - thursday_group-like_white_on_rice: 2; labels: [3, 4]\n",
            "Prediction for song 375 - tilopa-kyo_rei: 2; labels: [1, 3]\n",
            "Prediction for song 376 - tim_buckley-morning_glory: 2; labels: [1, 4]\n",
            "Prediction for song 377 - tim_hardin-dont_make_promises: 2; labels: [1, 3]\n",
            "Prediction for song 378 - todd_rundgren-bang_the_drum_all_day: 2; labels: [2, 3]\n",
            "Prediction for song 379 - tom_paul-little_part_of_me: 2; labels: [3, 4]\n",
            "Prediction for song 380 - tom_petty-i_wont_back_down: 2; labels: [3]\n",
            "Prediction for song 381 - tom_petty_and_the_heartbreakers-dont_come_around_here_no_more: 2; labels: [1, 3]\n",
            "Prediction for song 382 - tom_waits-time: 2; labels: [1, 4]\n",
            "Prediction for song 383 - tommy_james_and_the_shondells-i_think_were_alone_now: 2; labels: [2, 3]\n",
            "Prediction for song 384 - touchinggrace-wild_spring_apples: 2; labels: [1, 2, 3]\n",
            "Prediction for song 385 - traffic-pearly_queen: 2; labels: [0]\n",
            "Prediction for song 386 - troggs-wild_thing: 2; labels: [2, 3]\n",
            "Prediction for song 387 - turtles-elenore: 2; labels: [2]\n",
            "Prediction for song 388 - u2-hold_me_thrill_me_kiss_me_kill_me: 2; labels: [3]\n",
            "Prediction for song 389 - uncle_tupelo-the_long_cut: 2; labels: [1, 3]\n",
            "Prediction for song 390 - urge_overkill-sister_havana: 2; labels: [3]\n",
            "Prediction for song 391 - utopia_banished-by_mourning: 2; labels: [0, 4]\n",
            "Prediction for song 392 - van_halen-aint_talkin_bout_love: 2; labels: [3]\n",
            "Prediction for song 393 - van_morrison-and_it_stoned_me: 2; labels: [1, 3]\n",
            "Prediction for song 394 - vapors-turning_japanese: 2; labels: [2]\n",
            "Prediction for song 395 - velvet_underground-new_age: 2; labels: [3]\n",
            "Prediction for song 396 - version-universal_humans: 2; labels: [1]\n",
            "Prediction for song 397 - very_large_array-psychedelic_baby: 2; labels: [3]\n",
            "Prediction for song 398 - war-all_day_music: 2; labels: [1, 2]\n",
            "Prediction for song 399 - weezer-buddy_holly: 2; labels: [2, 3]\n",
            "Prediction for song 400 - wes_montgomery-bumpin: 2; labels: [1, 3]\n",
            "Prediction for song 401 - west_exit-nocturne: 2; labels: [3]\n",
            "Prediction for song 402 - white_stripes-hotel_yorba: 2; labels: [2, 3]\n",
            "Prediction for song 403 - whitney_houston-how_will_i_know: 2; labels: [3]\n",
            "Prediction for song 404 - who-bargain: 2; labels: [3]\n",
            "Prediction for song 405 - wicked_allstars-happy: 2; labels: [1, 2, 3]\n",
            "Prediction for song 406 - wilco-kingpin: 2; labels: [3]\n",
            "Prediction for song 407 - xtc-love_at_first_sight: 2; labels: [2]\n",
            "Prediction for song 408 - yakshi-chandra: 2; labels: [1]\n",
            "Prediction for song 409 - yeah_yeah_yeahs-maps: 2; labels: [1, 3]\n",
            "Prediction for song 410 - yes-leave_it: 2; labels: [3]\n",
            "Test accuracy: 0.32598039215686275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUIpGmogDgOC",
        "colab_type": "text"
      },
      "source": [
        "**Random search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1eOsPQVDgG6",
        "colab_type": "code",
        "outputId": "bab51b78-a8fc-4db4-961b-71fa1e7127f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# best scores\n",
        "# \n",
        "best_accuracy = 0.0\n",
        "best_loss = 0.0\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "import random\n",
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec-672'\n",
        "#TRAIN_DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n",
        "compose=[transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomGrayscale(),\n",
        "        transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "        transforms.ToTensor()\n",
        "        ]\n",
        "train_dataset, val_dataset = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, compose)\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 10]\n",
        "val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 10]\n",
        "val_dataset = Subset(val_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))\n",
        "best_net = vgg11()\n",
        "best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "best_net = best_net.to(DEVICE)\n",
        "best_set = {}\n",
        "N = 50\n",
        "for i in range(N):\n",
        "  BATCH_SIZE = int(random.uniform(8, 16))\n",
        "  LR = 10**random.uniform(-5, -3)\n",
        "  MOMENTUM = 0.9\n",
        "  WEIGHT_DECAY = 10**random.uniform(-6, -3)\n",
        "  NUM_EPOCHS = 80\n",
        "  STEP_SIZE = 48\n",
        "  GAMMA = 10**random.uniform(-2, 0)\n",
        "  set = {\"lr\": LR, \"batch_size\": BATCH_SIZE, \"weight_decay\": WEIGHT_DECAY, \"gamma\": GAMMA}\n",
        "  print(\"-------------------------------------\")\n",
        "  print(set)\n",
        "  net = vgg11()\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "  current_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True)\n",
        "\n",
        "  val_accuracies.append(val_accuracy)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  if val_accuracy > best_accuracy:\n",
        "    best_accuracy = val_accuracy\n",
        "    best_loss = val_loss\n",
        "    best_net = copy.deepcopy(current_net)\n",
        "    best_set = copy.deepcopy(set)\n",
        "\n",
        "  print(\"lr {}, batch {}, decay {}, gamma {}, val accuracy {}, val loss {} [{} / {}]\".format(LR, BATCH_SIZE, WEIGHT_DECAY, GAMMA, val_accuracy, val_loss, i+1, N))\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "print(\"\\n{}, best val accuracy {}, best val loss {}\".format(best_set, best_accuracy, best_loss))\n",
        "print(\"val accuracies\\n{}\".format(val_accuracies))\n",
        "print(\"val losses\\n{}\".format(val_losses))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set 910\n",
            "validation set 102\n",
            "-------------------------------------\n",
            "{'lr': 2.3170275303150695e-05, 'batch_size': 11, 'weight_decay': 0.00012271015129769862, 'gamma': 0.016100234551165854}\n",
            "train_acc: 0.16153846153846155, val_acc: 0.17647058823529413, train_loss: 1.7928433832231458, val_loss: 1.7922614497296951 (1 / 80)\n",
            "train_acc: 0.16373626373626374, val_acc: 0.17647058823529413, train_loss: 1.7919728057725088, val_loss: 1.791697705493254 (2 / 80)\n",
            "train_acc: 0.17472527472527472, val_acc: 0.17647058823529413, train_loss: 1.7917875389476399, val_loss: 1.7911660834854723 (3 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.17647058823529413, train_loss: 1.7909264036587307, val_loss: 1.790641801030028 (4 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.17647058823529413, train_loss: 1.791339478125939, val_loss: 1.790124255068162 (5 / 80)\n",
            "train_acc: 0.1813186813186813, val_acc: 0.17647058823529413, train_loss: 1.79077926834861, val_loss: 1.7896123855721717 (6 / 80)\n",
            "train_acc: 0.1978021978021978, val_acc: 0.17647058823529413, train_loss: 1.7893862309036674, val_loss: 1.7890963706315732 (7 / 80)\n",
            "train_acc: 0.17252747252747253, val_acc: 0.17647058823529413, train_loss: 1.7893691622293912, val_loss: 1.7885740411047841 (8 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.17647058823529413, train_loss: 1.7885631859957516, val_loss: 1.788056234518687 (9 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.17647058823529413, train_loss: 1.7876959583261511, val_loss: 1.7875539660453796 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1879120879120879\n",
            "lr 2.3170275303150695e-05, batch 11, decay 0.00012271015129769862, gamma 0.016100234551165854, val accuracy 0.17647058823529413, val loss 1.7922614497296951 [1 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00020981990349966764, 'batch_size': 9, 'weight_decay': 3.88674174584636e-05, 'gamma': 0.4135686316820952}\n",
            "train_acc: 0.17032967032967034, val_acc: 0.18627450980392157, train_loss: 1.7898549018325387, val_loss: 1.7825464045300203 (1 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.21568627450980393, train_loss: 1.7802644624814883, val_loss: 1.7721606037195992 (2 / 80)\n",
            "train_acc: 0.2087912087912088, val_acc: 0.18627450980392157, train_loss: 1.7710607066259278, val_loss: 1.7612749934196472 (3 / 80)\n",
            "train_acc: 0.1978021978021978, val_acc: 0.18627450980392157, train_loss: 1.7624242017557332, val_loss: 1.7512695999706493 (4 / 80)\n",
            "train_acc: 0.18571428571428572, val_acc: 0.18627450980392157, train_loss: 1.7587551689409948, val_loss: 1.7451958901741926 (5 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.22549019607843138, train_loss: 1.7540715926296109, val_loss: 1.7381657782722921 (6 / 80)\n",
            "train_acc: 0.210989010989011, val_acc: 0.20588235294117646, train_loss: 1.7478989884093568, val_loss: 1.7297970757764929 (7 / 80)\n",
            "train_acc: 0.256043956043956, val_acc: 0.29411764705882354, train_loss: 1.7370453038058438, val_loss: 1.718023016172297 (8 / 80)\n",
            "train_acc: 0.2692307692307692, val_acc: 0.3627450980392157, train_loss: 1.7240401621703263, val_loss: 1.6976339536554672 (9 / 80)\n",
            "train_acc: 0.3087912087912088, val_acc: 0.28431372549019607, train_loss: 1.7024694593398126, val_loss: 1.668092924005845 (10 / 80)\n",
            "train_acc: 0.2923076923076923, val_acc: 0.3137254901960784, train_loss: 1.6762328434776474, val_loss: 1.6082380869809318 (11 / 80)\n",
            "train_acc: 0.31758241758241756, val_acc: 0.29411764705882354, train_loss: 1.6336271223131118, val_loss: 1.5540019028327043 (12 / 80)\n",
            "train_acc: 0.3230769230769231, val_acc: 0.3333333333333333, train_loss: 1.5996042276476765, val_loss: 1.5500667375676773 (13 / 80)\n",
            "train_acc: 0.33186813186813185, val_acc: 0.3333333333333333, train_loss: 1.5763293596414418, val_loss: 1.5510720224941479 (14 / 80)\n",
            "train_acc: 0.31648351648351647, val_acc: 0.3431372549019608, train_loss: 1.564551169007689, val_loss: 1.5192564550568075 (15 / 80)\n",
            "train_acc: 0.35714285714285715, val_acc: 0.37254901960784315, train_loss: 1.528354217324938, val_loss: 1.473861880162183 (16 / 80)\n",
            "train_acc: 0.3395604395604396, val_acc: 0.3627450980392157, train_loss: 1.5421416209294245, val_loss: 1.4758312281440287 (17 / 80)\n",
            "train_acc: 0.35384615384615387, val_acc: 0.38235294117647056, train_loss: 1.5087841339818724, val_loss: 1.4695254494162167 (18 / 80)\n",
            "train_acc: 0.3516483516483517, val_acc: 0.3431372549019608, train_loss: 1.501183201454498, val_loss: 1.4670302938012516 (19 / 80)\n",
            "train_acc: 0.35714285714285715, val_acc: 0.3235294117647059, train_loss: 1.5030384092540532, val_loss: 1.4564948748139774 (20 / 80)\n",
            "train_acc: 0.35824175824175825, val_acc: 0.4411764705882353, train_loss: 1.4991775208777123, val_loss: 1.4253860606866724 (21 / 80)\n",
            "train_acc: 0.37032967032967035, val_acc: 0.38235294117647056, train_loss: 1.4650829570634025, val_loss: 1.4503713776083553 (22 / 80)\n",
            "train_acc: 0.37362637362637363, val_acc: 0.38235294117647056, train_loss: 1.4398299153034504, val_loss: 1.4410484994159025 (23 / 80)\n",
            "train_acc: 0.36923076923076925, val_acc: 0.4117647058823529, train_loss: 1.4952462831696312, val_loss: 1.414544799748589 (24 / 80)\n",
            "train_acc: 0.3824175824175824, val_acc: 0.45098039215686275, train_loss: 1.4569966369932825, val_loss: 1.410660074037664 (25 / 80)\n",
            "train_acc: 0.38351648351648354, val_acc: 0.45098039215686275, train_loss: 1.4240696212092598, val_loss: 1.3550361955867094 (26 / 80)\n",
            "train_acc: 0.3967032967032967, val_acc: 0.3431372549019608, train_loss: 1.435439124932656, val_loss: 1.442472368478775 (27 / 80)\n",
            "train_acc: 0.4021978021978022, val_acc: 0.35294117647058826, train_loss: 1.4104111407484328, val_loss: 1.3775891836951761 (28 / 80)\n",
            "train_acc: 0.378021978021978, val_acc: 0.3627450980392157, train_loss: 1.4233273601138985, val_loss: 1.3838980618645162 (29 / 80)\n",
            "train_acc: 0.41318681318681316, val_acc: 0.4411764705882353, train_loss: 1.4111911645302406, val_loss: 1.3278482906958635 (30 / 80)\n",
            "train_acc: 0.3945054945054945, val_acc: 0.43137254901960786, train_loss: 1.400770161309085, val_loss: 1.2910074854598326 (31 / 80)\n",
            "train_acc: 0.41208791208791207, val_acc: 0.5098039215686274, train_loss: 1.3723314153624104, val_loss: 1.2943446460892172 (32 / 80)\n",
            "train_acc: 0.42637362637362636, val_acc: 0.3235294117647059, train_loss: 1.374530521919439, val_loss: 1.4085574781193453 (33 / 80)\n",
            "train_acc: 0.4142857142857143, val_acc: 0.5392156862745098, train_loss: 1.3988094348828872, val_loss: 1.2517236734137815 (34 / 80)\n",
            "train_acc: 0.42637362637362636, val_acc: 0.5, train_loss: 1.3431547147887093, val_loss: 1.2481311051284565 (35 / 80)\n",
            "train_acc: 0.43846153846153846, val_acc: 0.46078431372549017, train_loss: 1.344506556909163, val_loss: 1.251106272725498 (36 / 80)\n",
            "train_acc: 0.42967032967032964, val_acc: 0.5490196078431373, train_loss: 1.3315385029211149, val_loss: 1.2130105548045214 (37 / 80)\n",
            "train_acc: 0.42637362637362636, val_acc: 0.5, train_loss: 1.3313071748057566, val_loss: 1.2159935376223396 (38 / 80)\n",
            "train_acc: 0.43956043956043955, val_acc: 0.5196078431372549, train_loss: 1.3263565582888468, val_loss: 1.1949142428005444 (39 / 80)\n",
            "train_acc: 0.46593406593406594, val_acc: 0.5490196078431373, train_loss: 1.2894176265040598, val_loss: 1.1800793146385866 (40 / 80)\n",
            "train_acc: 0.4472527472527473, val_acc: 0.5294117647058824, train_loss: 1.3055092885599031, val_loss: 1.2247902680845821 (41 / 80)\n",
            "train_acc: 0.45164835164835165, val_acc: 0.5294117647058824, train_loss: 1.2840593333427723, val_loss: 1.2074211590430315 (42 / 80)\n",
            "train_acc: 0.48021978021978023, val_acc: 0.4803921568627451, train_loss: 1.2782289195846726, val_loss: 1.226941704750061 (43 / 80)\n",
            "train_acc: 0.46153846153846156, val_acc: 0.5588235294117647, train_loss: 1.3003795490815089, val_loss: 1.189926086103215 (44 / 80)\n",
            "train_acc: 0.45714285714285713, val_acc: 0.49019607843137253, train_loss: 1.2704297398472881, val_loss: 1.1767351837719189 (45 / 80)\n",
            "train_acc: 0.4736263736263736, val_acc: 0.5294117647058824, train_loss: 1.2605915763220945, val_loss: 1.1269203126430511 (46 / 80)\n",
            "train_acc: 0.4835164835164835, val_acc: 0.5196078431372549, train_loss: 1.240802190723, val_loss: 1.1521806944819057 (47 / 80)\n",
            "train_acc: 0.47912087912087914, val_acc: 0.5196078431372549, train_loss: 1.2370233634671013, val_loss: 1.1730535223203546 (48 / 80)\n",
            "train_acc: 0.5263736263736264, val_acc: 0.5784313725490197, train_loss: 1.1719095891648597, val_loss: 1.1052813845522262 (49 / 80)\n",
            "train_acc: 0.5087912087912088, val_acc: 0.5392156862745098, train_loss: 1.1870280240918254, val_loss: 1.1357025398927576 (50 / 80)\n",
            "train_acc: 0.49230769230769234, val_acc: 0.5980392156862745, train_loss: 1.2036983177557097, val_loss: 1.1030066241236294 (51 / 80)\n",
            "train_acc: 0.521978021978022, val_acc: 0.5490196078431373, train_loss: 1.1765211979766468, val_loss: 1.07860844801454 (52 / 80)\n",
            "train_acc: 0.5208791208791209, val_acc: 0.5588235294117647, train_loss: 1.1695120483309358, val_loss: 1.113357440513723 (53 / 80)\n",
            "train_acc: 0.521978021978022, val_acc: 0.5980392156862745, train_loss: 1.1578069707194527, val_loss: 1.0752237912486582 (54 / 80)\n",
            "train_acc: 0.5010989010989011, val_acc: 0.5686274509803921, train_loss: 1.1803662988510761, val_loss: 1.0794651227838852 (55 / 80)\n",
            "train_acc: 0.5538461538461539, val_acc: 0.5392156862745098, train_loss: 1.1364301804657821, val_loss: 1.1524443293319029 (56 / 80)\n",
            "train_acc: 0.5351648351648352, val_acc: 0.5392156862745098, train_loss: 1.1523655920238285, val_loss: 1.0693574144559748 (57 / 80)\n",
            "train_acc: 0.5274725274725275, val_acc: 0.5490196078431373, train_loss: 1.1254678406558194, val_loss: 1.0614018177284914 (58 / 80)\n",
            "train_acc: 0.5505494505494506, val_acc: 0.5980392156862745, train_loss: 1.1168683910107875, val_loss: 1.0549411177635193 (59 / 80)\n",
            "train_acc: 0.5461538461538461, val_acc: 0.5588235294117647, train_loss: 1.117151617545348, val_loss: 1.0378777612658108 (60 / 80)\n",
            "train_acc: 0.5428571428571428, val_acc: 0.5588235294117647, train_loss: 1.116748710320546, val_loss: 1.0304832931827097 (61 / 80)\n",
            "train_acc: 0.545054945054945, val_acc: 0.5784313725490197, train_loss: 1.1346756365928021, val_loss: 1.0419362555531895 (62 / 80)\n",
            "train_acc: 0.5373626373626373, val_acc: 0.5686274509803921, train_loss: 1.0980236508034087, val_loss: 1.0277281578849344 (63 / 80)\n",
            "train_acc: 0.5604395604395604, val_acc: 0.5980392156862745, train_loss: 1.0748824861678448, val_loss: 1.0011466653908 (64 / 80)\n",
            "train_acc: 0.5527472527472528, val_acc: 0.5196078431372549, train_loss: 1.0905032263352321, val_loss: 1.0318945435916675 (65 / 80)\n",
            "train_acc: 0.5637362637362637, val_acc: 0.5882352941176471, train_loss: 1.0744906392071274, val_loss: 1.0586456814232994 (66 / 80)\n",
            "train_acc: 0.5604395604395604, val_acc: 0.5588235294117647, train_loss: 1.071547072143345, val_loss: 1.0312606867621927 (67 / 80)\n",
            "train_acc: 0.5912087912087912, val_acc: 0.6078431372549019, train_loss: 1.0340900937279502, val_loss: 1.0181405561811783 (68 / 80)\n",
            "train_acc: 0.5681318681318681, val_acc: 0.6078431372549019, train_loss: 1.0603960013651585, val_loss: 1.0903533399105072 (69 / 80)\n",
            "train_acc: 0.5945054945054945, val_acc: 0.5490196078431373, train_loss: 1.0594107728410553, val_loss: 0.9782932681195876 (70 / 80)\n",
            "train_acc: 0.5923076923076923, val_acc: 0.5686274509803921, train_loss: 1.0191901057958603, val_loss: 0.9981898875797496 (71 / 80)\n",
            "train_acc: 0.5835164835164836, val_acc: 0.5686274509803921, train_loss: 1.0598218658468226, val_loss: 0.9917539884062374 (72 / 80)\n",
            "train_acc: 0.5901098901098901, val_acc: 0.5784313725490197, train_loss: 1.053384771320846, val_loss: 1.0333059009383707 (73 / 80)\n",
            "train_acc: 0.5846153846153846, val_acc: 0.6078431372549019, train_loss: 1.025180787372065, val_loss: 0.9919548613183639 (74 / 80)\n",
            "train_acc: 0.6043956043956044, val_acc: 0.5490196078431373, train_loss: 1.0026413052619159, val_loss: 1.0521406776764815 (75 / 80)\n",
            "train_acc: 0.589010989010989, val_acc: 0.5784313725490197, train_loss: 1.0152848293165584, val_loss: 1.0136720695916344 (76 / 80)\n",
            "train_acc: 0.6010989010989011, val_acc: 0.5882352941176471, train_loss: 0.990724406858067, val_loss: 0.9142688284902012 (77 / 80)\n",
            "train_acc: 0.6010989010989011, val_acc: 0.5980392156862745, train_loss: 0.9731772881942791, val_loss: 0.9394807272097644 (78 / 80)\n",
            "train_acc: 0.6076923076923076, val_acc: 0.6372549019607843, train_loss: 0.9891592777692355, val_loss: 0.9874804037458756 (79 / 80)\n",
            "train_acc: 0.6263736263736264, val_acc: 0.5588235294117647, train_loss: 0.9768319862229483, val_loss: 0.987724244594574 (80 / 80)\n",
            "lr 0.00020981990349966764, batch 9, decay 3.88674174584636e-05, gamma 0.4135686316820952, val accuracy 0.6372549019607843, val loss 0.9874804037458756 [2 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.1805542726749927e-05, 'batch_size': 8, 'weight_decay': 0.00031703296970095194, 'gamma': 0.01620498665292351}\n",
            "train_acc: 0.2175824175824176, val_acc: 0.17647058823529413, train_loss: 1.7883332881298695, val_loss: 1.7888304112004298 (1 / 80)\n",
            "train_acc: 0.17362637362637362, val_acc: 0.19607843137254902, train_loss: 1.7892572798571744, val_loss: 1.788355967577766 (2 / 80)\n",
            "train_acc: 0.17912087912087912, val_acc: 0.19607843137254902, train_loss: 1.7900553868367122, val_loss: 1.7879552817812152 (3 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.19607843137254902, train_loss: 1.7882128503296402, val_loss: 1.7874726851781209 (4 / 80)\n",
            "train_acc: 0.18021978021978022, val_acc: 0.18627450980392157, train_loss: 1.7877824361507708, val_loss: 1.7869969816768871 (5 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.19607843137254902, train_loss: 1.7877550567899432, val_loss: 1.7865509636261885 (6 / 80)\n",
            "train_acc: 0.2153846153846154, val_acc: 0.20588235294117646, train_loss: 1.7861663894338922, val_loss: 1.7861028395447076 (7 / 80)\n",
            "train_acc: 0.2021978021978022, val_acc: 0.20588235294117646, train_loss: 1.7864950966049027, val_loss: 1.7856635463004018 (8 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.22549019607843138, train_loss: 1.7864920293891824, val_loss: 1.7852302602693146 (9 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.22549019607843138, train_loss: 1.7857430968965804, val_loss: 1.7847614101335114 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1967032967032967\n",
            "lr 1.1805542726749927e-05, batch 8, decay 0.00031703296970095194, gamma 0.01620498665292351, val accuracy 0.22549019607843138, val loss 1.7852302602693146 [3 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0006412328152012105, 'batch_size': 9, 'weight_decay': 2.0060591897123895e-05, 'gamma': 0.3072891585440319}\n",
            "train_acc: 0.17802197802197803, val_acc: 0.18627450980392157, train_loss: 1.7832208109425975, val_loss: 1.768639301552492 (1 / 80)\n",
            "train_acc: 0.19230769230769232, val_acc: 0.17647058823529413, train_loss: 1.7638474206348043, val_loss: 1.752227569327635 (2 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.24509803921568626, train_loss: 1.7570037899436532, val_loss: 1.737082186867209 (3 / 80)\n",
            "train_acc: 0.24835164835164836, val_acc: 0.20588235294117646, train_loss: 1.7419371600989457, val_loss: 1.7160542712492102 (4 / 80)\n",
            "train_acc: 0.27912087912087913, val_acc: 0.3137254901960784, train_loss: 1.6766768517075004, val_loss: 1.6058328256887549 (5 / 80)\n",
            "train_acc: 0.2945054945054945, val_acc: 0.2647058823529412, train_loss: 1.6681006185301057, val_loss: 1.6312168030177845 (6 / 80)\n",
            "train_acc: 0.3516483516483517, val_acc: 0.28431372549019607, train_loss: 1.5812477529703917, val_loss: 1.5621878504753113 (7 / 80)\n",
            "train_acc: 0.31648351648351647, val_acc: 0.3235294117647059, train_loss: 1.5490322910822354, val_loss: 1.5229159032597261 (8 / 80)\n",
            "train_acc: 0.3516483516483517, val_acc: 0.3627450980392157, train_loss: 1.5194747315658317, val_loss: 1.4228635219966663 (9 / 80)\n",
            "train_acc: 0.36813186813186816, val_acc: 0.38235294117647056, train_loss: 1.4852877618192317, val_loss: 1.4151048029170317 (10 / 80)\n",
            "train_acc: 0.35384615384615387, val_acc: 0.39215686274509803, train_loss: 1.4873452707961365, val_loss: 1.3711950323160957 (11 / 80)\n",
            "train_acc: 0.3758241758241758, val_acc: 0.3627450980392157, train_loss: 1.4569095360708761, val_loss: 1.3394307494163513 (12 / 80)\n",
            "train_acc: 0.3802197802197802, val_acc: 0.4117647058823529, train_loss: 1.4120727540372493, val_loss: 1.3154634342474096 (13 / 80)\n",
            "train_acc: 0.3967032967032967, val_acc: 0.49019607843137253, train_loss: 1.400964535199679, val_loss: 1.2854812793871935 (14 / 80)\n",
            "train_acc: 0.3923076923076923, val_acc: 0.4803921568627451, train_loss: 1.4076325892092107, val_loss: 1.2667328610139734 (15 / 80)\n",
            "train_acc: 0.3945054945054945, val_acc: 0.43137254901960786, train_loss: 1.3750220041353625, val_loss: 1.2839585577740389 (16 / 80)\n",
            "train_acc: 0.3956043956043956, val_acc: 0.4019607843137255, train_loss: 1.3723933903070598, val_loss: 1.2480629261802225 (17 / 80)\n",
            "train_acc: 0.4208791208791209, val_acc: 0.4803921568627451, train_loss: 1.3485773736303979, val_loss: 1.245998301926781 (18 / 80)\n",
            "train_acc: 0.4197802197802198, val_acc: 0.5784313725490197, train_loss: 1.3443548936110277, val_loss: 1.1984989134704365 (19 / 80)\n",
            "train_acc: 0.42967032967032964, val_acc: 0.5196078431372549, train_loss: 1.3147011085525975, val_loss: 1.1871120526510126 (20 / 80)\n",
            "train_acc: 0.45384615384615384, val_acc: 0.47058823529411764, train_loss: 1.2889000272357858, val_loss: 1.2038863774608164 (21 / 80)\n",
            "train_acc: 0.45714285714285713, val_acc: 0.45098039215686275, train_loss: 1.2862745738946475, val_loss: 1.2514439821243286 (22 / 80)\n",
            "train_acc: 0.46593406593406594, val_acc: 0.5196078431372549, train_loss: 1.258926763508346, val_loss: 1.173479206421796 (23 / 80)\n",
            "train_acc: 0.46703296703296704, val_acc: 0.5196078431372549, train_loss: 1.2465577293883314, val_loss: 1.148099760798847 (24 / 80)\n",
            "train_acc: 0.44945054945054946, val_acc: 0.5490196078431373, train_loss: 1.3032766340198099, val_loss: 1.0942537556676304 (25 / 80)\n",
            "train_acc: 0.4703296703296703, val_acc: 0.5, train_loss: 1.2196346772896065, val_loss: 1.1392984863589792 (26 / 80)\n",
            "train_acc: 0.5065934065934066, val_acc: 0.5490196078431373, train_loss: 1.1943349304435018, val_loss: 1.0628466027624466 (27 / 80)\n",
            "train_acc: 0.4758241758241758, val_acc: 0.5196078431372549, train_loss: 1.1726732804225042, val_loss: 1.1792055368423462 (28 / 80)\n",
            "train_acc: 0.5142857142857142, val_acc: 0.5294117647058824, train_loss: 1.195783162706501, val_loss: 1.0835586014915914 (29 / 80)\n",
            "train_acc: 0.5362637362637362, val_acc: 0.5196078431372549, train_loss: 1.1394036852396452, val_loss: 1.0496361956876867 (30 / 80)\n",
            "train_acc: 0.5285714285714286, val_acc: 0.5490196078431373, train_loss: 1.1177429634791154, val_loss: 0.9796155621023739 (31 / 80)\n",
            "train_acc: 0.5417582417582417, val_acc: 0.5392156862745098, train_loss: 1.097048103416359, val_loss: 0.9944946660714991 (32 / 80)\n",
            "train_acc: 0.5351648351648352, val_acc: 0.6078431372549019, train_loss: 1.102131425188138, val_loss: 0.954216843142229 (33 / 80)\n",
            "train_acc: 0.5747252747252747, val_acc: 0.5882352941176471, train_loss: 1.0148880200399146, val_loss: 1.0256887420135385 (34 / 80)\n",
            "train_acc: 0.5637362637362637, val_acc: 0.6470588235294118, train_loss: 1.0617291489800254, val_loss: 0.9135521737968221 (35 / 80)\n",
            "train_acc: 0.5967032967032967, val_acc: 0.5490196078431373, train_loss: 0.9895605661056854, val_loss: 1.1051266421290005 (36 / 80)\n",
            "train_acc: 0.6054945054945055, val_acc: 0.6470588235294118, train_loss: 0.9633197027575838, val_loss: 0.8432508707046509 (37 / 80)\n",
            "train_acc: 0.6263736263736264, val_acc: 0.5882352941176471, train_loss: 0.921399798137801, val_loss: 1.1317464934990686 (38 / 80)\n",
            "train_acc: 0.5912087912087912, val_acc: 0.6274509803921569, train_loss: 0.9985568744795663, val_loss: 0.813355168875526 (39 / 80)\n",
            "train_acc: 0.6153846153846154, val_acc: 0.5882352941176471, train_loss: 0.9450382134416602, val_loss: 0.9954078951302696 (40 / 80)\n",
            "train_acc: 0.6604395604395604, val_acc: 0.6666666666666666, train_loss: 0.8681664309003851, val_loss: 0.835374958374921 (41 / 80)\n",
            "train_acc: 0.667032967032967, val_acc: 0.5784313725490197, train_loss: 0.8393654713427627, val_loss: 0.9764993068049935 (42 / 80)\n",
            "train_acc: 0.6714285714285714, val_acc: 0.6764705882352942, train_loss: 0.815743130167107, val_loss: 0.7582473728586646 (43 / 80)\n",
            "train_acc: 0.7307692307692307, val_acc: 0.6470588235294118, train_loss: 0.692629683639977, val_loss: 0.9057745837113437 (44 / 80)\n",
            "train_acc: 0.7252747252747253, val_acc: 0.6862745098039216, train_loss: 0.7140118861100175, val_loss: 0.9348221631611094 (45 / 80)\n",
            "train_acc: 0.721978021978022, val_acc: 0.6666666666666666, train_loss: 0.7103262704971073, val_loss: 0.8274445441715857 (46 / 80)\n",
            "train_acc: 0.7582417582417582, val_acc: 0.6666666666666666, train_loss: 0.6398921781843835, val_loss: 0.7719061497379752 (47 / 80)\n",
            "train_acc: 0.7857142857142857, val_acc: 0.696078431372549, train_loss: 0.5362741507671691, val_loss: 0.752138182082597 (48 / 80)\n",
            "train_acc: 0.8307692307692308, val_acc: 0.6862745098039216, train_loss: 0.4502814872620197, val_loss: 0.7436805085881668 (49 / 80)\n",
            "train_acc: 0.8571428571428571, val_acc: 0.6862745098039216, train_loss: 0.37605773222397315, val_loss: 0.7229622812832103 (50 / 80)\n",
            "train_acc: 0.8681318681318682, val_acc: 0.7058823529411765, train_loss: 0.33221057699686224, val_loss: 0.824057103079908 (51 / 80)\n",
            "train_acc: 0.8912087912087913, val_acc: 0.7058823529411765, train_loss: 0.3095585439661211, val_loss: 0.7904114302466897 (52 / 80)\n",
            "train_acc: 0.8934065934065935, val_acc: 0.7254901960784313, train_loss: 0.28308216244964807, val_loss: 0.8506371343837065 (53 / 80)\n",
            "train_acc: 0.9010989010989011, val_acc: 0.7450980392156863, train_loss: 0.25092061846670544, val_loss: 0.7964265039738487 (54 / 80)\n",
            "train_acc: 0.8978021978021978, val_acc: 0.7352941176470589, train_loss: 0.2719435325721381, val_loss: 0.7985509127816733 (55 / 80)\n",
            "train_acc: 0.9285714285714286, val_acc: 0.696078431372549, train_loss: 0.21295664951839782, val_loss: 0.829382408191176 (56 / 80)\n",
            "train_acc: 0.9197802197802197, val_acc: 0.7254901960784313, train_loss: 0.23328438918976174, val_loss: 0.8377303100903245 (57 / 80)\n",
            "overfit -> train_accuracy 0.9461538461538461, val_accuracy 0.696078431372549\n",
            "lr 0.0006412328152012105, batch 9, decay 2.0060591897123895e-05, gamma 0.3072891585440319, val accuracy 0.7450980392156863, val loss 0.7964265039738487 [4 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 7.759254569036079e-05, 'batch_size': 11, 'weight_decay': 0.00033178416187367235, 'gamma': 0.6334285534378139}\n",
            "train_acc: 0.1835164835164835, val_acc: 0.1568627450980392, train_loss: 1.7895094791611472, val_loss: 1.788376861927556 (1 / 80)\n",
            "train_acc: 0.16153846153846155, val_acc: 0.16666666666666666, train_loss: 1.787950474351317, val_loss: 1.7863637781610675 (2 / 80)\n",
            "train_acc: 0.15714285714285714, val_acc: 0.21568627450980393, train_loss: 1.786654089309357, val_loss: 1.7844603003240098 (3 / 80)\n",
            "train_acc: 0.17692307692307693, val_acc: 0.18627450980392157, train_loss: 1.7847329496027349, val_loss: 1.782599035431357 (4 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.18627450980392157, train_loss: 1.7841472278584491, val_loss: 1.7806524272058524 (5 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.18627450980392157, train_loss: 1.7812507203647068, val_loss: 1.778592702220468 (6 / 80)\n",
            "train_acc: 0.19230769230769232, val_acc: 0.18627450980392157, train_loss: 1.7797663215752486, val_loss: 1.7763630747795105 (7 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.18627450980392157, train_loss: 1.7769924483456454, val_loss: 1.7741667523103601 (8 / 80)\n",
            "train_acc: 0.17692307692307693, val_acc: 0.18627450980392157, train_loss: 1.7746450711082626, val_loss: 1.7714191067452525 (9 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.18627450980392157, train_loss: 1.7717771038904295, val_loss: 1.768751884208006 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18461538461538463\n",
            "lr 7.759254569036079e-05, batch 11, decay 0.00033178416187367235, gamma 0.6334285534378139, val accuracy 0.21568627450980393, val loss 1.7844603003240098 [5 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 3.990261575595048e-05, 'batch_size': 11, 'weight_decay': 1.6184596592512047e-06, 'gamma': 0.1521212050821678}\n",
            "train_acc: 0.17472527472527472, val_acc: 0.18627450980392157, train_loss: 1.7926029562950134, val_loss: 1.7905273647869335 (1 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.18627450980392157, train_loss: 1.7905043134322534, val_loss: 1.7895179133789212 (2 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.18627450980392157, train_loss: 1.790100683746757, val_loss: 1.788546833337522 (3 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.7890272815148909, val_loss: 1.7876535420324289 (4 / 80)\n",
            "train_acc: 0.1912087912087912, val_acc: 0.18627450980392157, train_loss: 1.7884168867226486, val_loss: 1.78679840704974 (5 / 80)\n",
            "train_acc: 0.2, val_acc: 0.18627450980392157, train_loss: 1.78724046591874, val_loss: 1.785848125523212 (6 / 80)\n",
            "train_acc: 0.1813186813186813, val_acc: 0.20588235294117646, train_loss: 1.7873539223775758, val_loss: 1.7850082867285784 (7 / 80)\n",
            "train_acc: 0.21208791208791208, val_acc: 0.22549019607843138, train_loss: 1.7851392823261218, val_loss: 1.7841327354019763 (8 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.27450980392156865, train_loss: 1.7852445874895368, val_loss: 1.7831954254823572 (9 / 80)\n",
            "train_acc: 0.2010989010989011, val_acc: 0.29411764705882354, train_loss: 1.784054762845511, val_loss: 1.7823139779707964 (10 / 80)\n",
            "underfit -> train_accuracy = 0.2010989010989011\n",
            "lr 3.990261575595048e-05, batch 11, decay 1.6184596592512047e-06, gamma 0.1521212050821678, val accuracy 0.29411764705882354, val loss 1.7823139779707964 [6 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0004104615342972818, 'batch_size': 8, 'weight_decay': 7.10320647975993e-05, 'gamma': 0.08417856079385061}\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.783191214288984, val_loss: 1.7707008006525975 (1 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.7657574622185677, val_loss: 1.7509338715497185 (2 / 80)\n",
            "train_acc: 0.2087912087912088, val_acc: 0.18627450980392157, train_loss: 1.7581998007638113, val_loss: 1.739841143290202 (3 / 80)\n",
            "train_acc: 0.1956043956043956, val_acc: 0.2549019607843137, train_loss: 1.7552603532979776, val_loss: 1.7277071966844446 (4 / 80)\n",
            "train_acc: 0.26593406593406593, val_acc: 0.29411764705882354, train_loss: 1.7289007679446713, val_loss: 1.6996820861218023 (5 / 80)\n",
            "train_acc: 0.2967032967032967, val_acc: 0.2549019607843137, train_loss: 1.7021355665647067, val_loss: 1.653042381885005 (6 / 80)\n",
            "train_acc: 0.2978021978021978, val_acc: 0.23529411764705882, train_loss: 1.6443301591244373, val_loss: 1.5961201401317822 (7 / 80)\n",
            "train_acc: 0.34725274725274724, val_acc: 0.3333333333333333, train_loss: 1.5909298372792673, val_loss: 1.6009405360502356 (8 / 80)\n",
            "train_acc: 0.33296703296703295, val_acc: 0.30392156862745096, train_loss: 1.5692645966351688, val_loss: 1.5922930637995403 (9 / 80)\n",
            "train_acc: 0.33296703296703295, val_acc: 0.38235294117647056, train_loss: 1.5088145468261216, val_loss: 1.4509550800510482 (10 / 80)\n",
            "train_acc: 0.3384615384615385, val_acc: 0.37254901960784315, train_loss: 1.5327564543420142, val_loss: 1.4560203528871722 (11 / 80)\n",
            "train_acc: 0.35604395604395606, val_acc: 0.3431372549019608, train_loss: 1.4871624495956923, val_loss: 1.4706047212376314 (12 / 80)\n",
            "train_acc: 0.3824175824175824, val_acc: 0.3431372549019608, train_loss: 1.4662375358434825, val_loss: 1.3801018771003275 (13 / 80)\n",
            "train_acc: 0.3626373626373626, val_acc: 0.30392156862745096, train_loss: 1.4616578985046556, val_loss: 1.512326722051583 (14 / 80)\n",
            "train_acc: 0.38351648351648354, val_acc: 0.39215686274509803, train_loss: 1.44366414808965, val_loss: 1.4300181374830359 (15 / 80)\n",
            "train_acc: 0.3769230769230769, val_acc: 0.28431372549019607, train_loss: 1.4253232209237068, val_loss: 1.5964455908420039 (16 / 80)\n",
            "train_acc: 0.3923076923076923, val_acc: 0.3431372549019608, train_loss: 1.414274743625096, val_loss: 1.5002360717923033 (17 / 80)\n",
            "train_acc: 0.3956043956043956, val_acc: 0.3333333333333333, train_loss: 1.421909955569676, val_loss: 1.3906995525547103 (18 / 80)\n",
            "train_acc: 0.42527472527472526, val_acc: 0.45098039215686275, train_loss: 1.3746336651372386, val_loss: 1.2692347320855832 (19 / 80)\n",
            "train_acc: 0.43626373626373627, val_acc: 0.4411764705882353, train_loss: 1.3669743831341084, val_loss: 1.2559738369549023 (20 / 80)\n",
            "train_acc: 0.4021978021978022, val_acc: 0.47058823529411764, train_loss: 1.335854479768774, val_loss: 1.213271234549728 (21 / 80)\n",
            "train_acc: 0.42417582417582417, val_acc: 0.4411764705882353, train_loss: 1.3651785090729431, val_loss: 1.2092147901946424 (22 / 80)\n",
            "train_acc: 0.4208791208791209, val_acc: 0.38235294117647056, train_loss: 1.3366551480450473, val_loss: 1.317289832760306 (23 / 80)\n",
            "train_acc: 0.44945054945054946, val_acc: 0.4411764705882353, train_loss: 1.281608864239284, val_loss: 1.1696519442633087 (24 / 80)\n",
            "train_acc: 0.44065934065934065, val_acc: 0.47058823529411764, train_loss: 1.305286261013576, val_loss: 1.3067352748384662 (25 / 80)\n",
            "train_acc: 0.46483516483516485, val_acc: 0.5098039215686274, train_loss: 1.2733013485814189, val_loss: 1.1738591217527203 (26 / 80)\n",
            "train_acc: 0.432967032967033, val_acc: 0.5686274509803921, train_loss: 1.274648158628862, val_loss: 1.1321917678795608 (27 / 80)\n",
            "train_acc: 0.43626373626373627, val_acc: 0.5196078431372549, train_loss: 1.2764923622319986, val_loss: 1.2299481840694653 (28 / 80)\n",
            "train_acc: 0.489010989010989, val_acc: 0.49019607843137253, train_loss: 1.2211090140290313, val_loss: 1.223907837680742 (29 / 80)\n",
            "train_acc: 0.49230769230769234, val_acc: 0.5392156862745098, train_loss: 1.228450740824689, val_loss: 1.0660772580726474 (30 / 80)\n",
            "train_acc: 0.5021978021978022, val_acc: 0.5196078431372549, train_loss: 1.2031189051303235, val_loss: 1.094379696191526 (31 / 80)\n",
            "train_acc: 0.48021978021978023, val_acc: 0.5, train_loss: 1.1953813707435523, val_loss: 1.0873980089729907 (32 / 80)\n",
            "train_acc: 0.49230769230769234, val_acc: 0.5588235294117647, train_loss: 1.1494439671327779, val_loss: 1.073454550668305 (33 / 80)\n",
            "train_acc: 0.5153846153846153, val_acc: 0.5098039215686274, train_loss: 1.13927819113155, val_loss: 1.0931141633613437 (34 / 80)\n",
            "train_acc: 0.5439560439560439, val_acc: 0.5588235294117647, train_loss: 1.1358632051027737, val_loss: 1.076231697026421 (35 / 80)\n",
            "train_acc: 0.5285714285714286, val_acc: 0.5490196078431373, train_loss: 1.1156859965114803, val_loss: 1.029901055728688 (36 / 80)\n",
            "train_acc: 0.5417582417582417, val_acc: 0.5686274509803921, train_loss: 1.0930431017508873, val_loss: 0.9871011703622108 (37 / 80)\n",
            "train_acc: 0.5384615384615384, val_acc: 0.5784313725490197, train_loss: 1.078053901221726, val_loss: 0.9665959348865584 (38 / 80)\n",
            "train_acc: 0.5615384615384615, val_acc: 0.5588235294117647, train_loss: 1.0521009248691602, val_loss: 1.0662641969381594 (39 / 80)\n",
            "train_acc: 0.5846153846153846, val_acc: 0.45098039215686275, train_loss: 1.0604812729489672, val_loss: 1.2419154316771264 (40 / 80)\n",
            "train_acc: 0.589010989010989, val_acc: 0.6372549019607843, train_loss: 0.9954644386584942, val_loss: 0.8941286860727796 (41 / 80)\n",
            "train_acc: 0.6010989010989011, val_acc: 0.5784313725490197, train_loss: 0.9909214626956772, val_loss: 0.981063750444674 (42 / 80)\n",
            "train_acc: 0.6153846153846154, val_acc: 0.6176470588235294, train_loss: 0.971975168291029, val_loss: 0.8642498570330003 (43 / 80)\n",
            "train_acc: 0.6285714285714286, val_acc: 0.6372549019607843, train_loss: 0.9111969599357018, val_loss: 0.9188448319248125 (44 / 80)\n",
            "train_acc: 0.6263736263736264, val_acc: 0.5980392156862745, train_loss: 0.8808801197743678, val_loss: 0.8790921305908876 (45 / 80)\n",
            "train_acc: 0.6252747252747253, val_acc: 0.6568627450980392, train_loss: 0.9032494010506096, val_loss: 0.8599195106356752 (46 / 80)\n",
            "train_acc: 0.6736263736263737, val_acc: 0.6176470588235294, train_loss: 0.8322600651573349, val_loss: 0.9197769726023954 (47 / 80)\n",
            "train_acc: 0.6835164835164835, val_acc: 0.5686274509803921, train_loss: 0.782945358491206, val_loss: 0.925112432124568 (48 / 80)\n",
            "train_acc: 0.743956043956044, val_acc: 0.6372549019607843, train_loss: 0.6579646950403413, val_loss: 0.7800455911486757 (49 / 80)\n",
            "train_acc: 0.7934065934065934, val_acc: 0.6666666666666666, train_loss: 0.593528773692938, val_loss: 0.7798711458841959 (50 / 80)\n",
            "train_acc: 0.7923076923076923, val_acc: 0.6470588235294118, train_loss: 0.5748058688509595, val_loss: 0.7596829510202595 (51 / 80)\n",
            "train_acc: 0.7857142857142857, val_acc: 0.6470588235294118, train_loss: 0.5735304865863297, val_loss: 0.7244043595650617 (52 / 80)\n",
            "train_acc: 0.8076923076923077, val_acc: 0.6666666666666666, train_loss: 0.5403758203590309, val_loss: 0.7795278453359417 (53 / 80)\n",
            "train_acc: 0.8054945054945055, val_acc: 0.6666666666666666, train_loss: 0.5383461838895148, val_loss: 0.8152978180670271 (54 / 80)\n",
            "train_acc: 0.8065934065934066, val_acc: 0.6666666666666666, train_loss: 0.504132406102432, val_loss: 0.7152063601157245 (55 / 80)\n",
            "train_acc: 0.810989010989011, val_acc: 0.6666666666666666, train_loss: 0.5128031623232495, val_loss: 0.7459142289909662 (56 / 80)\n",
            "train_acc: 0.8175824175824176, val_acc: 0.7058823529411765, train_loss: 0.5177630979936202, val_loss: 0.7444202537630119 (57 / 80)\n",
            "train_acc: 0.8274725274725274, val_acc: 0.6666666666666666, train_loss: 0.47644160298200755, val_loss: 0.7054319124595791 (58 / 80)\n",
            "train_acc: 0.8296703296703297, val_acc: 0.6862745098039216, train_loss: 0.470299018575595, val_loss: 0.7497068246205648 (59 / 80)\n",
            "train_acc: 0.8395604395604396, val_acc: 0.6764705882352942, train_loss: 0.46177282752571525, val_loss: 0.6987003999597886 (60 / 80)\n",
            "train_acc: 0.8472527472527472, val_acc: 0.7156862745098039, train_loss: 0.4607674268575815, val_loss: 0.6960256006203446 (61 / 80)\n",
            "train_acc: 0.8395604395604396, val_acc: 0.6764705882352942, train_loss: 0.450726969228996, val_loss: 0.7111218790797627 (62 / 80)\n",
            "train_acc: 0.8406593406593407, val_acc: 0.696078431372549, train_loss: 0.45123847677157475, val_loss: 0.6739147036683326 (63 / 80)\n",
            "train_acc: 0.8516483516483516, val_acc: 0.6862745098039216, train_loss: 0.40164416818828375, val_loss: 0.7001670410527903 (64 / 80)\n",
            "train_acc: 0.8593406593406593, val_acc: 0.696078431372549, train_loss: 0.4140369510912633, val_loss: 0.672094279644536 (65 / 80)\n",
            "train_acc: 0.8604395604395605, val_acc: 0.696078431372549, train_loss: 0.39888962366423764, val_loss: 0.6688646498848411 (66 / 80)\n",
            "train_acc: 0.8582417582417582, val_acc: 0.7254901960784313, train_loss: 0.4173235722950527, val_loss: 0.7141376079297533 (67 / 80)\n",
            "train_acc: 0.8505494505494505, val_acc: 0.696078431372549, train_loss: 0.387961232989699, val_loss: 0.7194182639028511 (68 / 80)\n",
            "train_acc: 0.8802197802197802, val_acc: 0.6862745098039216, train_loss: 0.36270647386273186, val_loss: 0.6876264775500578 (69 / 80)\n",
            "train_acc: 0.8582417582417582, val_acc: 0.7156862745098039, train_loss: 0.37560685499683844, val_loss: 0.6719322076030806 (70 / 80)\n",
            "train_acc: 0.8670329670329671, val_acc: 0.7058823529411765, train_loss: 0.34167634687580906, val_loss: 0.7011165595522114 (71 / 80)\n",
            "train_acc: 0.8923076923076924, val_acc: 0.7058823529411765, train_loss: 0.3308094045618078, val_loss: 0.7143062516754749 (72 / 80)\n",
            "train_acc: 0.8824175824175824, val_acc: 0.7156862745098039, train_loss: 0.34295851401575317, val_loss: 0.6769896722307392 (73 / 80)\n",
            "train_acc: 0.8791208791208791, val_acc: 0.7352941176470589, train_loss: 0.3549277841389834, val_loss: 0.7460817437545926 (74 / 80)\n",
            "train_acc: 0.8846153846153846, val_acc: 0.6764705882352942, train_loss: 0.32841075838922146, val_loss: 0.7639471719077989 (75 / 80)\n",
            "train_acc: 0.8934065934065935, val_acc: 0.7156862745098039, train_loss: 0.3250600795169453, val_loss: 0.6601954865105012 (76 / 80)\n",
            "train_acc: 0.8923076923076924, val_acc: 0.7254901960784313, train_loss: 0.3167897150902958, val_loss: 0.6981550723898644 (77 / 80)\n",
            "train_acc: 0.8681318681318682, val_acc: 0.7058823529411765, train_loss: 0.3405553175197853, val_loss: 0.6577776360745523 (78 / 80)\n",
            "train_acc: 0.8978021978021978, val_acc: 0.7156862745098039, train_loss: 0.3104540134524251, val_loss: 0.7280334254690245 (79 / 80)\n",
            "train_acc: 0.9065934065934066, val_acc: 0.7254901960784313, train_loss: 0.28369628479192543, val_loss: 0.6998521533666873 (80 / 80)\n",
            "lr 0.0004104615342972818, batch 8, decay 7.10320647975993e-05, gamma 0.08417856079385061, val accuracy 0.7352941176470589, val loss 0.7460817437545926 [7 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0004886554999950922, 'batch_size': 9, 'weight_decay': 0.00017637711708359805, 'gamma': 0.04172680729940897}\n",
            "train_acc: 0.1967032967032967, val_acc: 0.18627450980392157, train_loss: 1.787236960903629, val_loss: 1.775775225723491 (1 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.23529411764705882, train_loss: 1.7714719978007643, val_loss: 1.7556929833748762 (2 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.20588235294117646, train_loss: 1.7594565690218746, val_loss: 1.7463344230371363 (3 / 80)\n",
            "train_acc: 0.22527472527472528, val_acc: 0.27450980392156865, train_loss: 1.7548281201949487, val_loss: 1.729385288322673 (4 / 80)\n",
            "train_acc: 0.24835164835164836, val_acc: 0.22549019607843138, train_loss: 1.7283577664867862, val_loss: 1.697451444233165 (5 / 80)\n",
            "train_acc: 0.27692307692307694, val_acc: 0.3235294117647059, train_loss: 1.6929757291144067, val_loss: 1.6227414572940153 (6 / 80)\n",
            "train_acc: 0.3087912087912088, val_acc: 0.28431372549019607, train_loss: 1.6135104235711988, val_loss: 1.8023538519354427 (7 / 80)\n",
            "train_acc: 0.3076923076923077, val_acc: 0.2549019607843137, train_loss: 1.6141219901514576, val_loss: 1.5887084182570963 (8 / 80)\n",
            "train_acc: 0.3373626373626374, val_acc: 0.35294117647058826, train_loss: 1.5669490810279008, val_loss: 1.481117259053623 (9 / 80)\n",
            "train_acc: 0.34175824175824177, val_acc: 0.35294117647058826, train_loss: 1.5266043170467838, val_loss: 1.4522961518343758 (10 / 80)\n",
            "train_acc: 0.34725274725274724, val_acc: 0.35294117647058826, train_loss: 1.5231179327755184, val_loss: 1.441073419416652 (11 / 80)\n",
            "train_acc: 0.3967032967032967, val_acc: 0.37254901960784315, train_loss: 1.4405196632657733, val_loss: 1.4973939096226412 (12 / 80)\n",
            "train_acc: 0.378021978021978, val_acc: 0.4411764705882353, train_loss: 1.491678090802916, val_loss: 1.3881093281156875 (13 / 80)\n",
            "train_acc: 0.3813186813186813, val_acc: 0.4019607843137255, train_loss: 1.441121114217318, val_loss: 1.4302623131695915 (14 / 80)\n",
            "train_acc: 0.34395604395604396, val_acc: 0.46078431372549017, train_loss: 1.4574099446391011, val_loss: 1.3220725795801949 (15 / 80)\n",
            "train_acc: 0.3879120879120879, val_acc: 0.4215686274509804, train_loss: 1.419156327024921, val_loss: 1.3111941568991716 (16 / 80)\n",
            "train_acc: 0.3989010989010989, val_acc: 0.43137254901960786, train_loss: 1.3991707205117403, val_loss: 1.28265639964272 (17 / 80)\n",
            "train_acc: 0.3868131868131868, val_acc: 0.3235294117647059, train_loss: 1.390361657312938, val_loss: 1.4386794041184818 (18 / 80)\n",
            "train_acc: 0.4175824175824176, val_acc: 0.4215686274509804, train_loss: 1.3663270958832332, val_loss: 1.2548942408140968 (19 / 80)\n",
            "train_acc: 0.4032967032967033, val_acc: 0.46078431372549017, train_loss: 1.3769301648323353, val_loss: 1.255182955194922 (20 / 80)\n",
            "train_acc: 0.4197802197802198, val_acc: 0.4019607843137255, train_loss: 1.3536119761047782, val_loss: 1.335064880988177 (21 / 80)\n",
            "train_acc: 0.4318681318681319, val_acc: 0.5196078431372549, train_loss: 1.3161947855582603, val_loss: 1.201167527367087 (22 / 80)\n",
            "train_acc: 0.42417582417582417, val_acc: 0.46078431372549017, train_loss: 1.3171192666986486, val_loss: 1.194037511068232 (23 / 80)\n",
            "train_acc: 0.4208791208791209, val_acc: 0.38235294117647056, train_loss: 1.3307121421609607, val_loss: 1.4708548749194426 (24 / 80)\n",
            "train_acc: 0.44835164835164837, val_acc: 0.5098039215686274, train_loss: 1.3215955371385093, val_loss: 1.2000789554680096 (25 / 80)\n",
            "train_acc: 0.44175824175824174, val_acc: 0.5392156862745098, train_loss: 1.295839210329475, val_loss: 1.187595463850919 (26 / 80)\n",
            "train_acc: 0.45054945054945056, val_acc: 0.47058823529411764, train_loss: 1.2616948051112038, val_loss: 1.1769911559189068 (27 / 80)\n",
            "train_acc: 0.45164835164835165, val_acc: 0.4019607843137255, train_loss: 1.2806012163450429, val_loss: 1.2934126643573536 (28 / 80)\n",
            "train_acc: 0.47912087912087914, val_acc: 0.45098039215686275, train_loss: 1.2496581943480523, val_loss: 1.1689510625951431 (29 / 80)\n",
            "train_acc: 0.48131868131868133, val_acc: 0.4803921568627451, train_loss: 1.2260516285896301, val_loss: 1.133449203827802 (30 / 80)\n",
            "train_acc: 0.48021978021978023, val_acc: 0.5686274509803921, train_loss: 1.224803353207452, val_loss: 1.055570027407478 (31 / 80)\n",
            "train_acc: 0.4703296703296703, val_acc: 0.5294117647058824, train_loss: 1.2433328852548704, val_loss: 1.081836355083129 (32 / 80)\n",
            "train_acc: 0.5010989010989011, val_acc: 0.5686274509803921, train_loss: 1.1862936980121739, val_loss: 1.1008134992683636 (33 / 80)\n",
            "train_acc: 0.5175824175824176, val_acc: 0.49019607843137253, train_loss: 1.1564220165158365, val_loss: 1.2246175233055563 (34 / 80)\n",
            "train_acc: 0.5252747252747253, val_acc: 0.4803921568627451, train_loss: 1.1614521213285216, val_loss: 1.144399525488124 (35 / 80)\n",
            "train_acc: 0.554945054945055, val_acc: 0.6274509803921569, train_loss: 1.121833548768536, val_loss: 1.0283031866830938 (36 / 80)\n",
            "train_acc: 0.5252747252747253, val_acc: 0.5980392156862745, train_loss: 1.1308603682360807, val_loss: 1.0139061843647676 (37 / 80)\n",
            "train_acc: 0.5527472527472528, val_acc: 0.5392156862745098, train_loss: 1.082167133983675, val_loss: 1.0210042999071234 (38 / 80)\n",
            "train_acc: 0.5637362637362637, val_acc: 0.5392156862745098, train_loss: 1.0682399114409646, val_loss: 0.9469674510114333 (39 / 80)\n",
            "train_acc: 0.5538461538461539, val_acc: 0.6176470588235294, train_loss: 1.1099441492950524, val_loss: 0.9529949181220111 (40 / 80)\n",
            "train_acc: 0.610989010989011, val_acc: 0.5784313725490197, train_loss: 1.0095902874574556, val_loss: 1.046312302350998 (41 / 80)\n",
            "train_acc: 0.5923076923076923, val_acc: 0.6372549019607843, train_loss: 0.9949857250019745, val_loss: 0.8902884800644482 (42 / 80)\n",
            "train_acc: 0.5923076923076923, val_acc: 0.5294117647058824, train_loss: 0.9983617814032586, val_loss: 1.0734306451152353 (43 / 80)\n",
            "train_acc: 0.5978021978021978, val_acc: 0.6666666666666666, train_loss: 0.9662704901708351, val_loss: 0.890468047822223 (44 / 80)\n",
            "train_acc: 0.6362637362637362, val_acc: 0.6176470588235294, train_loss: 0.918307413308175, val_loss: 0.9553244297995287 (45 / 80)\n",
            "train_acc: 0.6461538461538462, val_acc: 0.5686274509803921, train_loss: 0.8672034895354575, val_loss: 0.9647322577588698 (46 / 80)\n",
            "train_acc: 0.6494505494505495, val_acc: 0.6274509803921569, train_loss: 0.8485919200129561, val_loss: 0.9711377497981576 (47 / 80)\n",
            "train_acc: 0.6395604395604395, val_acc: 0.6764705882352942, train_loss: 0.914520159134498, val_loss: 0.8708294051534989 (48 / 80)\n",
            "train_acc: 0.7252747252747253, val_acc: 0.7156862745098039, train_loss: 0.7048961924655097, val_loss: 0.7705318831345614 (49 / 80)\n",
            "train_acc: 0.7373626373626374, val_acc: 0.7254901960784313, train_loss: 0.6799351986784201, val_loss: 0.7771077138536117 (50 / 80)\n",
            "train_acc: 0.7318681318681318, val_acc: 0.7156862745098039, train_loss: 0.6939849732013849, val_loss: 0.7760653057519127 (51 / 80)\n",
            "train_acc: 0.756043956043956, val_acc: 0.7058823529411765, train_loss: 0.6434407370922329, val_loss: 0.7754241245634416 (52 / 80)\n",
            "train_acc: 0.7538461538461538, val_acc: 0.7352941176470589, train_loss: 0.6411885600496124, val_loss: 0.775491813526434 (53 / 80)\n",
            "train_acc: 0.7615384615384615, val_acc: 0.7254901960784313, train_loss: 0.6519475537997026, val_loss: 0.7706978093175327 (54 / 80)\n",
            "train_acc: 0.7824175824175824, val_acc: 0.7058823529411765, train_loss: 0.6169028657805788, val_loss: 0.7751676089623395 (55 / 80)\n",
            "train_acc: 0.7835164835164835, val_acc: 0.7254901960784313, train_loss: 0.6495926840701601, val_loss: 0.7754336560473722 (56 / 80)\n",
            "train_acc: 0.7593406593406593, val_acc: 0.7156862745098039, train_loss: 0.6254174556705978, val_loss: 0.7746643143541673 (57 / 80)\n",
            "train_acc: 0.7637362637362637, val_acc: 0.696078431372549, train_loss: 0.6029529884621337, val_loss: 0.776406482738607 (58 / 80)\n",
            "train_acc: 0.7747252747252747, val_acc: 0.696078431372549, train_loss: 0.5878870034610832, val_loss: 0.7834967208259246 (59 / 80)\n",
            "train_acc: 0.7813186813186813, val_acc: 0.7058823529411765, train_loss: 0.5899993583723739, val_loss: 0.7672567814588547 (60 / 80)\n",
            "train_acc: 0.778021978021978, val_acc: 0.696078431372549, train_loss: 0.5834581689847694, val_loss: 0.7756855426465764 (61 / 80)\n",
            "train_acc: 0.7802197802197802, val_acc: 0.7058823529411765, train_loss: 0.5646450362853952, val_loss: 0.777435764670372 (62 / 80)\n",
            "train_acc: 0.7692307692307693, val_acc: 0.696078431372549, train_loss: 0.5766181211386409, val_loss: 0.7555809205069262 (63 / 80)\n",
            "train_acc: 0.7923076923076923, val_acc: 0.7156862745098039, train_loss: 0.5509284344266404, val_loss: 0.760924285825561 (64 / 80)\n",
            "train_acc: 0.7956043956043956, val_acc: 0.7058823529411765, train_loss: 0.5674765702787337, val_loss: 0.7731204234501895 (65 / 80)\n",
            "train_acc: 0.8164835164835165, val_acc: 0.696078431372549, train_loss: 0.519574011862278, val_loss: 0.7670591298271628 (66 / 80)\n",
            "train_acc: 0.8021978021978022, val_acc: 0.7058823529411765, train_loss: 0.5324837054606978, val_loss: 0.7663095361169647 (67 / 80)\n",
            "train_acc: 0.7857142857142857, val_acc: 0.7058823529411765, train_loss: 0.5673315868272886, val_loss: 0.7698620820746702 (68 / 80)\n",
            "train_acc: 0.7923076923076923, val_acc: 0.696078431372549, train_loss: 0.5403274412011052, val_loss: 0.765409322345958 (69 / 80)\n",
            "train_acc: 0.8, val_acc: 0.6862745098039216, train_loss: 0.5434116225648712, val_loss: 0.7671640261131174 (70 / 80)\n",
            "train_acc: 0.7923076923076923, val_acc: 0.696078431372549, train_loss: 0.529703186419639, val_loss: 0.7561668718562407 (71 / 80)\n",
            "train_acc: 0.7923076923076923, val_acc: 0.7058823529411765, train_loss: 0.536943600318589, val_loss: 0.7642106194706524 (72 / 80)\n",
            "train_acc: 0.8153846153846154, val_acc: 0.6862745098039216, train_loss: 0.48153888665876543, val_loss: 0.7610372015658546 (73 / 80)\n",
            "train_acc: 0.8076923076923077, val_acc: 0.696078431372549, train_loss: 0.5271048966508646, val_loss: 0.764964280759587 (74 / 80)\n",
            "train_acc: 0.7945054945054945, val_acc: 0.7058823529411765, train_loss: 0.5303891156400953, val_loss: 0.7626255023128846 (75 / 80)\n",
            "train_acc: 0.8065934065934066, val_acc: 0.696078431372549, train_loss: 0.5301785362700184, val_loss: 0.7603616828427595 (76 / 80)\n",
            "train_acc: 0.8395604395604396, val_acc: 0.6862745098039216, train_loss: 0.4746144080227548, val_loss: 0.7501242555239621 (77 / 80)\n",
            "train_acc: 0.8274725274725274, val_acc: 0.6862745098039216, train_loss: 0.48012171127311476, val_loss: 0.7610271266278099 (78 / 80)\n",
            "train_acc: 0.810989010989011, val_acc: 0.696078431372549, train_loss: 0.49657183406608446, val_loss: 0.7694795499829685 (79 / 80)\n",
            "train_acc: 0.8153846153846154, val_acc: 0.7156862745098039, train_loss: 0.49713459015711325, val_loss: 0.7680152084897546 (80 / 80)\n",
            "lr 0.0004886554999950922, batch 9, decay 0.00017637711708359805, gamma 0.04172680729940897, val accuracy 0.7352941176470589, val loss 0.775491813526434 [8 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.185155224082468e-05, 'batch_size': 11, 'weight_decay': 7.39443257432403e-06, 'gamma': 0.09525539853506983}\n",
            "train_acc: 0.16593406593406593, val_acc: 0.18627450980392157, train_loss: 1.791928375684298, val_loss: 1.7901779611905415 (1 / 80)\n",
            "train_acc: 0.17032967032967034, val_acc: 0.18627450980392157, train_loss: 1.7915609195992186, val_loss: 1.7898094151534287 (2 / 80)\n",
            "train_acc: 0.17582417582417584, val_acc: 0.18627450980392157, train_loss: 1.7904417007834046, val_loss: 1.7894278495919471 (3 / 80)\n",
            "train_acc: 0.16813186813186815, val_acc: 0.18627450980392157, train_loss: 1.7898068198790917, val_loss: 1.7890672917459525 (4 / 80)\n",
            "train_acc: 0.21978021978021978, val_acc: 0.18627450980392157, train_loss: 1.79003544823154, val_loss: 1.788712034038469 (5 / 80)\n",
            "train_acc: 0.17582417582417584, val_acc: 0.18627450980392157, train_loss: 1.789581289789179, val_loss: 1.7883351117956872 (6 / 80)\n",
            "train_acc: 0.1956043956043956, val_acc: 0.18627450980392157, train_loss: 1.7894624638033436, val_loss: 1.787940617869882 (7 / 80)\n",
            "train_acc: 0.2021978021978022, val_acc: 0.18627450980392157, train_loss: 1.7894123065602647, val_loss: 1.7875706378151388 (8 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.18627450980392157, train_loss: 1.7887274081890399, val_loss: 1.7872006308798696 (9 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.7883383876674779, val_loss: 1.786809956326204 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18681318681318682\n",
            "lr 1.185155224082468e-05, batch 11, decay 7.39443257432403e-06, gamma 0.09525539853506983, val accuracy 0.18627450980392157, val loss 1.7901779611905415 [9 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00017878313076609675, 'batch_size': 13, 'weight_decay': 7.684183733904238e-06, 'gamma': 0.01315692733819065}\n",
            "train_acc: 0.16703296703296702, val_acc: 0.18627450980392157, train_loss: 1.7886000497000558, val_loss: 1.7865550108984405 (1 / 80)\n",
            "train_acc: 0.16263736263736264, val_acc: 0.18627450980392157, train_loss: 1.7857001968792507, val_loss: 1.78257108669655 (2 / 80)\n",
            "train_acc: 0.17472527472527472, val_acc: 0.18627450980392157, train_loss: 1.782771817275456, val_loss: 1.7784477776172114 (3 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.7776719876698086, val_loss: 1.773729214481279 (4 / 80)\n",
            "train_acc: 0.17582417582417584, val_acc: 0.18627450980392157, train_loss: 1.775036346912384, val_loss: 1.7689735936183555 (5 / 80)\n",
            "train_acc: 0.1813186813186813, val_acc: 0.18627450980392157, train_loss: 1.7689507109778269, val_loss: 1.7632756233215332 (6 / 80)\n",
            "train_acc: 0.18571428571428572, val_acc: 0.18627450980392157, train_loss: 1.7658259051186698, val_loss: 1.7577200917636646 (7 / 80)\n",
            "train_acc: 0.189010989010989, val_acc: 0.18627450980392157, train_loss: 1.7622146044458662, val_loss: 1.7536445961279028 (8 / 80)\n",
            "train_acc: 0.18571428571428572, val_acc: 0.18627450980392157, train_loss: 1.7612166728292193, val_loss: 1.7494104083846598 (9 / 80)\n",
            "train_acc: 0.1989010989010989, val_acc: 0.18627450980392157, train_loss: 1.7544029763766698, val_loss: 1.7461353622230829 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1989010989010989\n",
            "lr 0.00017878313076609675, batch 13, decay 7.684183733904238e-06, gamma 0.01315692733819065, val accuracy 0.18627450980392157, val loss 1.7865550108984405 [10 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.2594411444174766e-05, 'batch_size': 15, 'weight_decay': 7.2453126195742714e-06, 'gamma': 0.038608385560167394}\n",
            "train_acc: 0.14835164835164835, val_acc: 0.18627450980392157, train_loss: 1.7928275262916482, val_loss: 1.7919293151182287 (1 / 80)\n",
            "train_acc: 0.15494505494505495, val_acc: 0.21568627450980393, train_loss: 1.7921787814779595, val_loss: 1.7916231646257288 (2 / 80)\n",
            "train_acc: 0.17142857142857143, val_acc: 0.22549019607843138, train_loss: 1.791367198739733, val_loss: 1.7913370412938736 (3 / 80)\n",
            "train_acc: 0.16373626373626374, val_acc: 0.22549019607843138, train_loss: 1.7917675834435682, val_loss: 1.791043204419753 (4 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.24509803921568626, train_loss: 1.7915449207955665, val_loss: 1.7907588026102852 (5 / 80)\n",
            "train_acc: 0.17252747252747253, val_acc: 0.24509803921568626, train_loss: 1.7911101429017036, val_loss: 1.7904726617476519 (6 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.24509803921568626, train_loss: 1.7908791676982418, val_loss: 1.7901893924264347 (7 / 80)\n",
            "train_acc: 0.19230769230769232, val_acc: 0.20588235294117646, train_loss: 1.7911876145299974, val_loss: 1.7899065403377308 (8 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.20588235294117646, train_loss: 1.7901750730944204, val_loss: 1.789628705557655 (9 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.19607843137254902, train_loss: 1.7905569672584534, val_loss: 1.789356200134053 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18681318681318682\n",
            "lr 1.2594411444174766e-05, batch 15, decay 7.2453126195742714e-06, gamma 0.038608385560167394, val accuracy 0.24509803921568626, val loss 1.7907588026102852 [11 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.8305753785038916e-05, 'batch_size': 11, 'weight_decay': 0.0007768143144096862, 'gamma': 0.011250227845502518}\n",
            "train_acc: 0.16153846153846155, val_acc: 0.11764705882352941, train_loss: 1.7925602939102676, val_loss: 1.7916740155687518 (1 / 80)\n",
            "train_acc: 0.14945054945054945, val_acc: 0.14705882352941177, train_loss: 1.7922018577764323, val_loss: 1.7912017784866632 (2 / 80)\n",
            "train_acc: 0.17802197802197803, val_acc: 0.19607843137254902, train_loss: 1.791510649565812, val_loss: 1.7907771059111053 (3 / 80)\n",
            "train_acc: 0.15934065934065933, val_acc: 0.19607843137254902, train_loss: 1.7910853189426463, val_loss: 1.7903064465990253 (4 / 80)\n",
            "train_acc: 0.16593406593406593, val_acc: 0.18627450980392157, train_loss: 1.7905898134787004, val_loss: 1.7898693084716797 (5 / 80)\n",
            "train_acc: 0.16153846153846155, val_acc: 0.17647058823529413, train_loss: 1.7901644334688291, val_loss: 1.789433545926038 (6 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.18627450980392157, train_loss: 1.7893014957616618, val_loss: 1.7890348831812541 (7 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.18627450980392157, train_loss: 1.789161223107642, val_loss: 1.7886263389213413 (8 / 80)\n",
            "train_acc: 0.16043956043956045, val_acc: 0.18627450980392157, train_loss: 1.7897344939001314, val_loss: 1.788196994977839 (9 / 80)\n",
            "train_acc: 0.18571428571428572, val_acc: 0.18627450980392157, train_loss: 1.7881815950949114, val_loss: 1.7877880276418199 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18571428571428572\n",
            "lr 1.8305753785038916e-05, batch 11, decay 0.0007768143144096862, gamma 0.011250227845502518, val accuracy 0.19607843137254902, val loss 1.7907771059111053 [12 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00023333026899970366, 'batch_size': 13, 'weight_decay': 2.637890554183147e-05, 'gamma': 0.20603355061176046}\n",
            "train_acc: 0.1978021978021978, val_acc: 0.22549019607843138, train_loss: 1.7897247433662415, val_loss: 1.7857890129089355 (1 / 80)\n",
            "train_acc: 0.210989010989011, val_acc: 0.18627450980392157, train_loss: 1.7849641118730817, val_loss: 1.779898845681957 (2 / 80)\n",
            "train_acc: 0.210989010989011, val_acc: 0.18627450980392157, train_loss: 1.7790529489517213, val_loss: 1.7731063295813168 (3 / 80)\n",
            "train_acc: 0.21428571428571427, val_acc: 0.18627450980392157, train_loss: 1.7720514127186366, val_loss: 1.7654568658155554 (4 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.18627450980392157, train_loss: 1.766475888660976, val_loss: 1.7577024546324038 (5 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.18627450980392157, train_loss: 1.7647889018058778, val_loss: 1.7512444783659542 (6 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.18627450980392157, train_loss: 1.7595131039619445, val_loss: 1.7464734830108344 (7 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.21568627450980393, train_loss: 1.7572714243616376, val_loss: 1.7424918868962456 (8 / 80)\n",
            "train_acc: 0.2164835164835165, val_acc: 0.23529411764705882, train_loss: 1.754555790764945, val_loss: 1.7372124428842581 (9 / 80)\n",
            "train_acc: 0.20659340659340658, val_acc: 0.21568627450980393, train_loss: 1.7426538365227835, val_loss: 1.7302093260428484 (10 / 80)\n",
            "underfit -> train_accuracy = 0.20659340659340658\n",
            "lr 0.00023333026899970366, batch 13, decay 2.637890554183147e-05, gamma 0.20603355061176046, val accuracy 0.23529411764705882, val loss 1.7372124428842581 [13 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 4.7246072964930276e-05, 'batch_size': 10, 'weight_decay': 2.0686687189438094e-06, 'gamma': 0.1397242969372128}\n",
            "train_acc: 0.19230769230769232, val_acc: 0.16666666666666666, train_loss: 1.7897737694310618, val_loss: 1.789740525039972 (1 / 80)\n",
            "train_acc: 0.17802197802197803, val_acc: 0.1568627450980392, train_loss: 1.7893187554327996, val_loss: 1.7884001708498187 (2 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.16666666666666666, train_loss: 1.7873583259163321, val_loss: 1.7869709099040312 (3 / 80)\n",
            "train_acc: 0.16923076923076924, val_acc: 0.16666666666666666, train_loss: 1.7864638422871684, val_loss: 1.7857078781314926 (4 / 80)\n",
            "train_acc: 0.19230769230769232, val_acc: 0.18627450980392157, train_loss: 1.7862092780542898, val_loss: 1.7844717923332662 (5 / 80)\n",
            "train_acc: 0.1989010989010989, val_acc: 0.17647058823529413, train_loss: 1.7846044367486305, val_loss: 1.7831836097380693 (6 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.16666666666666666, train_loss: 1.783552613887158, val_loss: 1.7817988138572842 (7 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.16666666666666666, train_loss: 1.7835929760566125, val_loss: 1.7806286554710538 (8 / 80)\n",
            "train_acc: 0.17252747252747253, val_acc: 0.18627450980392157, train_loss: 1.7811699594770158, val_loss: 1.7793267614701216 (9 / 80)\n",
            "train_acc: 0.1989010989010989, val_acc: 0.19607843137254902, train_loss: 1.7797799791608537, val_loss: 1.7779953059028177 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1989010989010989\n",
            "lr 4.7246072964930276e-05, batch 10, decay 2.0686687189438094e-06, gamma 0.1397242969372128, val accuracy 0.19607843137254902, val loss 1.7779953059028177 [14 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.5615973825314508e-05, 'batch_size': 13, 'weight_decay': 1.9799202308173527e-06, 'gamma': 0.062331241613596995}\n",
            "train_acc: 0.13406593406593406, val_acc: 0.18627450980392157, train_loss: 1.793381404876709, val_loss: 1.7917523910017574 (1 / 80)\n",
            "train_acc: 0.17032967032967034, val_acc: 0.18627450980392157, train_loss: 1.79232565675463, val_loss: 1.7912404677447151 (2 / 80)\n",
            "train_acc: 0.16373626373626374, val_acc: 0.18627450980392157, train_loss: 1.7922015564782279, val_loss: 1.7906275788942974 (3 / 80)\n",
            "train_acc: 0.15494505494505495, val_acc: 0.18627450980392157, train_loss: 1.7914539916174752, val_loss: 1.790070896055184 (4 / 80)\n",
            "train_acc: 0.16373626373626374, val_acc: 0.18627450980392157, train_loss: 1.7919377173696245, val_loss: 1.789560482782476 (5 / 80)\n",
            "train_acc: 0.16593406593406593, val_acc: 0.18627450980392157, train_loss: 1.7904778480529786, val_loss: 1.789076684736738 (6 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.18627450980392157, train_loss: 1.7901778595788138, val_loss: 1.7885141980414296 (7 / 80)\n",
            "train_acc: 0.17032967032967034, val_acc: 0.18627450980392157, train_loss: 1.7897156732422965, val_loss: 1.7880032880633485 (8 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.18627450980392157, train_loss: 1.788605751310076, val_loss: 1.7875061046843435 (9 / 80)\n",
            "train_acc: 0.16923076923076924, val_acc: 0.18627450980392157, train_loss: 1.7894927263259888, val_loss: 1.786956061335171 (10 / 80)\n",
            "underfit -> train_accuracy = 0.16923076923076924\n",
            "lr 1.5615973825314508e-05, batch 13, decay 1.9799202308173527e-06, gamma 0.062331241613596995, val accuracy 0.18627450980392157, val loss 1.7917523910017574 [15 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.018663842573828e-05, 'batch_size': 13, 'weight_decay': 5.3750323616975244e-05, 'gamma': 0.22543936610016752}\n",
            "train_acc: 0.16373626373626374, val_acc: 0.18627450980392157, train_loss: 1.7908184085573469, val_loss: 1.7903635420051276 (1 / 80)\n",
            "train_acc: 0.17802197802197803, val_acc: 0.18627450980392157, train_loss: 1.7913201059613908, val_loss: 1.7900100537374908 (2 / 80)\n",
            "train_acc: 0.17472527472527472, val_acc: 0.18627450980392157, train_loss: 1.7904464261872428, val_loss: 1.789633333683014 (3 / 80)\n",
            "train_acc: 0.16923076923076924, val_acc: 0.18627450980392157, train_loss: 1.7910223143441337, val_loss: 1.789274302183413 (4 / 80)\n",
            "train_acc: 0.17802197802197803, val_acc: 0.18627450980392157, train_loss: 1.7904457671301706, val_loss: 1.7889242476108027 (5 / 80)\n",
            "train_acc: 0.18021978021978022, val_acc: 0.18627450980392157, train_loss: 1.7885268262454441, val_loss: 1.7885540583554436 (6 / 80)\n",
            "train_acc: 0.17472527472527472, val_acc: 0.18627450980392157, train_loss: 1.7899263909884862, val_loss: 1.7882102681141274 (7 / 80)\n",
            "train_acc: 0.17692307692307693, val_acc: 0.18627450980392157, train_loss: 1.7892772010394504, val_loss: 1.7878793185832453 (8 / 80)\n",
            "train_acc: 0.17912087912087912, val_acc: 0.18627450980392157, train_loss: 1.788874898638044, val_loss: 1.7875337168282153 (9 / 80)\n",
            "train_acc: 0.19230769230769232, val_acc: 0.18627450980392157, train_loss: 1.7880393777574812, val_loss: 1.787212664005803 (10 / 80)\n",
            "underfit -> train_accuracy = 0.19230769230769232\n",
            "lr 1.018663842573828e-05, batch 13, decay 5.3750323616975244e-05, gamma 0.22543936610016752, val accuracy 0.18627450980392157, val loss 1.7903635420051276 [16 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.975548915397956e-05, 'batch_size': 8, 'weight_decay': 1.212836441260304e-06, 'gamma': 0.6473361007824802}\n",
            "train_acc: 0.14945054945054945, val_acc: 0.18627450980392157, train_loss: 1.7924767957938896, val_loss: 1.7917558445649988 (1 / 80)\n",
            "train_acc: 0.15054945054945054, val_acc: 0.18627450980392157, train_loss: 1.7924256702045818, val_loss: 1.791081991850161 (2 / 80)\n",
            "train_acc: 0.17032967032967034, val_acc: 0.18627450980392157, train_loss: 1.791236336676629, val_loss: 1.7905139899721332 (3 / 80)\n",
            "train_acc: 0.16813186813186815, val_acc: 0.19607843137254902, train_loss: 1.791474847479181, val_loss: 1.7898984703363157 (4 / 80)\n",
            "train_acc: 0.15494505494505495, val_acc: 0.21568627450980393, train_loss: 1.7908418194278255, val_loss: 1.7892193981245452 (5 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.23529411764705882, train_loss: 1.7899318925626986, val_loss: 1.7886629314983593 (6 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.24509803921568626, train_loss: 1.7895321990107442, val_loss: 1.7880694983052272 (7 / 80)\n",
            "train_acc: 0.17032967032967034, val_acc: 0.24509803921568626, train_loss: 1.7887959868043333, val_loss: 1.787512825984581 (8 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.24509803921568626, train_loss: 1.7882527781056834, val_loss: 1.7869434099571377 (9 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.2647058823529412, train_loss: 1.7885635294757047, val_loss: 1.786396821339925 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1879120879120879\n",
            "lr 1.975548915397956e-05, batch 8, decay 1.212836441260304e-06, gamma 0.6473361007824802, val accuracy 0.2647058823529412, val loss 1.786396821339925 [17 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0008554266290399184, 'batch_size': 11, 'weight_decay': 0.0006533010900552039, 'gamma': 0.06721270304757815}\n",
            "train_acc: 0.16263736263736264, val_acc: 0.18627450980392157, train_loss: 1.7835405850148462, val_loss: 1.76482380955827 (1 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.18627450980392157, train_loss: 1.7640416280253903, val_loss: 1.7418090911472546 (2 / 80)\n",
            "train_acc: 0.2087912087912088, val_acc: 0.22549019607843138, train_loss: 1.7467720069728054, val_loss: 1.717664294383105 (3 / 80)\n",
            "train_acc: 0.25384615384615383, val_acc: 0.3235294117647059, train_loss: 1.7177042055916, val_loss: 1.6577975820092594 (4 / 80)\n",
            "train_acc: 0.3131868131868132, val_acc: 0.35294117647058826, train_loss: 1.643285025214101, val_loss: 1.5913559911297817 (5 / 80)\n",
            "train_acc: 0.3131868131868132, val_acc: 0.28431372549019607, train_loss: 1.6128487854213505, val_loss: 1.5748723757033254 (6 / 80)\n",
            "train_acc: 0.3153846153846154, val_acc: 0.30392156862745096, train_loss: 1.572016038606455, val_loss: 1.6202531784188514 (7 / 80)\n",
            "train_acc: 0.32857142857142857, val_acc: 0.35294117647058826, train_loss: 1.5336988830304408, val_loss: 1.4534400362594455 (8 / 80)\n",
            "train_acc: 0.3505494505494505, val_acc: 0.3235294117647059, train_loss: 1.5399435267343626, val_loss: 1.4614349276411767 (9 / 80)\n",
            "train_acc: 0.3912087912087912, val_acc: 0.3431372549019608, train_loss: 1.457505213886827, val_loss: 1.4964046723702376 (10 / 80)\n",
            "train_acc: 0.36153846153846153, val_acc: 0.3627450980392157, train_loss: 1.4696752267879445, val_loss: 1.4235445471370922 (11 / 80)\n",
            "train_acc: 0.3747252747252747, val_acc: 0.4117647058823529, train_loss: 1.436575290593472, val_loss: 1.330248399108064 (12 / 80)\n",
            "train_acc: 0.3868131868131868, val_acc: 0.47058823529411764, train_loss: 1.4024862595966885, val_loss: 1.3221365932156057 (13 / 80)\n",
            "train_acc: 0.4032967032967033, val_acc: 0.43137254901960786, train_loss: 1.4029601075492062, val_loss: 1.2687384953685836 (14 / 80)\n",
            "train_acc: 0.42747252747252745, val_acc: 0.39215686274509803, train_loss: 1.3641927874350286, val_loss: 1.3489693122751572 (15 / 80)\n",
            "train_acc: 0.4197802197802198, val_acc: 0.4411764705882353, train_loss: 1.3650373170009027, val_loss: 1.22380858075385 (16 / 80)\n",
            "train_acc: 0.4043956043956044, val_acc: 0.43137254901960786, train_loss: 1.3478425869574913, val_loss: 1.2102522183867062 (17 / 80)\n",
            "train_acc: 0.4208791208791209, val_acc: 0.5588235294117647, train_loss: 1.3294827239198999, val_loss: 1.213786729994942 (18 / 80)\n",
            "train_acc: 0.43626373626373627, val_acc: 0.4803921568627451, train_loss: 1.3177017865600167, val_loss: 1.180383207166896 (19 / 80)\n",
            "train_acc: 0.43956043956043955, val_acc: 0.5392156862745098, train_loss: 1.3240347296327024, val_loss: 1.1975509699653177 (20 / 80)\n",
            "train_acc: 0.4307692307692308, val_acc: 0.38235294117647056, train_loss: 1.2764897532515473, val_loss: 1.2545738898071588 (21 / 80)\n",
            "train_acc: 0.45054945054945056, val_acc: 0.39215686274509803, train_loss: 1.2629360157054859, val_loss: 1.232100417800978 (22 / 80)\n",
            "train_acc: 0.46813186813186813, val_acc: 0.5196078431372549, train_loss: 1.2651606837471763, val_loss: 1.1327698867695004 (23 / 80)\n",
            "train_acc: 0.4725274725274725, val_acc: 0.5196078431372549, train_loss: 1.2510267605493357, val_loss: 1.142256614039926 (24 / 80)\n",
            "train_acc: 0.46483516483516485, val_acc: 0.47058823529411764, train_loss: 1.2577830481005239, val_loss: 1.115123399916817 (25 / 80)\n",
            "train_acc: 0.47802197802197804, val_acc: 0.5784313725490197, train_loss: 1.199945220907966, val_loss: 1.0622044801712036 (26 / 80)\n",
            "train_acc: 0.5098901098901099, val_acc: 0.5098039215686274, train_loss: 1.1861557030415797, val_loss: 1.1028116655115987 (27 / 80)\n",
            "train_acc: 0.49230769230769234, val_acc: 0.5490196078431373, train_loss: 1.1955970757610195, val_loss: 1.0613837271344428 (28 / 80)\n",
            "train_acc: 0.4978021978021978, val_acc: 0.5392156862745098, train_loss: 1.1553568133941063, val_loss: 1.0260741412639618 (29 / 80)\n",
            "train_acc: 0.5307692307692308, val_acc: 0.5686274509803921, train_loss: 1.1326007603289008, val_loss: 0.9750516435094908 (30 / 80)\n",
            "train_acc: 0.5505494505494506, val_acc: 0.5490196078431373, train_loss: 1.0842746613444862, val_loss: 1.21125468787025 (31 / 80)\n",
            "train_acc: 0.5659340659340659, val_acc: 0.5294117647058824, train_loss: 1.0436730657305036, val_loss: 1.0019920076809676 (32 / 80)\n",
            "train_acc: 0.5747252747252747, val_acc: 0.49019607843137253, train_loss: 1.0138154553188072, val_loss: 1.1394753812574874 (33 / 80)\n",
            "train_acc: 0.578021978021978, val_acc: 0.5882352941176471, train_loss: 1.0148264896738661, val_loss: 0.9358943271286347 (34 / 80)\n",
            "train_acc: 0.5912087912087912, val_acc: 0.6372549019607843, train_loss: 0.9860394644213247, val_loss: 0.8793432271363688 (35 / 80)\n",
            "train_acc: 0.6054945054945055, val_acc: 0.6470588235294118, train_loss: 0.9582601647455614, val_loss: 0.899877623015759 (36 / 80)\n",
            "train_acc: 0.6208791208791209, val_acc: 0.5784313725490197, train_loss: 0.918609553706515, val_loss: 0.9777344097109402 (37 / 80)\n",
            "train_acc: 0.6406593406593407, val_acc: 0.5980392156862745, train_loss: 0.8839313405555683, val_loss: 0.9673454960187277 (38 / 80)\n",
            "train_acc: 0.643956043956044, val_acc: 0.5686274509803921, train_loss: 0.9048453239621697, val_loss: 1.0288056059210908 (39 / 80)\n",
            "train_acc: 0.6648351648351648, val_acc: 0.6176470588235294, train_loss: 0.8478938529779623, val_loss: 0.8756001579995248 (40 / 80)\n",
            "train_acc: 0.7307692307692307, val_acc: 0.6274509803921569, train_loss: 0.7114053032882921, val_loss: 0.9748752402032123 (41 / 80)\n",
            "train_acc: 0.7065934065934066, val_acc: 0.5588235294117647, train_loss: 0.7748535912442993, val_loss: 0.9386654718249452 (42 / 80)\n",
            "train_acc: 0.7153846153846154, val_acc: 0.7058823529411765, train_loss: 0.6629919105833704, val_loss: 0.8959970164532755 (43 / 80)\n",
            "train_acc: 0.7670329670329671, val_acc: 0.6764705882352942, train_loss: 0.5942503761131684, val_loss: 0.8820480926364076 (44 / 80)\n",
            "train_acc: 0.7483516483516484, val_acc: 0.7450980392156863, train_loss: 0.6590984801997195, val_loss: 0.7630121114791608 (45 / 80)\n",
            "train_acc: 0.7681318681318682, val_acc: 0.6666666666666666, train_loss: 0.5935201650464927, val_loss: 1.1389422871756787 (46 / 80)\n",
            "train_acc: 0.8076923076923077, val_acc: 0.6372549019607843, train_loss: 0.5061035319835275, val_loss: 0.8947734648690504 (47 / 80)\n",
            "train_acc: 0.7956043956043956, val_acc: 0.6764705882352942, train_loss: 0.5417009475139472, val_loss: 0.8389460211071897 (48 / 80)\n",
            "train_acc: 0.889010989010989, val_acc: 0.7549019607843137, train_loss: 0.3273636115407878, val_loss: 0.765515255577424 (49 / 80)\n",
            "train_acc: 0.9087912087912088, val_acc: 0.7745098039215687, train_loss: 0.2941003753015628, val_loss: 0.7667363376594057 (50 / 80)\n",
            "train_acc: 0.921978021978022, val_acc: 0.7745098039215687, train_loss: 0.23406804491980718, val_loss: 0.793811544629873 (51 / 80)\n",
            "train_acc: 0.9241758241758242, val_acc: 0.7745098039215687, train_loss: 0.23111069545272614, val_loss: 0.8130629863224778 (52 / 80)\n",
            "train_acc: 0.9208791208791208, val_acc: 0.7843137254901961, train_loss: 0.20728393481266533, val_loss: 0.8104945301425224 (53 / 80)\n",
            "train_acc: 0.9318681318681319, val_acc: 0.7843137254901961, train_loss: 0.2106048929105912, val_loss: 0.8078922722853866 (54 / 80)\n",
            "train_acc: 0.9252747252747253, val_acc: 0.7647058823529411, train_loss: 0.221784738965005, val_loss: 0.8339553222644562 (55 / 80)\n",
            "train_acc: 0.9494505494505494, val_acc: 0.7549019607843137, train_loss: 0.16060750245597663, val_loss: 0.8547392832297905 (56 / 80)\n",
            "train_acc: 0.9142857142857143, val_acc: 0.7745098039215687, train_loss: 0.23075052858790854, val_loss: 0.8257580175265378 (57 / 80)\n",
            "train_acc: 0.9362637362637363, val_acc: 0.7745098039215687, train_loss: 0.18955059547468528, val_loss: 0.846230653278968 (58 / 80)\n",
            "train_acc: 0.9406593406593406, val_acc: 0.7745098039215687, train_loss: 0.18285682408528015, val_loss: 0.8774699570209372 (59 / 80)\n",
            "train_acc: 0.9340659340659341, val_acc: 0.7745098039215687, train_loss: 0.1950378590969594, val_loss: 0.8344661166854933 (60 / 80)\n",
            "train_acc: 0.9527472527472527, val_acc: 0.7549019607843137, train_loss: 0.15968280838045118, val_loss: 0.8603160818125687 (61 / 80)\n",
            "train_acc: 0.9582417582417583, val_acc: 0.7549019607843137, train_loss: 0.1545415398675007, val_loss: 0.8638371766782275 (62 / 80)\n",
            "train_acc: 0.9472527472527472, val_acc: 0.7647058823529411, train_loss: 0.15589752760255238, val_loss: 0.8916208601246277 (63 / 80)\n",
            "train_acc: 0.9538461538461539, val_acc: 0.7843137254901961, train_loss: 0.1551774746989115, val_loss: 0.8928800667179566 (64 / 80)\n",
            "train_acc: 0.9560439560439561, val_acc: 0.7745098039215687, train_loss: 0.13874535309099642, val_loss: 0.859806171381006 (65 / 80)\n",
            "train_acc: 0.9439560439560439, val_acc: 0.7549019607843137, train_loss: 0.1606182352087082, val_loss: 0.903717089773101 (66 / 80)\n",
            "train_acc: 0.9582417582417583, val_acc: 0.7647058823529411, train_loss: 0.1404683170220968, val_loss: 0.8652338990348154 (67 / 80)\n",
            "train_acc: 0.9615384615384616, val_acc: 0.7941176470588235, train_loss: 0.12882767009988919, val_loss: 0.9317472677897004 (68 / 80)\n",
            "train_acc: 0.9637362637362638, val_acc: 0.7745098039215687, train_loss: 0.12165438196171525, val_loss: 0.9193540867052826 (69 / 80)\n",
            "train_acc: 0.9527472527472527, val_acc: 0.7450980392156863, train_loss: 0.12368256230531567, val_loss: 0.9018706068688748 (70 / 80)\n",
            "train_acc: 0.9494505494505494, val_acc: 0.7647058823529411, train_loss: 0.1367758299508081, val_loss: 0.9246250116357616 (71 / 80)\n",
            "train_acc: 0.9483516483516483, val_acc: 0.7745098039215687, train_loss: 0.14070004617365506, val_loss: 0.9750818392152295 (72 / 80)\n",
            "train_acc: 0.967032967032967, val_acc: 0.7647058823529411, train_loss: 0.11519626375328708, val_loss: 0.9447339052590085 (73 / 80)\n",
            "train_acc: 0.9681318681318681, val_acc: 0.7941176470588235, train_loss: 0.11027076866139995, val_loss: 1.004272248816229 (74 / 80)\n",
            "train_acc: 0.9626373626373627, val_acc: 0.7647058823529411, train_loss: 0.12037766373532077, val_loss: 0.9918631887903401 (75 / 80)\n",
            "train_acc: 0.9626373626373627, val_acc: 0.7647058823529411, train_loss: 0.11289630032420814, val_loss: 0.9773665394853143 (76 / 80)\n",
            "train_acc: 0.9714285714285714, val_acc: 0.7745098039215687, train_loss: 0.10177291470815192, val_loss: 0.974052226134375 (77 / 80)\n",
            "train_acc: 0.9692307692307692, val_acc: 0.7745098039215687, train_loss: 0.10713839486092944, val_loss: 0.9891032915489346 (78 / 80)\n",
            "train_acc: 0.9736263736263736, val_acc: 0.7549019607843137, train_loss: 0.09392113795473271, val_loss: 0.9881847088827806 (79 / 80)\n",
            "train_acc: 0.9615384615384616, val_acc: 0.7745098039215687, train_loss: 0.10231117984167412, val_loss: 1.0219003045383621 (80 / 80)\n",
            "lr 0.0008554266290399184, batch 11, decay 0.0006533010900552039, gamma 0.06721270304757815, val accuracy 0.7941176470588235, val loss 0.9317472677897004 [18 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 7.56574270360908e-05, 'batch_size': 12, 'weight_decay': 5.035786440914622e-06, 'gamma': 0.01337687366230476}\n",
            "train_acc: 0.16483516483516483, val_acc: 0.21568627450980393, train_loss: 1.7918604494451167, val_loss: 1.7903095483779907 (1 / 80)\n",
            "train_acc: 0.1989010989010989, val_acc: 0.17647058823529413, train_loss: 1.7899848977288046, val_loss: 1.7882489386726828 (2 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.21568627450980393, train_loss: 1.7882703217831286, val_loss: 1.7862448692321777 (3 / 80)\n",
            "train_acc: 0.2010989010989011, val_acc: 0.24509803921568626, train_loss: 1.7866130813137515, val_loss: 1.7842790028628182 (4 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.20588235294117646, train_loss: 1.7847581604024867, val_loss: 1.7822092350791483 (5 / 80)\n",
            "train_acc: 0.1934065934065934, val_acc: 0.19607843137254902, train_loss: 1.7819484176216545, val_loss: 1.7801080030553482 (6 / 80)\n",
            "train_acc: 0.210989010989011, val_acc: 0.21568627450980393, train_loss: 1.780137739600716, val_loss: 1.7780128296683817 (7 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.21568627450980393, train_loss: 1.7802564720531087, val_loss: 1.775820907424478 (8 / 80)\n",
            "train_acc: 0.1978021978021978, val_acc: 0.18627450980392157, train_loss: 1.7772240478913863, val_loss: 1.7735899967305802 (9 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.18627450980392157, train_loss: 1.775774570611807, val_loss: 1.7713179167579203 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1967032967032967\n",
            "lr 7.56574270360908e-05, batch 12, decay 5.035786440914622e-06, gamma 0.01337687366230476, val accuracy 0.24509803921568626, val loss 1.7842790028628182 [19 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 7.664342237112183e-05, 'batch_size': 11, 'weight_decay': 1.2361242707845264e-06, 'gamma': 0.9746029420623341}\n",
            "train_acc: 0.17802197802197803, val_acc: 0.18627450980392157, train_loss: 1.7914825714551486, val_loss: 1.7901917871306925 (1 / 80)\n",
            "train_acc: 0.14945054945054945, val_acc: 0.18627450980392157, train_loss: 1.7901716323999257, val_loss: 1.7876265504780937 (2 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.7877490644926552, val_loss: 1.785135505246181 (3 / 80)\n",
            "train_acc: 0.18021978021978022, val_acc: 0.18627450980392157, train_loss: 1.7851629094763117, val_loss: 1.782905070220723 (4 / 80)\n",
            "train_acc: 0.17032967032967034, val_acc: 0.18627450980392157, train_loss: 1.7836238044958848, val_loss: 1.7806343179123074 (5 / 80)\n",
            "train_acc: 0.2010989010989011, val_acc: 0.18627450980392157, train_loss: 1.7813326201596102, val_loss: 1.7783128420511882 (6 / 80)\n",
            "train_acc: 0.20659340659340658, val_acc: 0.18627450980392157, train_loss: 1.7790252207399724, val_loss: 1.775952868601855 (7 / 80)\n",
            "train_acc: 0.2, val_acc: 0.18627450980392157, train_loss: 1.7769215053254432, val_loss: 1.7732113506279739 (8 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.18627450980392157, train_loss: 1.7738650171311348, val_loss: 1.7704194912723465 (9 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.18627450980392157, train_loss: 1.7708023223248157, val_loss: 1.767462033851474 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1945054945054945\n",
            "lr 7.664342237112183e-05, batch 11, decay 1.2361242707845264e-06, gamma 0.9746029420623341, val accuracy 0.18627450980392157, val loss 1.7901917871306925 [20 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0001214760652856029, 'batch_size': 14, 'weight_decay': 6.141556229486402e-06, 'gamma': 0.34555854476140474}\n",
            "train_acc: 0.14175824175824175, val_acc: 0.18627450980392157, train_loss: 1.7939747498585628, val_loss: 1.7908105639850391 (1 / 80)\n",
            "train_acc: 0.17362637362637362, val_acc: 0.19607843137254902, train_loss: 1.7906468959955069, val_loss: 1.7871325109519212 (2 / 80)\n",
            "train_acc: 0.18571428571428572, val_acc: 0.18627450980392157, train_loss: 1.7868607924534725, val_loss: 1.7835344987757065 (3 / 80)\n",
            "train_acc: 0.20549450549450549, val_acc: 0.18627450980392157, train_loss: 1.7828851809868447, val_loss: 1.7801018696205289 (4 / 80)\n",
            "train_acc: 0.1934065934065934, val_acc: 0.18627450980392157, train_loss: 1.7810769081115723, val_loss: 1.776591880648744 (5 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.18627450980392157, train_loss: 1.7781269715382502, val_loss: 1.7726893378239053 (6 / 80)\n",
            "train_acc: 0.17142857142857143, val_acc: 0.18627450980392157, train_loss: 1.774113471691425, val_loss: 1.7685810561273612 (7 / 80)\n",
            "train_acc: 0.18571428571428572, val_acc: 0.18627450980392157, train_loss: 1.7723503479590783, val_loss: 1.7646961913389319 (8 / 80)\n",
            "train_acc: 0.17692307692307693, val_acc: 0.18627450980392157, train_loss: 1.769727136538579, val_loss: 1.7602570617900175 (9 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.764578890800476, val_loss: 1.756882066820182 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18681318681318682\n",
            "lr 0.0001214760652856029, batch 14, decay 6.141556229486402e-06, gamma 0.34555854476140474, val accuracy 0.19607843137254902, val loss 1.7871325109519212 [21 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00018449819730322026, 'batch_size': 15, 'weight_decay': 1.4463533935516826e-06, 'gamma': 0.03467064980312956}\n",
            "train_acc: 0.19230769230769232, val_acc: 0.17647058823529413, train_loss: 1.789495817252568, val_loss: 1.7880632176118738 (1 / 80)\n",
            "train_acc: 0.19230769230769232, val_acc: 0.17647058823529413, train_loss: 1.7868951487017202, val_loss: 1.784445369944853 (2 / 80)\n",
            "train_acc: 0.2, val_acc: 0.18627450980392157, train_loss: 1.783927783861265, val_loss: 1.7801366202971514 (3 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.21568627450980393, train_loss: 1.780562577011821, val_loss: 1.7760494351387024 (4 / 80)\n",
            "train_acc: 0.17362637362637362, val_acc: 0.20588235294117646, train_loss: 1.776901050583347, val_loss: 1.77113496205386 (5 / 80)\n",
            "train_acc: 0.1989010989010989, val_acc: 0.18627450980392157, train_loss: 1.7707629590244083, val_loss: 1.765564708148732 (6 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.18627450980392157, train_loss: 1.7664162582093543, val_loss: 1.7602534890174866 (7 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.18627450980392157, train_loss: 1.763772984127422, val_loss: 1.755085422712214 (8 / 80)\n",
            "train_acc: 0.1956043956043956, val_acc: 0.18627450980392157, train_loss: 1.7630087510570065, val_loss: 1.7516740946208729 (9 / 80)\n",
            "train_acc: 0.210989010989011, val_acc: 0.18627450980392157, train_loss: 1.7587624146388128, val_loss: 1.7481260615236618 (10 / 80)\n",
            "underfit -> train_accuracy = 0.210989010989011\n",
            "lr 0.00018449819730322026, batch 15, decay 1.4463533935516826e-06, gamma 0.03467064980312956, val accuracy 0.21568627450980393, val loss 1.7760494351387024 [22 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0005964416106901886, 'batch_size': 8, 'weight_decay': 2.768377738819072e-06, 'gamma': 0.7895457717078328}\n",
            "train_acc: 0.18571428571428572, val_acc: 0.18627450980392157, train_loss: 1.7841208455326794, val_loss: 1.7711933734370213 (1 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.18627450980392157, train_loss: 1.768628195615915, val_loss: 1.7553390965742224 (2 / 80)\n",
            "train_acc: 0.1978021978021978, val_acc: 0.18627450980392157, train_loss: 1.7562204976658244, val_loss: 1.7376981833401848 (3 / 80)\n",
            "train_acc: 0.22197802197802197, val_acc: 0.24509803921568626, train_loss: 1.7413750449379721, val_loss: 1.7135292128020643 (4 / 80)\n",
            "train_acc: 0.24615384615384617, val_acc: 0.24509803921568626, train_loss: 1.7325618348278842, val_loss: 1.6838813412423228 (5 / 80)\n",
            "train_acc: 0.2989010989010989, val_acc: 0.3235294117647059, train_loss: 1.6708049087734014, val_loss: 1.6013526939878278 (6 / 80)\n",
            "train_acc: 0.30439560439560437, val_acc: 0.3235294117647059, train_loss: 1.6219379259989812, val_loss: 1.563078574105805 (7 / 80)\n",
            "train_acc: 0.33516483516483514, val_acc: 0.29411764705882354, train_loss: 1.5797029188701084, val_loss: 1.514506267566307 (8 / 80)\n",
            "train_acc: 0.34285714285714286, val_acc: 0.4019607843137255, train_loss: 1.5212343928578136, val_loss: 1.4336592473235785 (9 / 80)\n",
            "train_acc: 0.35824175824175825, val_acc: 0.4117647058823529, train_loss: 1.4979196744960743, val_loss: 1.4451635748732323 (10 / 80)\n",
            "train_acc: 0.3802197802197802, val_acc: 0.39215686274509803, train_loss: 1.445028234576131, val_loss: 1.3795797474244063 (11 / 80)\n",
            "train_acc: 0.35824175824175825, val_acc: 0.28431372549019607, train_loss: 1.4700760278072986, val_loss: 1.5586491869945152 (12 / 80)\n",
            "train_acc: 0.3879120879120879, val_acc: 0.4019607843137255, train_loss: 1.4332588497098986, val_loss: 1.3464586547776765 (13 / 80)\n",
            "train_acc: 0.4186813186813187, val_acc: 0.3431372549019608, train_loss: 1.3563298573860756, val_loss: 1.4340551437116136 (14 / 80)\n",
            "train_acc: 0.4087912087912088, val_acc: 0.4019607843137255, train_loss: 1.3561626476245923, val_loss: 1.3167958166085036 (15 / 80)\n",
            "train_acc: 0.3912087912087912, val_acc: 0.46078431372549017, train_loss: 1.349005555320572, val_loss: 1.2658006537194346 (16 / 80)\n",
            "train_acc: 0.421978021978022, val_acc: 0.4411764705882353, train_loss: 1.3464555588397351, val_loss: 1.271740901703928 (17 / 80)\n",
            "train_acc: 0.4175824175824176, val_acc: 0.5196078431372549, train_loss: 1.3152006911707448, val_loss: 1.1916564071879667 (18 / 80)\n",
            "train_acc: 0.44395604395604393, val_acc: 0.46078431372549017, train_loss: 1.2816858629603962, val_loss: 1.2450470036151362 (19 / 80)\n",
            "train_acc: 0.45274725274725275, val_acc: 0.5588235294117647, train_loss: 1.2748060064001399, val_loss: 1.128578546000462 (20 / 80)\n",
            "train_acc: 0.44395604395604393, val_acc: 0.5196078431372549, train_loss: 1.2543861467759687, val_loss: 1.1197281538271437 (21 / 80)\n",
            "train_acc: 0.44945054945054946, val_acc: 0.5686274509803921, train_loss: 1.2785008663659567, val_loss: 1.09878199708228 (22 / 80)\n",
            "train_acc: 0.47802197802197804, val_acc: 0.5392156862745098, train_loss: 1.2030282769884382, val_loss: 1.1170342097095414 (23 / 80)\n",
            "train_acc: 0.4945054945054945, val_acc: 0.5392156862745098, train_loss: 1.2115321574630318, val_loss: 1.1129913306703754 (24 / 80)\n",
            "train_acc: 0.5164835164835165, val_acc: 0.5392156862745098, train_loss: 1.1475062414840027, val_loss: 1.0387443678051818 (25 / 80)\n",
            "train_acc: 0.5076923076923077, val_acc: 0.5294117647058824, train_loss: 1.1487471129868057, val_loss: 1.1152717038696887 (26 / 80)\n",
            "train_acc: 0.5263736263736264, val_acc: 0.5, train_loss: 1.1377432094825493, val_loss: 1.0829704170133554 (27 / 80)\n",
            "train_acc: 0.5274725274725275, val_acc: 0.5882352941176471, train_loss: 1.1063013832647721, val_loss: 0.976630217888776 (28 / 80)\n",
            "train_acc: 0.5582417582417583, val_acc: 0.5980392156862745, train_loss: 1.0622569982822125, val_loss: 0.9626020239848717 (29 / 80)\n",
            "train_acc: 0.5857142857142857, val_acc: 0.5294117647058824, train_loss: 1.0281109574076894, val_loss: 1.002094426575829 (30 / 80)\n",
            "train_acc: 0.5637362637362637, val_acc: 0.5686274509803921, train_loss: 1.0258385076627627, val_loss: 1.0224530486499561 (31 / 80)\n",
            "train_acc: 0.6, val_acc: 0.5490196078431373, train_loss: 0.9695968554570125, val_loss: 0.9697624889074588 (32 / 80)\n",
            "train_acc: 0.6241758241758242, val_acc: 0.5686274509803921, train_loss: 0.9405184093412462, val_loss: 0.8771646782463672 (33 / 80)\n",
            "train_acc: 0.6483516483516484, val_acc: 0.5588235294117647, train_loss: 0.857154860994318, val_loss: 0.9389226646984324 (34 / 80)\n",
            "train_acc: 0.6472527472527473, val_acc: 0.6176470588235294, train_loss: 0.8850477593940693, val_loss: 0.9750320011494207 (35 / 80)\n",
            "train_acc: 0.6692307692307692, val_acc: 0.5784313725490197, train_loss: 0.8402986948306744, val_loss: 0.9370738805509081 (36 / 80)\n",
            "train_acc: 0.6824175824175824, val_acc: 0.6274509803921569, train_loss: 0.8010682802933913, val_loss: 0.883484378749249 (37 / 80)\n",
            "train_acc: 0.7131868131868132, val_acc: 0.5980392156862745, train_loss: 0.756328827208215, val_loss: 0.901439297432993 (38 / 80)\n",
            "train_acc: 0.7362637362637363, val_acc: 0.6372549019607843, train_loss: 0.6882194942170448, val_loss: 0.9855501721887028 (39 / 80)\n",
            "train_acc: 0.7329670329670329, val_acc: 0.6078431372549019, train_loss: 0.6625639181870681, val_loss: 0.9352206879971074 (40 / 80)\n",
            "train_acc: 0.7549450549450549, val_acc: 0.7156862745098039, train_loss: 0.6321350714662572, val_loss: 0.9362723149505316 (41 / 80)\n",
            "train_acc: 0.8043956043956044, val_acc: 0.6176470588235294, train_loss: 0.5228283007066329, val_loss: 1.1976539814004712 (42 / 80)\n",
            "train_acc: 0.7923076923076923, val_acc: 0.6764705882352942, train_loss: 0.5757947504520416, val_loss: 0.8870755866462109 (43 / 80)\n",
            "train_acc: 0.8153846153846154, val_acc: 0.6568627450980392, train_loss: 0.5420106154221754, val_loss: 0.8891398485969094 (44 / 80)\n",
            "train_acc: 0.8197802197802198, val_acc: 0.696078431372549, train_loss: 0.47823654652296843, val_loss: 0.8825258145145342 (45 / 80)\n",
            "train_acc: 0.8307692307692308, val_acc: 0.6862745098039216, train_loss: 0.44101821524756296, val_loss: 1.063786295114779 (46 / 80)\n",
            "train_acc: 0.845054945054945, val_acc: 0.7156862745098039, train_loss: 0.4202425323999845, val_loss: 1.004605772594611 (47 / 80)\n",
            "train_acc: 0.8725274725274725, val_acc: 0.6666666666666666, train_loss: 0.35349622526011626, val_loss: 1.2247321114820593 (48 / 80)\n",
            "train_acc: 0.8879120879120879, val_acc: 0.6764705882352942, train_loss: 0.3137299766907325, val_loss: 1.0118021544288187 (49 / 80)\n",
            "overfit -> train_accuracy 0.9032967032967033, val_accuracy 0.5882352941176471\n",
            "lr 0.0005964416106901886, batch 8, decay 2.768377738819072e-06, gamma 0.7895457717078328, val accuracy 0.7156862745098039, val loss 0.9362723149505316 [23 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0003903058820951515, 'batch_size': 15, 'weight_decay': 7.044665052151419e-05, 'gamma': 0.1804629364822045}\n",
            "train_acc: 0.17362637362637362, val_acc: 0.18627450980392157, train_loss: 1.7889219876174087, val_loss: 1.7824124869178324 (1 / 80)\n",
            "train_acc: 0.1912087912087912, val_acc: 0.18627450980392157, train_loss: 1.7791233691540393, val_loss: 1.7698143615442163 (2 / 80)\n",
            "train_acc: 0.17582417582417584, val_acc: 0.18627450980392157, train_loss: 1.770579192664597, val_loss: 1.7566979317104114 (3 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.18627450980392157, train_loss: 1.7569313985960824, val_loss: 1.745392382144928 (4 / 80)\n",
            "train_acc: 0.189010989010989, val_acc: 0.18627450980392157, train_loss: 1.7520446587394882, val_loss: 1.7369500153204973 (5 / 80)\n",
            "train_acc: 0.1978021978021978, val_acc: 0.18627450980392157, train_loss: 1.7471006217893663, val_loss: 1.727662205696106 (6 / 80)\n",
            "train_acc: 0.25384615384615383, val_acc: 0.22549019607843138, train_loss: 1.7443423834475842, val_loss: 1.7181508365799398 (7 / 80)\n",
            "train_acc: 0.24505494505494504, val_acc: 0.4117647058823529, train_loss: 1.7344333215074226, val_loss: 1.7033538047005148 (8 / 80)\n",
            "train_acc: 0.289010989010989, val_acc: 0.24509803921568626, train_loss: 1.7081087886632145, val_loss: 1.6763468069188736 (9 / 80)\n",
            "train_acc: 0.3252747252747253, val_acc: 0.3431372549019608, train_loss: 1.6906083016605167, val_loss: 1.6391899375354542 (10 / 80)\n",
            "train_acc: 0.3384615384615385, val_acc: 0.35294117647058826, train_loss: 1.6448140288447286, val_loss: 1.583654621068169 (11 / 80)\n",
            "train_acc: 0.3362637362637363, val_acc: 0.2647058823529412, train_loss: 1.6049358687558017, val_loss: 1.6388824476915247 (12 / 80)\n",
            "train_acc: 0.34285714285714286, val_acc: 0.2647058823529412, train_loss: 1.5533268726789033, val_loss: 1.5595548082800472 (13 / 80)\n",
            "train_acc: 0.310989010989011, val_acc: 0.3431372549019608, train_loss: 1.5675797357664003, val_loss: 1.5409296190037447 (14 / 80)\n",
            "train_acc: 0.3626373626373626, val_acc: 0.39215686274509803, train_loss: 1.5277761266781733, val_loss: 1.6263203726095312 (15 / 80)\n",
            "train_acc: 0.37362637362637363, val_acc: 0.2549019607843137, train_loss: 1.5233138904466734, val_loss: 1.6659786946633284 (16 / 80)\n",
            "train_acc: 0.37142857142857144, val_acc: 0.30392156862745096, train_loss: 1.4968209397661818, val_loss: 1.4645762548727148 (17 / 80)\n",
            "train_acc: 0.3791208791208791, val_acc: 0.37254901960784315, train_loss: 1.4575243441613166, val_loss: 1.415685955215903 (18 / 80)\n",
            "train_acc: 0.3802197802197802, val_acc: 0.3431372549019608, train_loss: 1.45432460635573, val_loss: 1.4999844502000248 (19 / 80)\n",
            "train_acc: 0.3868131868131868, val_acc: 0.43137254901960786, train_loss: 1.4770847480375688, val_loss: 1.408693839521969 (20 / 80)\n",
            "train_acc: 0.37362637362637363, val_acc: 0.43137254901960786, train_loss: 1.4490218516234512, val_loss: 1.3829133615774267 (21 / 80)\n",
            "train_acc: 0.4054945054945055, val_acc: 0.38235294117647056, train_loss: 1.4340159591737685, val_loss: 1.3975181299097397 (22 / 80)\n",
            "train_acc: 0.3978021978021978, val_acc: 0.49019607843137253, train_loss: 1.4346673364167686, val_loss: 1.3759925400509554 (23 / 80)\n",
            "train_acc: 0.4076923076923077, val_acc: 0.3431372549019608, train_loss: 1.4204046844126104, val_loss: 1.4109282668899088 (24 / 80)\n",
            "train_acc: 0.4230769230769231, val_acc: 0.5098039215686274, train_loss: 1.3959913555082384, val_loss: 1.3184670104699976 (25 / 80)\n",
            "train_acc: 0.421978021978022, val_acc: 0.46078431372549017, train_loss: 1.363622506896218, val_loss: 1.3512858467943527 (26 / 80)\n",
            "train_acc: 0.4175824175824176, val_acc: 0.46078431372549017, train_loss: 1.379065868618724, val_loss: 1.2909984167884379 (27 / 80)\n",
            "train_acc: 0.4340659340659341, val_acc: 0.4411764705882353, train_loss: 1.3608478982369978, val_loss: 1.3326471518067753 (28 / 80)\n",
            "train_acc: 0.45274725274725275, val_acc: 0.4215686274509804, train_loss: 1.3331107664239275, val_loss: 1.278487561380162 (29 / 80)\n",
            "train_acc: 0.42857142857142855, val_acc: 0.43137254901960786, train_loss: 1.341480090722933, val_loss: 1.2377621163340176 (30 / 80)\n",
            "train_acc: 0.43736263736263736, val_acc: 0.47058823529411764, train_loss: 1.352489594574813, val_loss: 1.2504036110990189 (31 / 80)\n",
            "train_acc: 0.4604395604395604, val_acc: 0.4411764705882353, train_loss: 1.3061149634502747, val_loss: 1.2409884018056534 (32 / 80)\n",
            "train_acc: 0.4340659340659341, val_acc: 0.47058823529411764, train_loss: 1.3343664826927604, val_loss: 1.2376562532256632 (33 / 80)\n",
            "train_acc: 0.45274725274725275, val_acc: 0.5098039215686274, train_loss: 1.2979740074702673, val_loss: 1.2098681085249956 (34 / 80)\n",
            "train_acc: 0.45274725274725275, val_acc: 0.5294117647058824, train_loss: 1.2778548588464549, val_loss: 1.1775310512851267 (35 / 80)\n",
            "train_acc: 0.45494505494505494, val_acc: 0.5490196078431373, train_loss: 1.269068439583202, val_loss: 1.1456160527818344 (36 / 80)\n",
            "train_acc: 0.4703296703296703, val_acc: 0.5196078431372549, train_loss: 1.26654559472105, val_loss: 1.150263100862503 (37 / 80)\n",
            "train_acc: 0.46813186813186813, val_acc: 0.5196078431372549, train_loss: 1.2795136701929701, val_loss: 1.1667933867258184 (38 / 80)\n",
            "train_acc: 0.4945054945054945, val_acc: 0.5098039215686274, train_loss: 1.2271040736973942, val_loss: 1.149132449837292 (39 / 80)\n",
            "train_acc: 0.4879120879120879, val_acc: 0.4803921568627451, train_loss: 1.2428141960075922, val_loss: 1.129891236038769 (40 / 80)\n",
            "train_acc: 0.4879120879120879, val_acc: 0.5294117647058824, train_loss: 1.2159566083452205, val_loss: 1.1370908249826992 (41 / 80)\n",
            "train_acc: 0.5032967032967033, val_acc: 0.5392156862745098, train_loss: 1.2034109519733178, val_loss: 1.1343902279348934 (42 / 80)\n",
            "train_acc: 0.5054945054945055, val_acc: 0.5196078431372549, train_loss: 1.2083419551561168, val_loss: 1.1273020032574148 (43 / 80)\n",
            "train_acc: 0.5197802197802198, val_acc: 0.5392156862745098, train_loss: 1.172936269542673, val_loss: 1.0654745470075047 (44 / 80)\n",
            "train_acc: 0.4846153846153846, val_acc: 0.5196078431372549, train_loss: 1.208640034054662, val_loss: 1.0736962591900545 (45 / 80)\n",
            "train_acc: 0.5087912087912088, val_acc: 0.5196078431372549, train_loss: 1.1902852222159668, val_loss: 1.145640178638346 (46 / 80)\n",
            "train_acc: 0.5054945054945055, val_acc: 0.5294117647058824, train_loss: 1.1699208600835487, val_loss: 1.069325697772643 (47 / 80)\n",
            "train_acc: 0.545054945054945, val_acc: 0.5294117647058824, train_loss: 1.1539468929007812, val_loss: 1.0355035908081953 (48 / 80)\n",
            "train_acc: 0.5406593406593406, val_acc: 0.5588235294117647, train_loss: 1.1126667866995046, val_loss: 1.019414566895541 (49 / 80)\n",
            "train_acc: 0.5725274725274725, val_acc: 0.5686274509803921, train_loss: 1.0782219557971744, val_loss: 1.0385705705951243 (50 / 80)\n",
            "train_acc: 0.5736263736263736, val_acc: 0.5490196078431373, train_loss: 1.094855200785857, val_loss: 1.0198532833772547 (51 / 80)\n",
            "train_acc: 0.5835164835164836, val_acc: 0.5588235294117647, train_loss: 1.046268655703618, val_loss: 1.001352962325601 (52 / 80)\n",
            "train_acc: 0.5802197802197803, val_acc: 0.5784313725490197, train_loss: 1.0466830009942527, val_loss: 1.0138901910361122 (53 / 80)\n",
            "train_acc: 0.610989010989011, val_acc: 0.5490196078431373, train_loss: 1.0345159664258852, val_loss: 0.9981302531326518 (54 / 80)\n",
            "train_acc: 0.5835164835164836, val_acc: 0.5588235294117647, train_loss: 1.0214942536511264, val_loss: 1.018401743734584 (55 / 80)\n",
            "train_acc: 0.5824175824175825, val_acc: 0.5588235294117647, train_loss: 1.0303192734718323, val_loss: 0.9849163171123055 (56 / 80)\n",
            "train_acc: 0.5967032967032967, val_acc: 0.5784313725490197, train_loss: 1.0258029312877865, val_loss: 0.9880285508492413 (57 / 80)\n",
            "train_acc: 0.5967032967032967, val_acc: 0.5980392156862745, train_loss: 1.0217338286258362, val_loss: 0.9670005668612087 (58 / 80)\n",
            "train_acc: 0.5857142857142857, val_acc: 0.5882352941176471, train_loss: 1.0014176391638243, val_loss: 1.0010505108272327 (59 / 80)\n",
            "train_acc: 0.5934065934065934, val_acc: 0.5980392156862745, train_loss: 0.9855063302176339, val_loss: 0.9732162777115317 (60 / 80)\n",
            "train_acc: 0.6098901098901099, val_acc: 0.5686274509803921, train_loss: 1.0135426380477108, val_loss: 0.9680910776643192 (61 / 80)\n",
            "train_acc: 0.6296703296703297, val_acc: 0.5784313725490197, train_loss: 0.965674815269617, val_loss: 0.9671771228313446 (62 / 80)\n",
            "train_acc: 0.6197802197802198, val_acc: 0.6078431372549019, train_loss: 0.9897415005898738, val_loss: 0.9482725017211017 (63 / 80)\n",
            "train_acc: 0.6197802197802198, val_acc: 0.6078431372549019, train_loss: 0.9570761424499553, val_loss: 0.94449424217729 (64 / 80)\n",
            "train_acc: 0.6318681318681318, val_acc: 0.5980392156862745, train_loss: 0.9410898899966544, val_loss: 0.9307940426994773 (65 / 80)\n",
            "train_acc: 0.6428571428571429, val_acc: 0.5784313725490197, train_loss: 0.9516651761400831, val_loss: 0.9440909904592177 (66 / 80)\n",
            "train_acc: 0.6186813186813187, val_acc: 0.6176470588235294, train_loss: 0.9503260886931157, val_loss: 0.9427474596921135 (67 / 80)\n",
            "train_acc: 0.6373626373626373, val_acc: 0.5490196078431373, train_loss: 0.9598652514127585, val_loss: 0.9658936332253849 (68 / 80)\n",
            "train_acc: 0.6428571428571429, val_acc: 0.5980392156862745, train_loss: 0.9319594744797591, val_loss: 0.9168012405143064 (69 / 80)\n",
            "train_acc: 0.6428571428571429, val_acc: 0.6372549019607843, train_loss: 0.9411225325458652, val_loss: 0.9192430867868311 (70 / 80)\n",
            "train_acc: 0.6120879120879121, val_acc: 0.5784313725490197, train_loss: 0.9752310851773063, val_loss: 0.9184252023696899 (71 / 80)\n",
            "train_acc: 0.6076923076923076, val_acc: 0.6176470588235294, train_loss: 0.9527223768470051, val_loss: 0.9135989511714262 (72 / 80)\n",
            "train_acc: 0.6582417582417582, val_acc: 0.5784313725490197, train_loss: 0.8945918577712971, val_loss: 0.9483240325661266 (73 / 80)\n",
            "train_acc: 0.6527472527472528, val_acc: 0.6078431372549019, train_loss: 0.9061924045557505, val_loss: 0.9013097584247589 (74 / 80)\n",
            "train_acc: 0.6252747252747253, val_acc: 0.5686274509803921, train_loss: 0.9383818058522193, val_loss: 0.9079890706959892 (75 / 80)\n",
            "train_acc: 0.6483516483516484, val_acc: 0.6078431372549019, train_loss: 0.8977384721184825, val_loss: 0.8678018994191113 (76 / 80)\n",
            "train_acc: 0.6373626373626373, val_acc: 0.5980392156862745, train_loss: 0.9087454375329909, val_loss: 0.8784234926981085 (77 / 80)\n",
            "train_acc: 0.6516483516483517, val_acc: 0.5980392156862745, train_loss: 0.8926979280435122, val_loss: 0.8870417195207932 (78 / 80)\n",
            "train_acc: 0.6472527472527473, val_acc: 0.5980392156862745, train_loss: 0.8987252505925986, val_loss: 0.8761479644214406 (79 / 80)\n",
            "train_acc: 0.6681318681318681, val_acc: 0.5784313725490197, train_loss: 0.8760170785935371, val_loss: 0.8840517366633696 (80 / 80)\n",
            "lr 0.0003903058820951515, batch 15, decay 7.044665052151419e-05, gamma 0.1804629364822045, val accuracy 0.6372549019607843, val loss 0.9192430867868311 [24 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 2.0471512269306707e-05, 'batch_size': 14, 'weight_decay': 3.989894269216976e-05, 'gamma': 0.6568225712685103}\n",
            "train_acc: 0.16813186813186815, val_acc: 0.18627450980392157, train_loss: 1.792124346586374, val_loss: 1.791660287800957 (1 / 80)\n",
            "train_acc: 0.17142857142857143, val_acc: 0.18627450980392157, train_loss: 1.7921103825935951, val_loss: 1.7911547422409058 (2 / 80)\n",
            "train_acc: 0.16703296703296702, val_acc: 0.18627450980392157, train_loss: 1.7920474107448872, val_loss: 1.7907049001431932 (3 / 80)\n",
            "train_acc: 0.18021978021978022, val_acc: 0.18627450980392157, train_loss: 1.7907838582992555, val_loss: 1.7901738741818596 (4 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.18627450980392157, train_loss: 1.7901382427949173, val_loss: 1.7897148553062887 (5 / 80)\n",
            "train_acc: 0.1956043956043956, val_acc: 0.18627450980392157, train_loss: 1.7906856848643375, val_loss: 1.7892843578376023 (6 / 80)\n",
            "train_acc: 0.17802197802197803, val_acc: 0.18627450980392157, train_loss: 1.7905168588344866, val_loss: 1.7888013334835278 (7 / 80)\n",
            "train_acc: 0.1956043956043956, val_acc: 0.18627450980392157, train_loss: 1.7890455631109385, val_loss: 1.7883305058759802 (8 / 80)\n",
            "train_acc: 0.16813186813186815, val_acc: 0.18627450980392157, train_loss: 1.7904058694839478, val_loss: 1.787913205576878 (9 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.18627450980392157, train_loss: 1.787608766555786, val_loss: 1.7874251183341532 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1879120879120879\n",
            "lr 2.0471512269306707e-05, batch 14, decay 3.989894269216976e-05, gamma 0.6568225712685103, val accuracy 0.18627450980392157, val loss 1.791660287800957 [25 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00010495457171031911, 'batch_size': 11, 'weight_decay': 0.00020349658691820642, 'gamma': 0.015297586835211813}\n",
            "train_acc: 0.16703296703296702, val_acc: 0.19607843137254902, train_loss: 1.7912495762437255, val_loss: 1.7885096669197083 (1 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.18627450980392157, train_loss: 1.7877875373913692, val_loss: 1.785875001374413 (2 / 80)\n",
            "train_acc: 0.1912087912087912, val_acc: 0.18627450980392157, train_loss: 1.7859500616461366, val_loss: 1.7833287330234753 (3 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.18627450980392157, train_loss: 1.7838438987731933, val_loss: 1.7807131573265673 (4 / 80)\n",
            "train_acc: 0.1967032967032967, val_acc: 0.18627450980392157, train_loss: 1.7813487766863225, val_loss: 1.7780488039932998 (5 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.18627450980392157, train_loss: 1.7792999845284683, val_loss: 1.7756181020362705 (6 / 80)\n",
            "train_acc: 0.21208791208791208, val_acc: 0.18627450980392157, train_loss: 1.776643721480946, val_loss: 1.7725262969147926 (7 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.18627450980392157, train_loss: 1.774926562492664, val_loss: 1.769599607177809 (8 / 80)\n",
            "train_acc: 0.17912087912087912, val_acc: 0.18627450980392157, train_loss: 1.7745112960155194, val_loss: 1.7668476420290329 (9 / 80)\n",
            "train_acc: 0.2021978021978022, val_acc: 0.18627450980392157, train_loss: 1.7688288430591206, val_loss: 1.7634770239100737 (10 / 80)\n",
            "underfit -> train_accuracy = 0.2021978021978022\n",
            "lr 0.00010495457171031911, batch 11, decay 0.00020349658691820642, gamma 0.015297586835211813, val accuracy 0.19607843137254902, val loss 1.7885096669197083 [26 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 2.9750133795724745e-05, 'batch_size': 12, 'weight_decay': 1.6601071463460558e-06, 'gamma': 0.48717290620001363}\n",
            "train_acc: 0.16703296703296702, val_acc: 0.19607843137254902, train_loss: 1.7906246305822016, val_loss: 1.7886610171374153 (1 / 80)\n",
            "train_acc: 0.17362637362637362, val_acc: 0.20588235294117646, train_loss: 1.78975289444347, val_loss: 1.787432775777929 (2 / 80)\n",
            "train_acc: 0.17142857142857143, val_acc: 0.19607843137254902, train_loss: 1.7903292532805557, val_loss: 1.7862914099412806 (3 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.18627450980392157, train_loss: 1.7872293095012288, val_loss: 1.7851464397767012 (4 / 80)\n",
            "train_acc: 0.1989010989010989, val_acc: 0.18627450980392157, train_loss: 1.7868517312374743, val_loss: 1.7840272959540873 (5 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.7861382940313317, val_loss: 1.782880846191855 (6 / 80)\n",
            "train_acc: 0.17692307692307693, val_acc: 0.18627450980392157, train_loss: 1.7843823563921584, val_loss: 1.781708002090454 (7 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.18627450980392157, train_loss: 1.783820218044323, val_loss: 1.7806779426686905 (8 / 80)\n",
            "train_acc: 0.18571428571428572, val_acc: 0.18627450980392157, train_loss: 1.7828593657566951, val_loss: 1.7795043061761295 (9 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.18627450980392157, train_loss: 1.7813466698258789, val_loss: 1.778259045937482 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18461538461538463\n",
            "lr 2.9750133795724745e-05, batch 12, decay 1.6601071463460558e-06, gamma 0.48717290620001363, val accuracy 0.20588235294117646, val loss 1.787432775777929 [27 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 2.6017184738794248e-05, 'batch_size': 12, 'weight_decay': 3.560042831581887e-05, 'gamma': 0.17254524583803524}\n",
            "train_acc: 0.1989010989010989, val_acc: 0.18627450980392157, train_loss: 1.7899499791009086, val_loss: 1.7890341632506426 (1 / 80)\n",
            "train_acc: 0.17142857142857143, val_acc: 0.18627450980392157, train_loss: 1.7892077052986228, val_loss: 1.7881766136954813 (2 / 80)\n",
            "train_acc: 0.16923076923076924, val_acc: 0.19607843137254902, train_loss: 1.7889800050756433, val_loss: 1.7872781473047592 (3 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.19607843137254902, train_loss: 1.7889694965802707, val_loss: 1.786512339816374 (4 / 80)\n",
            "train_acc: 0.2043956043956044, val_acc: 0.19607843137254902, train_loss: 1.7871491592008988, val_loss: 1.7856800415936638 (5 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.22549019607843138, train_loss: 1.7857904329404726, val_loss: 1.7848697339787203 (6 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.2647058823529412, train_loss: 1.7860967680648132, val_loss: 1.7840682618758257 (7 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.30392156862745096, train_loss: 1.7848880165225858, val_loss: 1.7832280537661385 (8 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.29411764705882354, train_loss: 1.786194791636624, val_loss: 1.7824694689582377 (9 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.2549019607843137, train_loss: 1.785604637009757, val_loss: 1.781681544640485 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1901098901098901\n",
            "lr 2.6017184738794248e-05, batch 12, decay 3.560042831581887e-05, gamma 0.17254524583803524, val accuracy 0.30392156862745096, val loss 1.7832280537661385 [28 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 4.729758023257205e-05, 'batch_size': 8, 'weight_decay': 4.93601424782617e-06, 'gamma': 0.03312849022765353}\n",
            "train_acc: 0.2, val_acc: 0.1568627450980392, train_loss: 1.789693441757789, val_loss: 1.7887887206732058 (1 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.16666666666666666, train_loss: 1.7877493127361759, val_loss: 1.786117336329292 (2 / 80)\n",
            "train_acc: 0.1956043956043956, val_acc: 0.16666666666666666, train_loss: 1.7855879194133883, val_loss: 1.7833342482061947 (3 / 80)\n",
            "train_acc: 0.17692307692307693, val_acc: 0.12745098039215685, train_loss: 1.7836070451107653, val_loss: 1.7809274406994091 (4 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.17647058823529413, train_loss: 1.7824571947475056, val_loss: 1.778363545735677 (5 / 80)\n",
            "train_acc: 0.20659340659340658, val_acc: 0.18627450980392157, train_loss: 1.7783737263836703, val_loss: 1.7758746871761246 (6 / 80)\n",
            "train_acc: 0.2153846153846154, val_acc: 0.18627450980392157, train_loss: 1.7765587783121801, val_loss: 1.7731630638533948 (7 / 80)\n",
            "train_acc: 0.2, val_acc: 0.18627450980392157, train_loss: 1.77428066101703, val_loss: 1.7704892789616304 (8 / 80)\n",
            "train_acc: 0.1824175824175824, val_acc: 0.18627450980392157, train_loss: 1.7732354392062177, val_loss: 1.7680195149253397 (9 / 80)\n",
            "train_acc: 0.1912087912087912, val_acc: 0.18627450980392157, train_loss: 1.7717518575898894, val_loss: 1.7656892327701343 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1912087912087912\n",
            "lr 4.729758023257205e-05, batch 8, decay 4.93601424782617e-06, gamma 0.03312849022765353, val accuracy 0.18627450980392157, val loss 1.7758746871761246 [29 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 2.386385329167478e-05, 'batch_size': 15, 'weight_decay': 1.3457871009528628e-06, 'gamma': 0.09843879133617368}\n",
            "train_acc: 0.16483516483516483, val_acc: 0.18627450980392157, train_loss: 1.7917939254215784, val_loss: 1.790701918742236 (1 / 80)\n",
            "train_acc: 0.17472527472527472, val_acc: 0.18627450980392157, train_loss: 1.7910926178261475, val_loss: 1.7903118168606478 (2 / 80)\n",
            "train_acc: 0.16923076923076924, val_acc: 0.18627450980392157, train_loss: 1.7914624070073222, val_loss: 1.7899173673461466 (3 / 80)\n",
            "train_acc: 0.1813186813186813, val_acc: 0.18627450980392157, train_loss: 1.7906805663318424, val_loss: 1.789564023999607 (4 / 80)\n",
            "train_acc: 0.17252747252747253, val_acc: 0.18627450980392157, train_loss: 1.7904979036404536, val_loss: 1.7892055967274834 (5 / 80)\n",
            "train_acc: 0.1989010989010989, val_acc: 0.18627450980392157, train_loss: 1.789533752006489, val_loss: 1.7888229945126701 (6 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.18627450980392157, train_loss: 1.7889932145129193, val_loss: 1.788451745229609 (7 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.18627450980392157, train_loss: 1.7886959229196822, val_loss: 1.7880537965718437 (8 / 80)\n",
            "train_acc: 0.16923076923076924, val_acc: 0.18627450980392157, train_loss: 1.7891396788450389, val_loss: 1.787701866206001 (9 / 80)\n",
            "train_acc: 0.16923076923076924, val_acc: 0.18627450980392157, train_loss: 1.7894682235770174, val_loss: 1.7873330572072197 (10 / 80)\n",
            "underfit -> train_accuracy = 0.16923076923076924\n",
            "lr 2.386385329167478e-05, batch 15, decay 1.3457871009528628e-06, gamma 0.09843879133617368, val accuracy 0.18627450980392157, val loss 1.790701918742236 [30 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0008706177961383172, 'batch_size': 13, 'weight_decay': 1.4584053651222393e-06, 'gamma': 0.01994298786624128}\n",
            "train_acc: 0.1824175824175824, val_acc: 0.18627450980392157, train_loss: 1.7841432741710117, val_loss: 1.772350943556019 (1 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.18627450980392157, train_loss: 1.7676337054797582, val_loss: 1.7513117918781205 (2 / 80)\n",
            "train_acc: 0.2, val_acc: 0.2647058823529412, train_loss: 1.7582950506891524, val_loss: 1.7421181248683555 (3 / 80)\n",
            "train_acc: 0.24725274725274726, val_acc: 0.24509803921568626, train_loss: 1.748421915939876, val_loss: 1.7207567656741423 (4 / 80)\n",
            "train_acc: 0.2868131868131868, val_acc: 0.3235294117647059, train_loss: 1.7104151998247419, val_loss: 1.6557234443870246 (5 / 80)\n",
            "train_acc: 0.2945054945054945, val_acc: 0.37254901960784315, train_loss: 1.6605187790734428, val_loss: 1.5805719097455342 (6 / 80)\n",
            "train_acc: 0.34505494505494505, val_acc: 0.3235294117647059, train_loss: 1.6136211565562657, val_loss: 1.534489471538394 (7 / 80)\n",
            "train_acc: 0.3252747252747253, val_acc: 0.30392156862745096, train_loss: 1.5746892605509077, val_loss: 1.5364798459352231 (8 / 80)\n",
            "train_acc: 0.34835164835164834, val_acc: 0.3627450980392157, train_loss: 1.524624867098672, val_loss: 1.5426063210356469 (9 / 80)\n",
            "train_acc: 0.34285714285714286, val_acc: 0.3431372549019608, train_loss: 1.499639986242567, val_loss: 1.3630915050413095 (10 / 80)\n",
            "train_acc: 0.321978021978022, val_acc: 0.38235294117647056, train_loss: 1.538921412399837, val_loss: 1.3743999436789869 (11 / 80)\n",
            "train_acc: 0.36923076923076925, val_acc: 0.4019607843137255, train_loss: 1.4514424937111992, val_loss: 1.4265198485524047 (12 / 80)\n",
            "train_acc: 0.37362637362637363, val_acc: 0.38235294117647056, train_loss: 1.4731410809925625, val_loss: 1.3568417838975495 (13 / 80)\n",
            "train_acc: 0.4, val_acc: 0.39215686274509803, train_loss: 1.3941768271582466, val_loss: 1.4921466123824025 (14 / 80)\n",
            "train_acc: 0.4, val_acc: 0.47058823529411764, train_loss: 1.4074682201657975, val_loss: 1.3170041500353347 (15 / 80)\n",
            "train_acc: 0.3978021978021978, val_acc: 0.43137254901960786, train_loss: 1.3818688239370074, val_loss: 1.2180708155912512 (16 / 80)\n",
            "train_acc: 0.4186813186813187, val_acc: 0.45098039215686275, train_loss: 1.3649052952017102, val_loss: 1.2305282763406342 (17 / 80)\n",
            "train_acc: 0.4340659340659341, val_acc: 0.4215686274509804, train_loss: 1.3496581094605582, val_loss: 1.2518646705384349 (18 / 80)\n",
            "train_acc: 0.44065934065934065, val_acc: 0.3333333333333333, train_loss: 1.3249767592975072, val_loss: 1.3870112872591205 (19 / 80)\n",
            "train_acc: 0.42967032967032964, val_acc: 0.45098039215686275, train_loss: 1.3675018634114946, val_loss: 1.274580458799998 (20 / 80)\n",
            "train_acc: 0.43626373626373627, val_acc: 0.46078431372549017, train_loss: 1.2936232439109256, val_loss: 1.194428850622738 (21 / 80)\n",
            "train_acc: 0.43736263736263736, val_acc: 0.4803921568627451, train_loss: 1.3101743749209813, val_loss: 1.2119575750594045 (22 / 80)\n",
            "train_acc: 0.4703296703296703, val_acc: 0.45098039215686275, train_loss: 1.2637664828981672, val_loss: 1.2163027291204416 (23 / 80)\n",
            "train_acc: 0.4736263736263736, val_acc: 0.5490196078431373, train_loss: 1.2515309716973986, val_loss: 1.1297414922246747 (24 / 80)\n",
            "train_acc: 0.4714285714285714, val_acc: 0.5098039215686274, train_loss: 1.2166887819766998, val_loss: 1.0979264267519409 (25 / 80)\n",
            "train_acc: 0.45274725274725275, val_acc: 0.5196078431372549, train_loss: 1.2303145698138647, val_loss: 1.1055773350537992 (26 / 80)\n",
            "train_acc: 0.47692307692307695, val_acc: 0.5294117647058824, train_loss: 1.2130883838449205, val_loss: 1.0866360126757155 (27 / 80)\n",
            "train_acc: 0.47912087912087914, val_acc: 0.5588235294117647, train_loss: 1.2153335358415331, val_loss: 1.0737867285223568 (28 / 80)\n",
            "train_acc: 0.5142857142857142, val_acc: 0.5490196078431373, train_loss: 1.1402126133441925, val_loss: 1.0166143050380783 (29 / 80)\n",
            "train_acc: 0.5131868131868131, val_acc: 0.49019607843137253, train_loss: 1.1506998743329728, val_loss: 1.097943311812831 (30 / 80)\n",
            "train_acc: 0.489010989010989, val_acc: 0.5588235294117647, train_loss: 1.192284255368369, val_loss: 0.9854967003943873 (31 / 80)\n",
            "train_acc: 0.5351648351648352, val_acc: 0.5784313725490197, train_loss: 1.127329181773322, val_loss: 0.9860544368332508 (32 / 80)\n",
            "train_acc: 0.5571428571428572, val_acc: 0.5098039215686274, train_loss: 1.0701536016804831, val_loss: 0.9781079303984549 (33 / 80)\n",
            "train_acc: 0.5637362637362637, val_acc: 0.5392156862745098, train_loss: 1.0701594139848436, val_loss: 0.9426537179479412 (34 / 80)\n",
            "train_acc: 0.5439560439560439, val_acc: 0.5980392156862745, train_loss: 1.052532924924578, val_loss: 1.0270421575097477 (35 / 80)\n",
            "train_acc: 0.5593406593406594, val_acc: 0.5490196078431373, train_loss: 1.0353284784725734, val_loss: 0.9054290476967307 (36 / 80)\n",
            "train_acc: 0.5824175824175825, val_acc: 0.6078431372549019, train_loss: 1.0106829860380717, val_loss: 0.8913679146299175 (37 / 80)\n",
            "train_acc: 0.578021978021978, val_acc: 0.6764705882352942, train_loss: 1.0316862485238485, val_loss: 0.9159897641808379 (38 / 80)\n",
            "train_acc: 0.6131868131868132, val_acc: 0.6568627450980392, train_loss: 0.9602800003119878, val_loss: 0.8132956559751549 (39 / 80)\n",
            "train_acc: 0.6450549450549451, val_acc: 0.6078431372549019, train_loss: 0.8926832394940513, val_loss: 0.9239167296418956 (40 / 80)\n",
            "train_acc: 0.6307692307692307, val_acc: 0.5980392156862745, train_loss: 0.9065515820469175, val_loss: 0.9172965586185455 (41 / 80)\n",
            "train_acc: 0.6626373626373626, val_acc: 0.5686274509803921, train_loss: 0.8716352718217032, val_loss: 0.9824406352697634 (42 / 80)\n",
            "train_acc: 0.6879120879120879, val_acc: 0.5980392156862745, train_loss: 0.8041958459786006, val_loss: 0.9210132366301966 (43 / 80)\n",
            "train_acc: 0.7054945054945055, val_acc: 0.5980392156862745, train_loss: 0.7546816936561039, val_loss: 1.0571731317277049 (44 / 80)\n",
            "train_acc: 0.7032967032967034, val_acc: 0.5686274509803921, train_loss: 0.7554961817605155, val_loss: 0.9305593219457888 (45 / 80)\n",
            "train_acc: 0.6923076923076923, val_acc: 0.6078431372549019, train_loss: 0.7677646092006138, val_loss: 0.8163983932897156 (46 / 80)\n",
            "train_acc: 0.7505494505494505, val_acc: 0.5980392156862745, train_loss: 0.650122600368091, val_loss: 0.8747790639891344 (47 / 80)\n",
            "train_acc: 0.7241758241758242, val_acc: 0.6176470588235294, train_loss: 0.6522195266825812, val_loss: 0.8964750500870686 (48 / 80)\n",
            "train_acc: 0.8131868131868132, val_acc: 0.6568627450980392, train_loss: 0.5139930906040328, val_loss: 0.7879316123092875 (49 / 80)\n",
            "train_acc: 0.8307692307692308, val_acc: 0.6372549019607843, train_loss: 0.4627256314669337, val_loss: 0.7889455875345305 (50 / 80)\n",
            "train_acc: 0.8527472527472527, val_acc: 0.6470588235294118, train_loss: 0.43191538453102113, val_loss: 0.7935893068126604 (51 / 80)\n",
            "train_acc: 0.8505494505494505, val_acc: 0.6470588235294118, train_loss: 0.4302514508366585, val_loss: 0.8033873684266034 (52 / 80)\n",
            "train_acc: 0.8461538461538461, val_acc: 0.6568627450980392, train_loss: 0.4334658680217607, val_loss: 0.7997698328074287 (53 / 80)\n",
            "train_acc: 0.843956043956044, val_acc: 0.6666666666666666, train_loss: 0.42093694061040876, val_loss: 0.803334009997985 (54 / 80)\n",
            "train_acc: 0.8571428571428571, val_acc: 0.6666666666666666, train_loss: 0.41758042850664684, val_loss: 0.7987308969684676 (55 / 80)\n",
            "train_acc: 0.8681318681318682, val_acc: 0.6568627450980392, train_loss: 0.39172485816691605, val_loss: 0.8051518660550024 (56 / 80)\n",
            "train_acc: 0.8549450549450549, val_acc: 0.6470588235294118, train_loss: 0.402584157032626, val_loss: 0.8073819557241365 (57 / 80)\n",
            "train_acc: 0.8593406593406593, val_acc: 0.6470588235294118, train_loss: 0.3948449640401772, val_loss: 0.8017332711640526 (58 / 80)\n",
            "train_acc: 0.8747252747252747, val_acc: 0.6470588235294118, train_loss: 0.37062434351869994, val_loss: 0.8205246522146112 (59 / 80)\n",
            "train_acc: 0.8802197802197802, val_acc: 0.6470588235294118, train_loss: 0.36669859109180314, val_loss: 0.8231775997316136 (60 / 80)\n",
            "train_acc: 0.8736263736263736, val_acc: 0.6372549019607843, train_loss: 0.3595014233674322, val_loss: 0.8216704369760027 (61 / 80)\n",
            "train_acc: 0.8747252747252747, val_acc: 0.6372549019607843, train_loss: 0.3607011470411505, val_loss: 0.8087817412965438 (62 / 80)\n",
            "train_acc: 0.8747252747252747, val_acc: 0.6568627450980392, train_loss: 0.3449524089694023, val_loss: 0.8297775603976905 (63 / 80)\n",
            "overfit -> train_accuracy 0.8846153846153846, val_accuracy 0.6274509803921569\n",
            "lr 0.0008706177961383172, batch 13, decay 1.4584053651222393e-06, gamma 0.01994298786624128, val accuracy 0.6764705882352942, val loss 0.9159897641808379 [31 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.424777220495185e-05, 'batch_size': 12, 'weight_decay': 1.2710284628414733e-06, 'gamma': 0.010144299472033349}\n",
            "train_acc: 0.189010989010989, val_acc: 0.17647058823529413, train_loss: 1.7916447621125442, val_loss: 1.791276588159449 (1 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.17647058823529413, train_loss: 1.7913052323100331, val_loss: 1.7908301704070146 (2 / 80)\n",
            "train_acc: 0.1978021978021978, val_acc: 0.17647058823529413, train_loss: 1.7896240344414345, val_loss: 1.7904389816171982 (3 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.17647058823529413, train_loss: 1.789979806313148, val_loss: 1.7900230253443998 (4 / 80)\n",
            "train_acc: 0.18571428571428572, val_acc: 0.17647058823529413, train_loss: 1.7899260995152233, val_loss: 1.7895853449316586 (5 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.17647058823529413, train_loss: 1.7889310834172008, val_loss: 1.7891946470036226 (6 / 80)\n",
            "train_acc: 0.18461538461538463, val_acc: 0.17647058823529413, train_loss: 1.7900339776343042, val_loss: 1.7887550732668709 (7 / 80)\n",
            "train_acc: 0.1989010989010989, val_acc: 0.17647058823529413, train_loss: 1.788965573153653, val_loss: 1.7883985533433802 (8 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.18627450980392157, train_loss: 1.788951772909898, val_loss: 1.787999622962054 (9 / 80)\n",
            "train_acc: 0.15604395604395604, val_acc: 0.18627450980392157, train_loss: 1.788808971970946, val_loss: 1.7876230618532967 (10 / 80)\n",
            "underfit -> train_accuracy = 0.15604395604395604\n",
            "lr 1.424777220495185e-05, batch 12, decay 1.2710284628414733e-06, gamma 0.010144299472033349, val accuracy 0.18627450980392157, val loss 1.787999622962054 [32 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 4.468074940990011e-05, 'batch_size': 8, 'weight_decay': 6.641953734570395e-05, 'gamma': 0.9973182396502519}\n",
            "train_acc: 0.14725274725274726, val_acc: 0.17647058823529413, train_loss: 1.7932392353539939, val_loss: 1.7910609782910814 (1 / 80)\n",
            "train_acc: 0.16923076923076924, val_acc: 0.17647058823529413, train_loss: 1.7895990748981854, val_loss: 1.7887572611079496 (2 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.17647058823529413, train_loss: 1.788265016576746, val_loss: 1.7865619659423828 (3 / 80)\n",
            "train_acc: 0.17032967032967034, val_acc: 0.19607843137254902, train_loss: 1.7870249572691026, val_loss: 1.7844498391244925 (4 / 80)\n",
            "train_acc: 0.1912087912087912, val_acc: 0.21568627450980393, train_loss: 1.7850531418244917, val_loss: 1.7823533076866 (5 / 80)\n",
            "train_acc: 0.16263736263736264, val_acc: 0.18627450980392157, train_loss: 1.784205059166793, val_loss: 1.7801060699949078 (6 / 80)\n",
            "train_acc: 0.18681318681318682, val_acc: 0.18627450980392157, train_loss: 1.7822273841271035, val_loss: 1.7779551884707283 (7 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.18627450980392157, train_loss: 1.7780556555632707, val_loss: 1.7755350528978835 (8 / 80)\n",
            "train_acc: 0.1835164835164835, val_acc: 0.18627450980392157, train_loss: 1.7773241632587307, val_loss: 1.7730525334676106 (9 / 80)\n",
            "train_acc: 0.21428571428571427, val_acc: 0.18627450980392157, train_loss: 1.7741116631162035, val_loss: 1.7706427036547194 (10 / 80)\n",
            "underfit -> train_accuracy = 0.21428571428571427\n",
            "lr 4.468074940990011e-05, batch 8, decay 6.641953734570395e-05, gamma 0.9973182396502519, val accuracy 0.21568627450980393, val loss 1.7823533076866 [33 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00042277150250555486, 'batch_size': 11, 'weight_decay': 8.92920211559269e-05, 'gamma': 0.05010441705290558}\n",
            "train_acc: 0.18461538461538463, val_acc: 0.18627450980392157, train_loss: 1.7874811138425555, val_loss: 1.7778390938160467 (1 / 80)\n",
            "train_acc: 0.18021978021978022, val_acc: 0.18627450980392157, train_loss: 1.773616503752195, val_loss: 1.7613814309531568 (2 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.18627450980392157, train_loss: 1.7629749939991877, val_loss: 1.7485989402322208 (3 / 80)\n",
            "train_acc: 0.1934065934065934, val_acc: 0.2549019607843137, train_loss: 1.7573640101558559, val_loss: 1.7413282511281032 (4 / 80)\n",
            "train_acc: 0.189010989010989, val_acc: 0.19607843137254902, train_loss: 1.7517189794844323, val_loss: 1.7345028543004803 (5 / 80)\n",
            "train_acc: 0.2032967032967033, val_acc: 0.2647058823529412, train_loss: 1.7378348845701952, val_loss: 1.7177715523570192 (6 / 80)\n",
            "train_acc: 0.23956043956043957, val_acc: 0.20588235294117646, train_loss: 1.731104130273337, val_loss: 1.6951057981042301 (7 / 80)\n",
            "train_acc: 0.27912087912087913, val_acc: 0.29411764705882354, train_loss: 1.694598871272999, val_loss: 1.649055447064194 (8 / 80)\n",
            "train_acc: 0.3098901098901099, val_acc: 0.30392156862745096, train_loss: 1.648763644695282, val_loss: 1.5882482540373708 (9 / 80)\n",
            "train_acc: 0.3153846153846154, val_acc: 0.35294117647058826, train_loss: 1.6091711210680533, val_loss: 1.5122931482745152 (10 / 80)\n",
            "train_acc: 0.3274725274725275, val_acc: 0.3333333333333333, train_loss: 1.5663337362991585, val_loss: 1.4753375953319026 (11 / 80)\n",
            "train_acc: 0.3505494505494505, val_acc: 0.3235294117647059, train_loss: 1.5348281405784272, val_loss: 1.7765023100609874 (12 / 80)\n",
            "train_acc: 0.33296703296703295, val_acc: 0.39215686274509803, train_loss: 1.5664324237750127, val_loss: 1.479382717141918 (13 / 80)\n",
            "train_acc: 0.34285714285714286, val_acc: 0.39215686274509803, train_loss: 1.527586794423533, val_loss: 1.3899599944843966 (14 / 80)\n",
            "train_acc: 0.3868131868131868, val_acc: 0.39215686274509803, train_loss: 1.4327610059098883, val_loss: 1.369269161832099 (15 / 80)\n",
            "train_acc: 0.3934065934065934, val_acc: 0.4117647058823529, train_loss: 1.4426068189379933, val_loss: 1.3668278163554621 (16 / 80)\n",
            "train_acc: 0.3868131868131868, val_acc: 0.45098039215686275, train_loss: 1.430242899616996, val_loss: 1.3256764388551898 (17 / 80)\n",
            "train_acc: 0.3648351648351648, val_acc: 0.45098039215686275, train_loss: 1.4344835067843342, val_loss: 1.3682443429442013 (18 / 80)\n",
            "train_acc: 0.4043956043956044, val_acc: 0.4215686274509804, train_loss: 1.4285163044929505, val_loss: 1.27606133035585 (19 / 80)\n",
            "train_acc: 0.4318681318681319, val_acc: 0.4803921568627451, train_loss: 1.3858945765993098, val_loss: 1.2546264844782211 (20 / 80)\n",
            "train_acc: 0.4208791208791209, val_acc: 0.45098039215686275, train_loss: 1.3610507734529265, val_loss: 1.2751587185205198 (21 / 80)\n",
            "train_acc: 0.42747252747252745, val_acc: 0.4411764705882353, train_loss: 1.3443362903463971, val_loss: 1.306987867635839 (22 / 80)\n",
            "train_acc: 0.4054945054945055, val_acc: 0.4215686274509804, train_loss: 1.3654748481053574, val_loss: 1.290007184533512 (23 / 80)\n",
            "train_acc: 0.4153846153846154, val_acc: 0.5196078431372549, train_loss: 1.3296299910152352, val_loss: 1.1774983534625931 (24 / 80)\n",
            "train_acc: 0.43626373626373627, val_acc: 0.5294117647058824, train_loss: 1.3544239270163105, val_loss: 1.2443202850865382 (25 / 80)\n",
            "train_acc: 0.44505494505494503, val_acc: 0.4803921568627451, train_loss: 1.2980387557338882, val_loss: 1.16328634000292 (26 / 80)\n",
            "train_acc: 0.45714285714285713, val_acc: 0.43137254901960786, train_loss: 1.3031607867597224, val_loss: 1.2823251982529957 (27 / 80)\n",
            "train_acc: 0.43736263736263736, val_acc: 0.5196078431372549, train_loss: 1.2858133195520758, val_loss: 1.1567960767184986 (28 / 80)\n",
            "train_acc: 0.4593406593406593, val_acc: 0.5, train_loss: 1.289315993838258, val_loss: 1.1736660634770113 (29 / 80)\n",
            "train_acc: 0.48131868131868133, val_acc: 0.49019607843137253, train_loss: 1.2769146040900723, val_loss: 1.1966413043293298 (30 / 80)\n",
            "train_acc: 0.45384615384615384, val_acc: 0.5196078431372549, train_loss: 1.259935155740151, val_loss: 1.097568697204777 (31 / 80)\n",
            "train_acc: 0.4747252747252747, val_acc: 0.5196078431372549, train_loss: 1.2372696347289034, val_loss: 1.1467205771044189 (32 / 80)\n",
            "train_acc: 0.4736263736263736, val_acc: 0.5294117647058824, train_loss: 1.2344833848895607, val_loss: 1.1307623666875504 (33 / 80)\n",
            "train_acc: 0.5065934065934066, val_acc: 0.43137254901960786, train_loss: 1.1870667910837864, val_loss: 1.2556563598268173 (34 / 80)\n",
            "train_acc: 0.4714285714285714, val_acc: 0.5294117647058824, train_loss: 1.238271366436403, val_loss: 1.0874210818141115 (35 / 80)\n",
            "train_acc: 0.4912087912087912, val_acc: 0.5392156862745098, train_loss: 1.1854116663173004, val_loss: 1.1182220584037257 (36 / 80)\n",
            "train_acc: 0.4978021978021978, val_acc: 0.5196078431372549, train_loss: 1.1788112728805333, val_loss: 1.0569641023289924 (37 / 80)\n",
            "train_acc: 0.5197802197802198, val_acc: 0.46078431372549017, train_loss: 1.154771542614633, val_loss: 1.2637746842468487 (38 / 80)\n",
            "train_acc: 0.5076923076923077, val_acc: 0.5, train_loss: 1.156954393020043, val_loss: 1.119896601228153 (39 / 80)\n",
            "train_acc: 0.5252747252747253, val_acc: 0.5686274509803921, train_loss: 1.1546868579073266, val_loss: 0.9954492636755401 (40 / 80)\n",
            "train_acc: 0.5263736263736264, val_acc: 0.5294117647058824, train_loss: 1.131008312204382, val_loss: 0.9922554919532701 (41 / 80)\n",
            "train_acc: 0.5340659340659341, val_acc: 0.5588235294117647, train_loss: 1.1040243729130252, val_loss: 0.9895493586858114 (42 / 80)\n",
            "train_acc: 0.5593406593406594, val_acc: 0.5490196078431373, train_loss: 1.1056138136884668, val_loss: 1.0245047597324146 (43 / 80)\n",
            "train_acc: 0.5593406593406594, val_acc: 0.5686274509803921, train_loss: 1.0765230974653266, val_loss: 1.064999179220667 (44 / 80)\n",
            "train_acc: 0.5604395604395604, val_acc: 0.5588235294117647, train_loss: 1.0574009068064638, val_loss: 0.9777575462472206 (45 / 80)\n",
            "train_acc: 0.5615384615384615, val_acc: 0.5588235294117647, train_loss: 1.0709088839017429, val_loss: 0.9344190008500043 (46 / 80)\n",
            "train_acc: 0.5703296703296703, val_acc: 0.6176470588235294, train_loss: 1.0292751229398853, val_loss: 0.9868218331944709 (47 / 80)\n",
            "train_acc: 0.5813186813186814, val_acc: 0.6078431372549019, train_loss: 1.0171147672684637, val_loss: 1.0072449048360188 (48 / 80)\n",
            "train_acc: 0.6373626373626373, val_acc: 0.6372549019607843, train_loss: 0.9442911126456418, val_loss: 0.9150579238639158 (49 / 80)\n",
            "train_acc: 0.6417582417582418, val_acc: 0.6274509803921569, train_loss: 0.9065487685766849, val_loss: 0.9018858647813984 (50 / 80)\n",
            "train_acc: 0.6582417582417582, val_acc: 0.6372549019607843, train_loss: 0.8967957488455616, val_loss: 0.9014404135591844 (51 / 80)\n",
            "train_acc: 0.6362637362637362, val_acc: 0.6372549019607843, train_loss: 0.907862594297954, val_loss: 0.8990702035964704 (52 / 80)\n",
            "train_acc: 0.6461538461538462, val_acc: 0.6176470588235294, train_loss: 0.8546821781239666, val_loss: 0.8961662425130021 (53 / 80)\n",
            "train_acc: 0.6758241758241759, val_acc: 0.6176470588235294, train_loss: 0.8838955394186816, val_loss: 0.8897278808495578 (54 / 80)\n",
            "train_acc: 0.6659340659340659, val_acc: 0.6176470588235294, train_loss: 0.8624265155294439, val_loss: 0.8867186830908644 (55 / 80)\n",
            "train_acc: 0.667032967032967, val_acc: 0.6274509803921569, train_loss: 0.8501053152831046, val_loss: 0.9169801461930368 (56 / 80)\n",
            "train_acc: 0.6857142857142857, val_acc: 0.6078431372549019, train_loss: 0.8338369376384295, val_loss: 0.8970037318912207 (57 / 80)\n",
            "train_acc: 0.6604395604395604, val_acc: 0.6176470588235294, train_loss: 0.8466945972416428, val_loss: 0.8935201372586045 (58 / 80)\n",
            "train_acc: 0.656043956043956, val_acc: 0.6078431372549019, train_loss: 0.8529690557456279, val_loss: 0.9096645914456424 (59 / 80)\n",
            "train_acc: 0.667032967032967, val_acc: 0.5980392156862745, train_loss: 0.8252973555536061, val_loss: 0.8773717173174316 (60 / 80)\n",
            "train_acc: 0.6758241758241759, val_acc: 0.6176470588235294, train_loss: 0.838736625621607, val_loss: 0.883359810885261 (61 / 80)\n",
            "train_acc: 0.6989010989010989, val_acc: 0.6078431372549019, train_loss: 0.8151993853705269, val_loss: 0.8767318944720661 (62 / 80)\n",
            "train_acc: 0.667032967032967, val_acc: 0.6078431372549019, train_loss: 0.8303161371540237, val_loss: 0.8796514710959267 (63 / 80)\n",
            "train_acc: 0.6736263736263737, val_acc: 0.6078431372549019, train_loss: 0.8263726070032015, val_loss: 0.91414427903353 (64 / 80)\n",
            "train_acc: 0.6868131868131868, val_acc: 0.5980392156862745, train_loss: 0.8011233988371524, val_loss: 0.886528387373569 (65 / 80)\n",
            "train_acc: 0.689010989010989, val_acc: 0.5980392156862745, train_loss: 0.8095920059052143, val_loss: 0.8886955736898908 (66 / 80)\n",
            "train_acc: 0.6967032967032967, val_acc: 0.6372549019607843, train_loss: 0.8094909296258466, val_loss: 0.8823528973495259 (67 / 80)\n",
            "train_acc: 0.676923076923077, val_acc: 0.5980392156862745, train_loss: 0.8214907560702208, val_loss: 0.8800304827152514 (68 / 80)\n",
            "train_acc: 0.6901098901098901, val_acc: 0.5980392156862745, train_loss: 0.7765353493965589, val_loss: 0.9149051741057751 (69 / 80)\n",
            "train_acc: 0.6912087912087912, val_acc: 0.5882352941176471, train_loss: 0.804972703738527, val_loss: 0.8780361977862376 (70 / 80)\n",
            "train_acc: 0.7252747252747253, val_acc: 0.6078431372549019, train_loss: 0.7677958928294234, val_loss: 0.8641806496124641 (71 / 80)\n",
            "train_acc: 0.6967032967032967, val_acc: 0.6078431372549019, train_loss: 0.7777784097980667, val_loss: 0.8669253383197036 (72 / 80)\n",
            "train_acc: 0.701098901098901, val_acc: 0.6078431372549019, train_loss: 0.7779287894348522, val_loss: 0.8793593881761327 (73 / 80)\n",
            "train_acc: 0.7230769230769231, val_acc: 0.6176470588235294, train_loss: 0.773986876633141, val_loss: 0.8710388710685805 (74 / 80)\n",
            "train_acc: 0.6989010989010989, val_acc: 0.6176470588235294, train_loss: 0.7549609418098743, val_loss: 0.8794083902064491 (75 / 80)\n",
            "train_acc: 0.7296703296703296, val_acc: 0.6176470588235294, train_loss: 0.7129688055305691, val_loss: 0.8726615806420644 (76 / 80)\n",
            "train_acc: 0.7098901098901099, val_acc: 0.5882352941176471, train_loss: 0.7776574243556013, val_loss: 0.870239359198832 (77 / 80)\n",
            "train_acc: 0.7252747252747253, val_acc: 0.5980392156862745, train_loss: 0.7429070983941738, val_loss: 0.8872787379751018 (78 / 80)\n",
            "train_acc: 0.6956043956043956, val_acc: 0.6372549019607843, train_loss: 0.7768962108826899, val_loss: 0.8853978263396843 (79 / 80)\n",
            "train_acc: 0.7329670329670329, val_acc: 0.6176470588235294, train_loss: 0.7425076752737328, val_loss: 0.8781935292131761 (80 / 80)\n",
            "lr 0.00042277150250555486, batch 11, decay 8.92920211559269e-05, gamma 0.05010441705290558, val accuracy 0.6372549019607843, val loss 0.9150579238639158 [34 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.3850081251055618e-05, 'batch_size': 11, 'weight_decay': 3.8236394397401325e-05, 'gamma': 0.13172199339080107}\n",
            "train_acc: 0.17912087912087912, val_acc: 0.18627450980392157, train_loss: 1.7917745467070694, val_loss: 1.791317746919744 (1 / 80)\n",
            "train_acc: 0.16593406593406593, val_acc: 0.18627450980392157, train_loss: 1.7913470670417115, val_loss: 1.7909238396906386 (2 / 80)\n",
            "train_acc: 0.18021978021978022, val_acc: 0.18627450980392157, train_loss: 1.7899844180096636, val_loss: 1.790480845114764 (3 / 80)\n",
            "train_acc: 0.17912087912087912, val_acc: 0.18627450980392157, train_loss: 1.790586885384151, val_loss: 1.790107289950053 (4 / 80)\n",
            "train_acc: 0.189010989010989, val_acc: 0.18627450980392157, train_loss: 1.7901429152750707, val_loss: 1.7897239958538729 (5 / 80)\n",
            "train_acc: 0.1813186813186813, val_acc: 0.18627450980392157, train_loss: 1.7899472323092787, val_loss: 1.7893135722945719 (6 / 80)\n",
            "train_acc: 0.1813186813186813, val_acc: 0.18627450980392157, train_loss: 1.7893614250225025, val_loss: 1.7889201781328987 (7 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.18627450980392157, train_loss: 1.7890305553163801, val_loss: 1.788537599292456 (8 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.18627450980392157, train_loss: 1.7880786259095747, val_loss: 1.7881855298491085 (9 / 80)\n",
            "train_acc: 0.18021978021978022, val_acc: 0.18627450980392157, train_loss: 1.788744129060389, val_loss: 1.7877897865632002 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18021978021978022\n",
            "lr 1.3850081251055618e-05, batch 11, decay 3.8236394397401325e-05, gamma 0.13172199339080107, val accuracy 0.18627450980392157, val loss 1.791317746919744 [35 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00013957251474088612, 'batch_size': 10, 'weight_decay': 4.205625127268475e-06, 'gamma': 0.01711142737927632}\n",
            "train_acc: 0.17472527472527472, val_acc: 0.28431372549019607, train_loss: 1.7863457006412549, val_loss: 1.7818695821014106 (1 / 80)\n",
            "train_acc: 0.1945054945054945, val_acc: 0.22549019607843138, train_loss: 1.7798291182780004, val_loss: 1.7753302256266277 (2 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.18627450980392157, train_loss: 1.7749153690023736, val_loss: 1.7681921580258537 (3 / 80)\n",
            "train_acc: 0.2098901098901099, val_acc: 0.18627450980392157, train_loss: 1.7703981320936601, val_loss: 1.7615951603534175 (4 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.18627450980392157, train_loss: 1.764074784058791, val_loss: 1.7551386636846207 (5 / 80)\n",
            "train_acc: 0.1879120879120879, val_acc: 0.18627450980392157, train_loss: 1.760349636549478, val_loss: 1.7501634359359741 (6 / 80)\n",
            "train_acc: 0.1956043956043956, val_acc: 0.18627450980392157, train_loss: 1.7556127204999818, val_loss: 1.7455393286312328 (7 / 80)\n",
            "train_acc: 0.1956043956043956, val_acc: 0.2647058823529412, train_loss: 1.7538623259617732, val_loss: 1.7423561554329068 (8 / 80)\n",
            "train_acc: 0.1978021978021978, val_acc: 0.27450980392156865, train_loss: 1.7487941613564124, val_loss: 1.7382412845013189 (9 / 80)\n",
            "train_acc: 0.1901098901098901, val_acc: 0.3137254901960784, train_loss: 1.744955062866211, val_loss: 1.7333561462514542 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1901098901098901\n",
            "lr 0.00013957251474088612, batch 10, decay 4.205625127268475e-06, gamma 0.01711142737927632, val accuracy 0.3137254901960784, val loss 1.7333561462514542 [36 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0003509552676831811, 'batch_size': 9, 'weight_decay': 0.00019778700307357554, 'gamma': 0.045466329750095374}\n",
            "train_acc: 0.17912087912087912, val_acc: 0.18627450980392157, train_loss: 1.7886268893441002, val_loss: 1.7818012377795052 (1 / 80)\n",
            "train_acc: 0.1912087912087912, val_acc: 0.18627450980392157, train_loss: 1.779180090374999, val_loss: 1.7695170711068546 (2 / 80)\n",
            "train_acc: 0.189010989010989, val_acc: 0.18627450980392157, train_loss: 1.769641150354029, val_loss: 1.756446336998659 (3 / 80)\n",
            "train_acc: 0.17692307692307693, val_acc: 0.18627450980392157, train_loss: 1.7613691570994618, val_loss: 1.745295847163481 (4 / 80)\n",
            "train_acc: 0.2010989010989011, val_acc: 0.18627450980392157, train_loss: 1.7609397141488043, val_loss: 1.740047037601471 (5 / 80)\n",
            "train_acc: 0.22857142857142856, val_acc: 0.20588235294117646, train_loss: 1.7460737275553273, val_loss: 1.7256153681698967 (6 / 80)\n",
            "train_acc: 0.22747252747252747, val_acc: 0.2647058823529412, train_loss: 1.732411789894104, val_loss: 1.704888824154349 (7 / 80)\n",
            "train_acc: 0.29010989010989013, val_acc: 0.29411764705882354, train_loss: 1.7046424500234834, val_loss: 1.6610537732348722 (8 / 80)\n",
            "train_acc: 0.3252747252747253, val_acc: 0.29411764705882354, train_loss: 1.6544769342129046, val_loss: 1.5733165916274576 (9 / 80)\n",
            "train_acc: 0.3032967032967033, val_acc: 0.29411764705882354, train_loss: 1.613605114391872, val_loss: 1.5478848499410294 (10 / 80)\n",
            "train_acc: 0.34065934065934067, val_acc: 0.30392156862745096, train_loss: 1.5770549440121913, val_loss: 1.611763529917773 (11 / 80)\n",
            "train_acc: 0.3362637362637363, val_acc: 0.30392156862745096, train_loss: 1.5535294089998517, val_loss: 1.5035519003868103 (12 / 80)\n",
            "train_acc: 0.3373626373626374, val_acc: 0.3333333333333333, train_loss: 1.5323965312360408, val_loss: 1.5124059670111711 (13 / 80)\n",
            "train_acc: 0.33296703296703295, val_acc: 0.29411764705882354, train_loss: 1.5305573261046148, val_loss: 1.4699817440089058 (14 / 80)\n",
            "train_acc: 0.3802197802197802, val_acc: 0.43137254901960786, train_loss: 1.4816470043344812, val_loss: 1.4433852697119993 (15 / 80)\n",
            "train_acc: 0.34615384615384615, val_acc: 0.37254901960784315, train_loss: 1.5124794360045548, val_loss: 1.4109715328497046 (16 / 80)\n",
            "train_acc: 0.37142857142857144, val_acc: 0.3333333333333333, train_loss: 1.4620182394981385, val_loss: 1.5115718841552734 (17 / 80)\n",
            "train_acc: 0.367032967032967, val_acc: 0.4215686274509804, train_loss: 1.4432200661072365, val_loss: 1.4572096922818352 (18 / 80)\n",
            "train_acc: 0.3824175824175824, val_acc: 0.4117647058823529, train_loss: 1.4298689093563584, val_loss: 1.3852454564150642 (19 / 80)\n",
            "train_acc: 0.38461538461538464, val_acc: 0.4117647058823529, train_loss: 1.4067317714402965, val_loss: 1.3735432467039894 (20 / 80)\n",
            "train_acc: 0.42857142857142855, val_acc: 0.5098039215686274, train_loss: 1.4096039510035252, val_loss: 1.3110060025663937 (21 / 80)\n",
            "train_acc: 0.3802197802197802, val_acc: 0.4803921568627451, train_loss: 1.4146066088597853, val_loss: 1.3167810475125032 (22 / 80)\n",
            "train_acc: 0.4043956043956044, val_acc: 0.45098039215686275, train_loss: 1.4015038415625856, val_loss: 1.2840001916184145 (23 / 80)\n",
            "train_acc: 0.3978021978021978, val_acc: 0.5, train_loss: 1.3896561168052337, val_loss: 1.2653655041666592 (24 / 80)\n",
            "train_acc: 0.4065934065934066, val_acc: 0.45098039215686275, train_loss: 1.3552911796412626, val_loss: 1.2304849607103012 (25 / 80)\n",
            "train_acc: 0.43846153846153846, val_acc: 0.43137254901960786, train_loss: 1.3260019257173434, val_loss: 1.2741247047396267 (26 / 80)\n",
            "train_acc: 0.4582417582417582, val_acc: 0.46078431372549017, train_loss: 1.3180817044698274, val_loss: 1.2879245666896595 (27 / 80)\n",
            "train_acc: 0.4142857142857143, val_acc: 0.5392156862745098, train_loss: 1.3572028687367073, val_loss: 1.207540296456393 (28 / 80)\n",
            "train_acc: 0.44835164835164837, val_acc: 0.5, train_loss: 1.302966925076076, val_loss: 1.2622966082657086 (29 / 80)\n",
            "train_acc: 0.4230769230769231, val_acc: 0.5196078431372549, train_loss: 1.30314230630686, val_loss: 1.1732070761568405 (30 / 80)\n",
            "train_acc: 0.4747252747252747, val_acc: 0.5392156862745098, train_loss: 1.2796999635277213, val_loss: 1.1849315587212057 (31 / 80)\n",
            "train_acc: 0.46263736263736266, val_acc: 0.5588235294117647, train_loss: 1.2897792704812774, val_loss: 1.1831512959564434 (32 / 80)\n",
            "train_acc: 0.4747252747252747, val_acc: 0.5392156862745098, train_loss: 1.2846577269690378, val_loss: 1.187642539248747 (33 / 80)\n",
            "train_acc: 0.4582417582417582, val_acc: 0.5588235294117647, train_loss: 1.2663902174640487, val_loss: 1.1308135758428013 (34 / 80)\n",
            "train_acc: 0.47912087912087914, val_acc: 0.5294117647058824, train_loss: 1.2663029762414786, val_loss: 1.1674755916875952 (35 / 80)\n",
            "train_acc: 0.46923076923076923, val_acc: 0.5686274509803921, train_loss: 1.2457399298856546, val_loss: 1.1377367272096521 (36 / 80)\n",
            "train_acc: 0.5131868131868131, val_acc: 0.5, train_loss: 1.207046083369098, val_loss: 1.1214100578252006 (37 / 80)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8699ce4285f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mcurrent_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHT_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTEP_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mval_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e187b98eb2aa>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(net, parameters_to_optimize, learning_rate, num_epochs, batch_size, weight_decay, step_size, gamma, train_dataset, val_dataset, verbosity, plot)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0msum_train_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxj7-SlSKb_3"
      },
      "source": [
        "**Grid search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aPmSObkPKbu3",
        "outputId": "9e0bc70f-78da-4995-b35e-f46070fab2b8",
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NUM_CLASSES = 6\n",
        "DEVICE = 'cuda'\n",
        "#BATCH_SIZE = 16\n",
        "#LR = 0.001\n",
        "MOMENTUM = 0.9\n",
        "#WEIGHT_DECAY = 5e-5\n",
        "NUM_EPOCHS = 100\n",
        "STEP_SIZE = 60\n",
        "#GAMMA = 0.1\n",
        "'''\n",
        "lr_range = [0.005, 0.001, 0.0005]\n",
        "batch_size_range = [16, 8]\n",
        "weight_decay_range = [5e-5, 5e-3]\n",
        "gamma_range = [0.1, 0.01]\n",
        "hyperparameters_sets = []\n",
        "\n",
        "for lr in lr_range:\n",
        "  for batch_size in batch_size_range:\n",
        "    for weight_decay in weight_decay_range:\n",
        "      for gamma in gamma_range:\n",
        "        hyperparameters_sets.append({'lr': lr, 'batch_size': batch_size, 'weight_decay': weight_decay, 'gamma': gamma})\n",
        "'''\n",
        "\n",
        "# lr 0.0006444500508054211, batch 14, decay 2.1280582227123365e-05, gamma 0.19924404264743992, val accuracy 0.6305418719211823, val loss 1.0403618915327664 [5 / 50]\n",
        "# lr 0.00038041059192815333, batch 9, decay 3.8372561126798785e-05, gamma 0.057680309789029396, val accuracy 0.6108374384236454, val loss 0.9877617955207825 [38 / 50]\n",
        "# lr 0.00043660847130590896, batch 10, decay 0.00025031720443271155, gamma 0.011678955740792939, val accuracy 0.5615763546798029, val loss 1.0453474251507537 [43 / 50]\n",
        "# lr 0.00027531434783290124, batch 9, decay 4.3783604017624755e-06, gamma 0.11844128056704877, val accuracy 0.5517241379310345, val loss 1.117455956383879 [44 / 50]\n",
        "# lr 0.0007220498435995008, batch 14, decay 2.228552014354877e-05, gamma 0.08113961287843949, val accuracy 0.625615763546798, val loss 0.9968108699239534 [49 / 50]\n",
        "# lr 0.0008377019231346562, batch 8, decay 2.4427015675775187e-06, gamma 0.00903130010323455, val accuracy 0.5849802371541502, val loss 1.0701400147596367 [1 / 50]\n",
        "# lr 0.0010316163585472981, batch 8, decay 1.8309942558988887e-05, gamma 0.002673690056313373, val accuracy 0.5592885375494071, val loss 1.0480610431418589 [5 / 50]\n",
        "# lr 0.0016661746592012004, batch 8, decay 3.3763075569909223e-06, gamma 0.006052773438030023, val accuracy 0.6067193675889329, val loss 1.0441360360548901 [6 / 50]\n",
        "\n",
        "hyperparameters_sets = []\n",
        "lr_list = [0.0006444500508054211, 0.00038041059192815333, 0.00043660847130590896, 0.00027531434783290124, 0.0007220498435995008, 0.0008377019231346562, 0.0010316163585472981, 0.0016661746592012004]\n",
        "bs_list = [14, 9, 10, 9, 14, 8, 8, 8]\n",
        "wd_list = [2.1280582227123365e-05, 3.8372561126798785e-05, 0.00025031720443271155, 4.3783604017624755e-06, 2.228552014354877e-05, 2.4427015675775187e-06, 1.8309942558988887e-05, 3.3763075569909223e-06]\n",
        "g_list = [0.19924404264743992, 0.057680309789029396, 0.011678955740792939, 0.11844128056704877, 0.08113961287843949, 0.00903130010323455, 0.002673690056313373, 0.006052773438030023]\n",
        "\n",
        "for i in range(8):\n",
        "  set = {\"lr\": lr_list[i], \"batch_size\": bs_list[i], \"weight_decay\": wd_list[i], \"gamma\": g_list[i]}\n",
        "  hyperparameters_sets.append(set)\n",
        "\n",
        "for set in hyperparameters_sets:\n",
        "  print(set)\n",
        "\n",
        "\n",
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec-672'\n",
        "compose=[transforms.Resize(224),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.RandomGrayscale(),\n",
        "         transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "         transforms.ToTensor()\n",
        "         ]\n",
        "train_dataset, val_dataset = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, compose)\n",
        "\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 5]\n",
        "val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 5]\n",
        "val_dataset = Subset(val_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))\n",
        "\n",
        "best_net = vgg11()\n",
        "best_net = best_net.to(DEVICE)\n",
        "best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "best_set = {}\n",
        "best_accuracy = 0.0\n",
        "best_loss = 0.0\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "n = 0\n",
        "for set in hyperparameters_sets:\n",
        "\n",
        "  net = vgg11()\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "  current_net, val_accuracy, val_loss = train_network(net, net.parameters(), set['lr'], NUM_EPOCHS, set['batch_size'], set['weight_decay'], STEP_SIZE, set['gamma'], train_dataset, val_dataset=val_dataset, verbosity=True)\n",
        "  val_accuracies.append(val_accuracy)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  if val_accuracy > best_accuracy:\n",
        "    best_accuracy = val_accuracy\n",
        "    best_loss = val_loss\n",
        "    best_net = copy.deepcopy(current_net)\n",
        "    best_set = copy.deepcopy(set)\n",
        "  n += 1\n",
        "  print(\"({}), val accuracy {}, val loss {} [{} / {}]\".format(set, val_accuracy, val_loss, n, len(hyperparameters_sets)))\n",
        "\n",
        "print(\"\\n\\n({}), best val accuracy {}, best val loss {}\\n\".format(best_set, best_accuracy, best_loss))\n",
        "print(\"\\nval_accuracies\")\n",
        "print(val_accuracies)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lr': 0.0006444500508054211, 'batch_size': 14, 'weight_decay': 2.1280582227123365e-05, 'gamma': 0.19924404264743992}\n",
            "{'lr': 0.00038041059192815333, 'batch_size': 9, 'weight_decay': 3.8372561126798785e-05, 'gamma': 0.057680309789029396}\n",
            "{'lr': 0.00043660847130590896, 'batch_size': 10, 'weight_decay': 0.00025031720443271155, 'gamma': 0.011678955740792939}\n",
            "{'lr': 0.00027531434783290124, 'batch_size': 9, 'weight_decay': 4.3783604017624755e-06, 'gamma': 0.11844128056704877}\n",
            "{'lr': 0.0007220498435995008, 'batch_size': 14, 'weight_decay': 2.228552014354877e-05, 'gamma': 0.08113961287843949}\n",
            "{'lr': 0.0008377019231346562, 'batch_size': 8, 'weight_decay': 2.4427015675775187e-06, 'gamma': 0.00903130010323455}\n",
            "{'lr': 0.0010316163585472981, 'batch_size': 8, 'weight_decay': 1.8309942558988887e-05, 'gamma': 0.002673690056313373}\n",
            "{'lr': 0.0016661746592012004, 'batch_size': 8, 'weight_decay': 3.3763075569909223e-06, 'gamma': 0.006052773438030023}\n",
            "training set 809\n",
            "validation set 203\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7862234551768073, val_loss: 1.7795154925050407 (1 / 100)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7753021369316375, val_loss: 1.7675633101627743 (2 / 100)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.768649269535456, val_loss: 1.7580222138043107 (3 / 100)\n",
            "train_acc: 0.16934487021013597, val_acc: 0.18226600985221675, train_loss: 1.7633179278839357, val_loss: 1.7531074285507202 (4 / 100)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7577222226134632, val_loss: 1.7474955772531444 (5 / 100)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7545714600861293, val_loss: 1.7431811916417088 (6 / 100)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.19704433497536947, train_loss: 1.7494169974651266, val_loss: 1.7369723854393795 (7 / 100)\n",
            "train_acc: 0.22126081582200247, val_acc: 0.23645320197044334, train_loss: 1.7439721078601549, val_loss: 1.7263395621858795 (8 / 100)\n",
            "train_acc: 0.27935723114956734, val_acc: 0.20689655172413793, train_loss: 1.7341709987194782, val_loss: 1.7090324738930012 (9 / 100)\n",
            "train_acc: 0.28059332509270707, val_acc: 0.21674876847290642, train_loss: 1.7224935019119414, val_loss: 1.6904847334171165 (10 / 100)\n",
            "train_acc: 0.2669962917181706, val_acc: 0.32019704433497537, train_loss: 1.6945288285926188, val_loss: 1.6300613962370774 (11 / 100)\n",
            "train_acc: 0.31396786155747836, val_acc: 0.22660098522167488, train_loss: 1.6604212404623608, val_loss: 1.7054822239382514 (12 / 100)\n",
            "train_acc: 0.29171817058096416, val_acc: 0.3251231527093596, train_loss: 1.664362433814295, val_loss: 1.5818871300795982 (13 / 100)\n",
            "train_acc: 0.3127317676143387, val_acc: 0.3497536945812808, train_loss: 1.6212048042837268, val_loss: 1.5631290468676338 (14 / 100)\n",
            "train_acc: 0.29295426452410384, val_acc: 0.33497536945812806, train_loss: 1.6492813846236842, val_loss: 1.5348215432002628 (15 / 100)\n",
            "train_acc: 0.31025957972805934, val_acc: 0.3054187192118227, train_loss: 1.6093457589485443, val_loss: 1.540602499041064 (16 / 100)\n",
            "train_acc: 0.3065512978986403, val_acc: 0.30049261083743845, train_loss: 1.5876217862436885, val_loss: 1.5530940170945793 (17 / 100)\n",
            "train_acc: 0.3535228677379481, val_acc: 0.30049261083743845, train_loss: 1.5626992576643917, val_loss: 1.5631563622376015 (18 / 100)\n",
            "train_acc: 0.3288009888751545, val_acc: 0.29064039408866993, train_loss: 1.5723912281217918, val_loss: 1.5165082504009377 (19 / 100)\n",
            "train_acc: 0.3362175525339926, val_acc: 0.3251231527093596, train_loss: 1.5562335557784375, val_loss: 1.530579188774372 (20 / 100)\n",
            "train_acc: 0.30778739184178, val_acc: 0.3448275862068966, train_loss: 1.5792912109231183, val_loss: 1.5303333422233318 (21 / 100)\n",
            "train_acc: 0.3362175525339926, val_acc: 0.270935960591133, train_loss: 1.555518690381563, val_loss: 1.576153775741314 (22 / 100)\n",
            "train_acc: 0.33127317676143386, val_acc: 0.33004926108374383, train_loss: 1.547411620395882, val_loss: 1.5221815890279309 (23 / 100)\n",
            "train_acc: 0.33868974042027195, val_acc: 0.3251231527093596, train_loss: 1.5472987511544056, val_loss: 1.6201243770533595 (24 / 100)\n",
            "train_acc: 0.3300370828182942, val_acc: 0.35960591133004927, train_loss: 1.5440704651609782, val_loss: 1.4659623113171807 (25 / 100)\n",
            "train_acc: 0.3658838071693449, val_acc: 0.32019704433497537, train_loss: 1.51856546761521, val_loss: 1.47112906390223 (26 / 100)\n",
            "train_acc: 0.3411619283065513, val_acc: 0.2955665024630542, train_loss: 1.5225526539445366, val_loss: 1.461863505429235 (27 / 100)\n",
            "train_acc: 0.3411619283065513, val_acc: 0.37438423645320196, train_loss: 1.5201985763825652, val_loss: 1.442565367139619 (28 / 100)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.3793103448275862, train_loss: 1.4560831736575246, val_loss: 1.4206522250997609 (29 / 100)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.3497536945812808, train_loss: 1.4926876077251174, val_loss: 1.4680830527996194 (30 / 100)\n",
            "train_acc: 0.3856613102595797, val_acc: 0.31527093596059114, train_loss: 1.4522850864160488, val_loss: 1.4799481671431969 (31 / 100)\n",
            "train_acc: 0.34857849196538937, val_acc: 0.3842364532019704, train_loss: 1.50289146596628, val_loss: 1.407558223296856 (32 / 100)\n",
            "train_acc: 0.3658838071693449, val_acc: 0.33004926108374383, train_loss: 1.4834878107231244, val_loss: 1.455953704899755 (33 / 100)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.39901477832512317, train_loss: 1.4111788854905496, val_loss: 1.4260368963767742 (34 / 100)\n",
            "train_acc: 0.39060568603213847, val_acc: 0.4236453201970443, train_loss: 1.4313970149963247, val_loss: 1.3792402785399864 (35 / 100)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.39408866995073893, train_loss: 1.4241694637812554, val_loss: 1.3863251784752155 (36 / 100)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.4039408866995074, train_loss: 1.4210392154191422, val_loss: 1.3951835303471005 (37 / 100)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.4236453201970443, train_loss: 1.4252224714116497, val_loss: 1.3814587305332053 (38 / 100)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.3793103448275862, train_loss: 1.40379174486521, val_loss: 1.3931798195016796 (39 / 100)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.3891625615763547, train_loss: 1.3641783570034984, val_loss: 1.3754376954045788 (40 / 100)\n",
            "train_acc: 0.4215080346106304, val_acc: 0.4187192118226601, train_loss: 1.3725579738322236, val_loss: 1.4163036058688987 (41 / 100)\n",
            "train_acc: 0.4363411619283066, val_acc: 0.4088669950738916, train_loss: 1.3736871097821683, val_loss: 1.3294315297028114 (42 / 100)\n",
            "train_acc: 0.446229913473424, val_acc: 0.4236453201970443, train_loss: 1.3404310267552162, val_loss: 1.3235571014470067 (43 / 100)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.3793103448275862, train_loss: 1.3022197987446826, val_loss: 1.4262169352893173 (44 / 100)\n",
            "train_acc: 0.446229913473424, val_acc: 0.39901477832512317, train_loss: 1.332316825652152, val_loss: 1.3583365555467277 (45 / 100)\n",
            "train_acc: 0.45982694684796044, val_acc: 0.37438423645320196, train_loss: 1.3090097946937949, val_loss: 1.3199768271939507 (46 / 100)\n",
            "train_acc: 0.4227441285537701, val_acc: 0.39901477832512317, train_loss: 1.3093961696836947, val_loss: 1.3110748570540856 (47 / 100)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.3891625615763547, train_loss: 1.2977483353597126, val_loss: 1.3122564677534432 (48 / 100)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.45320197044334976, train_loss: 1.2476803584505514, val_loss: 1.3005824171263596 (49 / 100)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.4433497536945813, train_loss: 1.2288342621476747, val_loss: 1.314315910997062 (50 / 100)\n",
            "train_acc: 0.484548825710754, val_acc: 0.47783251231527096, train_loss: 1.2414617842001143, val_loss: 1.3215967129016746 (51 / 100)\n",
            "train_acc: 0.48825710754017304, val_acc: 0.46798029556650245, train_loss: 1.2413510386227677, val_loss: 1.2714984170321761 (52 / 100)\n",
            "train_acc: 0.5166872682323856, val_acc: 0.45320197044334976, train_loss: 1.17698467175657, val_loss: 1.4073651905717521 (53 / 100)\n",
            "train_acc: 0.5018541409147095, val_acc: 0.4876847290640394, train_loss: 1.1763107244694335, val_loss: 1.2603593900285919 (54 / 100)\n",
            "train_acc: 0.5105067985166872, val_acc: 0.47783251231527096, train_loss: 1.1968526212482429, val_loss: 1.28347895885336 (55 / 100)\n",
            "train_acc: 0.519159456118665, val_acc: 0.4630541871921182, train_loss: 1.1914512148893661, val_loss: 1.4141594541483913 (56 / 100)\n",
            "train_acc: 0.5364647713226205, val_acc: 0.4433497536945813, train_loss: 1.1451300308937196, val_loss: 1.339782871049026 (57 / 100)\n",
            "train_acc: 0.5352286773794809, val_acc: 0.43349753694581283, train_loss: 1.1557640650658436, val_loss: 1.294190447905968 (58 / 100)\n",
            "train_acc: 0.5871446229913473, val_acc: 0.4236453201970443, train_loss: 1.0972717345423988, val_loss: 1.3151261436528172 (59 / 100)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.4482758620689655, train_loss: 1.1001234803123143, val_loss: 1.2991117896704838 (60 / 100)\n",
            "train_acc: 0.6279357231149567, val_acc: 0.5221674876847291, train_loss: 1.0270199797798882, val_loss: 1.3104394715407799 (61 / 100)\n",
            "train_acc: 0.61557478368356, val_acc: 0.5123152709359606, train_loss: 0.9903782267210952, val_loss: 1.2803454892388706 (62 / 100)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.4975369458128079, train_loss: 0.9618690145030452, val_loss: 1.301558523342527 (63 / 100)\n",
            "train_acc: 0.6415327564894932, val_acc: 0.5024630541871922, train_loss: 0.9400523800785079, val_loss: 1.2633350882036933 (64 / 100)\n",
            "train_acc: 0.6551297898640297, val_acc: 0.4827586206896552, train_loss: 0.9088630471447637, val_loss: 1.3166613332156478 (65 / 100)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.5024630541871922, train_loss: 0.9209719165441279, val_loss: 1.31769178242519 (66 / 100)\n",
            "train_acc: 0.688504326328801, val_acc: 0.5123152709359606, train_loss: 0.8907585224350834, val_loss: 1.335598871625703 (67 / 100)\n",
            "train_acc: 0.6773794808405439, val_acc: 0.4975369458128079, train_loss: 0.8766973966868463, val_loss: 1.4251203495880653 (68 / 100)\n",
            "train_acc: 0.6526576019777504, val_acc: 0.5320197044334976, train_loss: 0.887015393844199, val_loss: 1.3442578932334637 (69 / 100)\n",
            "train_acc: 0.6699629171817059, val_acc: 0.5024630541871922, train_loss: 0.900962598170574, val_loss: 1.3290299608789642 (70 / 100)\n",
            "train_acc: 0.6872682323856613, val_acc: 0.5024630541871922, train_loss: 0.851704263274543, val_loss: 1.3531044865476674 (71 / 100)\n",
            "train_acc: 0.7045735475896168, val_acc: 0.5221674876847291, train_loss: 0.8169086505515024, val_loss: 1.3565081892342403 (72 / 100)\n",
            "train_acc: 0.6699629171817059, val_acc: 0.5172413793103449, train_loss: 0.8447445693652621, val_loss: 1.2958588476838737 (73 / 100)\n",
            "train_acc: 0.6897404202719407, val_acc: 0.5221674876847291, train_loss: 0.8272735220834853, val_loss: 1.3256680184397205 (74 / 100)\n",
            "train_acc: 0.7058096415327565, val_acc: 0.5172413793103449, train_loss: 0.7770020074691112, val_loss: 1.4711154000512485 (75 / 100)\n",
            "train_acc: 0.7033374536464772, val_acc: 0.49261083743842365, train_loss: 0.7928200960896071, val_loss: 1.3512210393774098 (76 / 100)\n",
            "train_acc: 0.7082818294190358, val_acc: 0.5320197044334976, train_loss: 0.7738178038626577, val_loss: 1.4033225384251824 (77 / 100)\n",
            "train_acc: 0.7379480840543882, val_acc: 0.5172413793103449, train_loss: 0.7259849243612019, val_loss: 1.3890230902310075 (78 / 100)\n",
            "train_acc: 0.7453646477132262, val_acc: 0.5024630541871922, train_loss: 0.7119516286036582, val_loss: 1.4068726301193237 (79 / 100)\n",
            "train_acc: 0.7292954264524104, val_acc: 0.5024630541871922, train_loss: 0.7151297702924873, val_loss: 1.3775327452297867 (80 / 100)\n",
            "train_acc: 0.7330037082818294, val_acc: 0.5073891625615764, train_loss: 0.7012386506950605, val_loss: 1.4823426748144215 (81 / 100)\n",
            "train_acc: 0.7503090234857849, val_acc: 0.5566502463054187, train_loss: 0.7153541706548485, val_loss: 1.3541624217197812 (82 / 100)\n",
            "overfit -> train_accuracy 0.757725587144623, val_accuracy 0.4876847290640394\n",
            "({'lr': 0.0006444500508054211, 'batch_size': 14, 'weight_decay': 2.1280582227123365e-05, 'gamma': 0.19924404264743992}), val accuracy 0.5566502463054187, val loss 1.3541624217197812 [1 / 8]\n",
            "train_acc: 0.15327564894932014, val_acc: 0.18226600985221675, train_loss: 1.7889878251791296, val_loss: 1.780834421148441 (1 / 100)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7787700946015688, val_loss: 1.767925506154892 (2 / 100)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.766851466429985, val_loss: 1.7583666164886775 (3 / 100)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7626344765661968, val_loss: 1.7542896887351727 (4 / 100)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.19704433497536947, train_loss: 1.7569039016777859, val_loss: 1.7488717745090354 (5 / 100)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.18226600985221675, train_loss: 1.7582658850513075, val_loss: 1.7433960508243205 (6 / 100)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.18719211822660098, train_loss: 1.7558194499080644, val_loss: 1.7345495687916948 (7 / 100)\n",
            "train_acc: 0.24969097651421507, val_acc: 0.19704433497536947, train_loss: 1.7470682118230756, val_loss: 1.7249257541055163 (8 / 100)\n",
            "train_acc: 0.2360939431396786, val_acc: 0.2315270935960591, train_loss: 1.7422814818630996, val_loss: 1.7120840531851858 (9 / 100)\n",
            "train_acc: 0.24721878862793573, val_acc: 0.24630541871921183, train_loss: 1.7136114028239868, val_loss: 1.6773910369779088 (10 / 100)\n",
            "train_acc: 0.2830655129789864, val_acc: 0.29064039408866993, train_loss: 1.687194741405869, val_loss: 1.6494636600240697 (11 / 100)\n",
            "train_acc: 0.26452410383189123, val_acc: 0.3251231527093596, train_loss: 1.6794057810409697, val_loss: 1.5735995710776944 (12 / 100)\n",
            "train_acc: 0.30778739184178, val_acc: 0.31527093596059114, train_loss: 1.6592737582174897, val_loss: 1.5892485879324927 (13 / 100)\n",
            "train_acc: 0.28182941903584674, val_acc: 0.31527093596059114, train_loss: 1.6510339472880322, val_loss: 1.595882510904021 (14 / 100)\n",
            "train_acc: 0.3226205191594561, val_acc: 0.3645320197044335, train_loss: 1.6152950893668516, val_loss: 1.524122645702268 (15 / 100)\n",
            "train_acc: 0.311495673671199, val_acc: 0.29064039408866993, train_loss: 1.59112913190066, val_loss: 1.5628099136164624 (16 / 100)\n",
            "train_acc: 0.3053152039555006, val_acc: 0.31527093596059114, train_loss: 1.582060621901702, val_loss: 1.5375669560408944 (17 / 100)\n",
            "train_acc: 0.31396786155747836, val_acc: 0.33497536945812806, train_loss: 1.5921327366375069, val_loss: 1.5337167819732516 (18 / 100)\n",
            "train_acc: 0.30778739184178, val_acc: 0.33497536945812806, train_loss: 1.5987103708889634, val_loss: 1.512908300742727 (19 / 100)\n",
            "train_acc: 0.3300370828182942, val_acc: 0.3448275862068966, train_loss: 1.5569926908490683, val_loss: 1.4991680060701418 (20 / 100)\n",
            "train_acc: 0.3658838071693449, val_acc: 0.27586206896551724, train_loss: 1.541245191000006, val_loss: 1.54535508919232 (21 / 100)\n",
            "train_acc: 0.3238566131025958, val_acc: 0.3497536945812808, train_loss: 1.5646132862906816, val_loss: 1.4707113463303139 (22 / 100)\n",
            "train_acc: 0.3658838071693449, val_acc: 0.3399014778325123, train_loss: 1.5019470802491026, val_loss: 1.4564788899398202 (23 / 100)\n",
            "train_acc: 0.33498145859085293, val_acc: 0.35960591133004927, train_loss: 1.5180306138450048, val_loss: 1.483212680652224 (24 / 100)\n",
            "train_acc: 0.36093943139678614, val_acc: 0.32019704433497537, train_loss: 1.5053371335431582, val_loss: 1.4568400347761332 (25 / 100)\n",
            "train_acc: 0.36093943139678614, val_acc: 0.3103448275862069, train_loss: 1.5004304323267141, val_loss: 1.5609304129783743 (26 / 100)\n",
            "train_acc: 0.33868974042027195, val_acc: 0.31527093596059114, train_loss: 1.515598572523544, val_loss: 1.4635147343715424 (27 / 100)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.41379310344827586, train_loss: 1.4692571552191735, val_loss: 1.422322514609163 (28 / 100)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.2955665024630542, train_loss: 1.462878286175439, val_loss: 1.5144307660351832 (29 / 100)\n",
            "train_acc: 0.36093943139678614, val_acc: 0.4039408866995074, train_loss: 1.4757583848625533, val_loss: 1.4160100204016774 (30 / 100)\n",
            "train_acc: 0.3658838071693449, val_acc: 0.3793103448275862, train_loss: 1.4683783014566258, val_loss: 1.411042264529637 (31 / 100)\n",
            "train_acc: 0.4042027194066749, val_acc: 0.3842364532019704, train_loss: 1.4086768678740607, val_loss: 1.4166151120744903 (32 / 100)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.4088669950738916, train_loss: 1.4609956900475198, val_loss: 1.383501643617752 (33 / 100)\n",
            "train_acc: 0.3695920889987639, val_acc: 0.4236453201970443, train_loss: 1.4360395553527567, val_loss: 1.380808062154084 (34 / 100)\n",
            "train_acc: 0.40667490729295425, val_acc: 0.33004926108374383, train_loss: 1.4228118525447304, val_loss: 1.4781385102295523 (35 / 100)\n",
            "train_acc: 0.3967861557478368, val_acc: 0.39901477832512317, train_loss: 1.397171830659449, val_loss: 1.3918997889081832 (36 / 100)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.37438423645320196, train_loss: 1.4332588726864168, val_loss: 1.413560742227902 (37 / 100)\n",
            "train_acc: 0.41285537700865266, val_acc: 0.45320197044334976, train_loss: 1.4039469486556035, val_loss: 1.3558927104978138 (38 / 100)\n",
            "train_acc: 0.411619283065513, val_acc: 0.43842364532019706, train_loss: 1.3664261319286597, val_loss: 1.3624212057719678 (39 / 100)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.4433497536945813, train_loss: 1.38170673791204, val_loss: 1.3348620067089063 (40 / 100)\n",
            "train_acc: 0.4215080346106304, val_acc: 0.3793103448275862, train_loss: 1.3484324561650733, val_loss: 1.3998724909251548 (41 / 100)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.3694581280788177, train_loss: 1.3629197676485343, val_loss: 1.419548515615792 (42 / 100)\n",
            "train_acc: 0.41903584672435107, val_acc: 0.4088669950738916, train_loss: 1.332977025485304, val_loss: 1.3360376604672135 (43 / 100)\n",
            "train_acc: 0.45488257107540175, val_acc: 0.42857142857142855, train_loss: 1.345008532771074, val_loss: 1.3146831243496222 (44 / 100)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.43842364532019706, train_loss: 1.3013360810515613, val_loss: 1.3233833406946343 (45 / 100)\n",
            "train_acc: 0.44870210135970334, val_acc: 0.458128078817734, train_loss: 1.2924598683827593, val_loss: 1.329693503567738 (46 / 100)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.4088669950738916, train_loss: 1.2870029488069608, val_loss: 1.3063922506835073 (47 / 100)\n",
            "train_acc: 0.47342398022249693, val_acc: 0.45320197044334976, train_loss: 1.2536369414795168, val_loss: 1.2808215439025992 (48 / 100)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.45320197044334976, train_loss: 1.2748750697991758, val_loss: 1.2701200176342367 (49 / 100)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.4236453201970443, train_loss: 1.2470222869968532, val_loss: 1.3019364440969645 (50 / 100)\n",
            "train_acc: 0.49814585908529047, val_acc: 0.42857142857142855, train_loss: 1.226240205823712, val_loss: 1.3588363983361005 (51 / 100)\n",
            "train_acc: 0.49814585908529047, val_acc: 0.4630541871921182, train_loss: 1.1779224858590494, val_loss: 1.290090707135318 (52 / 100)\n",
            "train_acc: 0.5216316440049443, val_acc: 0.43842364532019706, train_loss: 1.190067638204478, val_loss: 1.364774578016967 (53 / 100)\n",
            "train_acc: 0.5067985166872683, val_acc: 0.47783251231527096, train_loss: 1.2070089951433858, val_loss: 1.2450794192957761 (54 / 100)\n",
            "train_acc: 0.5290482076637825, val_acc: 0.37438423645320196, train_loss: 1.2116156236645024, val_loss: 1.4073354181984963 (55 / 100)\n",
            "train_acc: 0.5352286773794809, val_acc: 0.458128078817734, train_loss: 1.1682792450353152, val_loss: 1.3261996537006546 (56 / 100)\n",
            "train_acc: 0.553770086526576, val_acc: 0.4630541871921182, train_loss: 1.1303936599653643, val_loss: 1.370417629850322 (57 / 100)\n",
            "train_acc: 0.5203955500618047, val_acc: 0.4482758620689655, train_loss: 1.1517118596470695, val_loss: 1.2729347257191324 (58 / 100)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.4729064039408867, train_loss: 1.0985234656793668, val_loss: 1.2536036715718912 (59 / 100)\n",
            "train_acc: 0.5636588380716935, val_acc: 0.49261083743842365, train_loss: 1.0998073982072554, val_loss: 1.3473975629054855 (60 / 100)\n",
            "train_acc: 0.61557478368356, val_acc: 0.4729064039408867, train_loss: 1.0068683787093617, val_loss: 1.243010787247437 (61 / 100)\n",
            "train_acc: 0.6069221260815822, val_acc: 0.4729064039408867, train_loss: 0.9943776499797741, val_loss: 1.2338687337090817 (62 / 100)\n",
            "train_acc: 0.6180469715698393, val_acc: 0.47783251231527096, train_loss: 0.9939430662153383, val_loss: 1.2629986514011626 (63 / 100)\n",
            "train_acc: 0.61557478368356, val_acc: 0.4729064039408867, train_loss: 1.0061729959268653, val_loss: 1.2453934556157717 (64 / 100)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.4729064039408867, train_loss: 0.9753502871845797, val_loss: 1.2519497210756312 (65 / 100)\n",
            "train_acc: 0.6588380716934487, val_acc: 0.4729064039408867, train_loss: 0.9196342280680229, val_loss: 1.2578404222803163 (66 / 100)\n",
            "train_acc: 0.6254635352286774, val_acc: 0.47783251231527096, train_loss: 0.960038479864229, val_loss: 1.2639153265013483 (67 / 100)\n",
            "train_acc: 0.6551297898640297, val_acc: 0.4729064039408867, train_loss: 0.9307206968589236, val_loss: 1.2886274562680662 (68 / 100)\n",
            "train_acc: 0.6353522867737948, val_acc: 0.4729064039408867, train_loss: 0.9296316610573249, val_loss: 1.2948866037312399 (69 / 100)\n",
            "train_acc: 0.6650185414091471, val_acc: 0.4729064039408867, train_loss: 0.9121697356070223, val_loss: 1.2938044720095367 (70 / 100)\n",
            "train_acc: 0.6390605686032138, val_acc: 0.4827586206896552, train_loss: 0.9233152567971917, val_loss: 1.2827747477392846 (71 / 100)\n",
            "train_acc: 0.6452410383189122, val_acc: 0.4827586206896552, train_loss: 0.9408757788657552, val_loss: 1.2634942152817261 (72 / 100)\n",
            "train_acc: 0.6724351050679852, val_acc: 0.4876847290640394, train_loss: 0.8774960739135153, val_loss: 1.30075593476225 (73 / 100)\n",
            "train_acc: 0.65389369592089, val_acc: 0.4827586206896552, train_loss: 0.8898516917700231, val_loss: 1.2940581383669905 (74 / 100)\n",
            "train_acc: 0.6613102595797281, val_acc: 0.47783251231527096, train_loss: 0.8983023287486677, val_loss: 1.2866618536376013 (75 / 100)\n",
            "train_acc: 0.681087762669963, val_acc: 0.4827586206896552, train_loss: 0.8927202322353391, val_loss: 1.29005816593546 (76 / 100)\n",
            "train_acc: 0.6526576019777504, val_acc: 0.47783251231527096, train_loss: 0.9068045081992084, val_loss: 1.282265129934978 (77 / 100)\n",
            "train_acc: 0.6749072929542645, val_acc: 0.49261083743842365, train_loss: 0.8839522321675115, val_loss: 1.3169158708873054 (78 / 100)\n",
            "train_acc: 0.6662546353522868, val_acc: 0.4975369458128079, train_loss: 0.8725103107459465, val_loss: 1.286271485789069 (79 / 100)\n",
            "train_acc: 0.6613102595797281, val_acc: 0.47783251231527096, train_loss: 0.9053879965957812, val_loss: 1.3110832433982436 (80 / 100)\n",
            "train_acc: 0.6526576019777504, val_acc: 0.4827586206896552, train_loss: 0.8891674679864028, val_loss: 1.2883481406813184 (81 / 100)\n",
            "train_acc: 0.6711990111248455, val_acc: 0.4975369458128079, train_loss: 0.8732005079980244, val_loss: 1.3171813752263637 (82 / 100)\n",
            "train_acc: 0.6798516687268232, val_acc: 0.4975369458128079, train_loss: 0.862403800740967, val_loss: 1.3032033810474601 (83 / 100)\n",
            "train_acc: 0.6736711990111248, val_acc: 0.4876847290640394, train_loss: 0.8778581710990486, val_loss: 1.312554189430669 (84 / 100)\n",
            "train_acc: 0.6773794808405439, val_acc: 0.4876847290640394, train_loss: 0.8747806148638094, val_loss: 1.2787691231431633 (85 / 100)\n",
            "train_acc: 0.6823238566131026, val_acc: 0.47783251231527096, train_loss: 0.8455022319285626, val_loss: 1.306925951847302 (86 / 100)\n",
            "train_acc: 0.6749072929542645, val_acc: 0.4975369458128079, train_loss: 0.8546522300971305, val_loss: 1.3057975642786825 (87 / 100)\n",
            "train_acc: 0.6823238566131026, val_acc: 0.4827586206896552, train_loss: 0.8719989154188535, val_loss: 1.317641092932283 (88 / 100)\n",
            "train_acc: 0.6946847960444994, val_acc: 0.4876847290640394, train_loss: 0.8308012352148888, val_loss: 1.2904629713208804 (89 / 100)\n",
            "train_acc: 0.6872682323856613, val_acc: 0.4975369458128079, train_loss: 0.8574709369166672, val_loss: 1.3029718343260253 (90 / 100)\n",
            "train_acc: 0.7255871446229913, val_acc: 0.5073891625615764, train_loss: 0.8029738367340621, val_loss: 1.309272113985616 (91 / 100)\n",
            "train_acc: 0.7082818294190358, val_acc: 0.4876847290640394, train_loss: 0.8275238235374022, val_loss: 1.287531813083611 (92 / 100)\n",
            "train_acc: 0.69221260815822, val_acc: 0.5024630541871922, train_loss: 0.8112039983051522, val_loss: 1.3361052304065872 (93 / 100)\n",
            "train_acc: 0.7194066749072929, val_acc: 0.4975369458128079, train_loss: 0.7966210054710268, val_loss: 1.3345131881424945 (94 / 100)\n",
            "train_acc: 0.6897404202719407, val_acc: 0.5221674876847291, train_loss: 0.7814892935664456, val_loss: 1.34345588249526 (95 / 100)\n",
            "train_acc: 0.6946847960444994, val_acc: 0.5123152709359606, train_loss: 0.8089941758823631, val_loss: 1.3340887342180525 (96 / 100)\n",
            "train_acc: 0.6996291718170581, val_acc: 0.5024630541871922, train_loss: 0.7956507151073814, val_loss: 1.3276637904162478 (97 / 100)\n",
            "train_acc: 0.695920889987639, val_acc: 0.5073891625615764, train_loss: 0.7843726665260471, val_loss: 1.3253116789709758 (98 / 100)\n",
            "train_acc: 0.7243510506798516, val_acc: 0.4975369458128079, train_loss: 0.8100123051807229, val_loss: 1.3263510574260955 (99 / 100)\n",
            "train_acc: 0.723114956736712, val_acc: 0.5123152709359606, train_loss: 0.7645042988808989, val_loss: 1.343534636086431 (100 / 100)\n",
            "({'lr': 0.00038041059192815333, 'batch_size': 9, 'weight_decay': 3.8372561126798785e-05, 'gamma': 0.057680309789029396}), val accuracy 0.5221674876847291, val loss 1.34345588249526 [2 / 8]\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7889252286611588, val_loss: 1.7847846675976156 (1 / 100)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7811227328108326, val_loss: 1.7765625879682343 (2 / 100)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7690881361035982, val_loss: 1.7658698306295084 (3 / 100)\n",
            "train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7677115287415324, val_loss: 1.75939460634598 (4 / 100)\n",
            "train_acc: 0.16934487021013597, val_acc: 0.18226600985221675, train_loss: 1.7635342532536302, val_loss: 1.7566013406650187 (5 / 100)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.759589360582519, val_loss: 1.752849178948426 (6 / 100)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7517162503948613, val_loss: 1.7474477296979556 (7 / 100)\n",
            "train_acc: 0.22373300370828184, val_acc: 0.19704433497536947, train_loss: 1.752858533258049, val_loss: 1.7431960511090132 (8 / 100)\n",
            "train_acc: 0.23980222496909764, val_acc: 0.19704433497536947, train_loss: 1.7406878714508416, val_loss: 1.7339284425885806 (9 / 100)\n",
            "train_acc: 0.2360939431396786, val_acc: 0.24630541871921183, train_loss: 1.7424869282431301, val_loss: 1.7253259302947321 (10 / 100)\n",
            "train_acc: 0.2484548825710754, val_acc: 0.31527093596059114, train_loss: 1.7319466472408827, val_loss: 1.7053300242118647 (11 / 100)\n",
            "train_acc: 0.28182941903584674, val_acc: 0.33497536945812806, train_loss: 1.7113556922292532, val_loss: 1.6665411976170657 (12 / 100)\n",
            "train_acc: 0.2941903584672435, val_acc: 0.3251231527093596, train_loss: 1.6831602451975178, val_loss: 1.6227554206190438 (13 / 100)\n",
            "train_acc: 0.2843016069221261, val_acc: 0.26108374384236455, train_loss: 1.6612514058503292, val_loss: 1.606940444467103 (14 / 100)\n",
            "train_acc: 0.3176761433868974, val_acc: 0.31527093596059114, train_loss: 1.6444508165894687, val_loss: 1.559741618598036 (15 / 100)\n",
            "train_acc: 0.31396786155747836, val_acc: 0.28078817733990147, train_loss: 1.6321942125468967, val_loss: 1.5608311251466498 (16 / 100)\n",
            "train_acc: 0.2978986402966625, val_acc: 0.27586206896551724, train_loss: 1.6139199609956871, val_loss: 1.5714852322498565 (17 / 100)\n",
            "train_acc: 0.32138442521631644, val_acc: 0.33497536945812806, train_loss: 1.5863491373392207, val_loss: 1.5162320642048501 (18 / 100)\n",
            "train_acc: 0.29295426452410384, val_acc: 0.3251231527093596, train_loss: 1.608746210046398, val_loss: 1.5505559368086566 (19 / 100)\n",
            "train_acc: 0.3300370828182942, val_acc: 0.29064039408866993, train_loss: 1.5612347019912285, val_loss: 1.577541231521832 (20 / 100)\n",
            "train_acc: 0.3065512978986403, val_acc: 0.3645320197044335, train_loss: 1.594206557285653, val_loss: 1.510656773750418 (21 / 100)\n",
            "train_acc: 0.33868974042027195, val_acc: 0.3399014778325123, train_loss: 1.5702912251940468, val_loss: 1.50422975934785 (22 / 100)\n",
            "train_acc: 0.31396786155747836, val_acc: 0.3645320197044335, train_loss: 1.5734494979066225, val_loss: 1.49772797958017 (23 / 100)\n",
            "train_acc: 0.33498145859085293, val_acc: 0.33497536945812806, train_loss: 1.5594383478164673, val_loss: 1.5045603801464211 (24 / 100)\n",
            "train_acc: 0.33868974042027195, val_acc: 0.37438423645320196, train_loss: 1.535032663121359, val_loss: 1.481818139846689 (25 / 100)\n",
            "train_acc: 0.3535228677379481, val_acc: 0.3694581280788177, train_loss: 1.516695396861865, val_loss: 1.4859090714619076 (26 / 100)\n",
            "train_acc: 0.3584672435105068, val_acc: 0.32019704433497537, train_loss: 1.5400164072828917, val_loss: 1.506372771239633 (27 / 100)\n",
            "train_acc: 0.3362175525339926, val_acc: 0.3448275862068966, train_loss: 1.5417137502592486, val_loss: 1.5016371798632768 (28 / 100)\n",
            "train_acc: 0.34487021013597036, val_acc: 0.32019704433497537, train_loss: 1.5330188340692792, val_loss: 1.4716010628075435 (29 / 100)\n",
            "train_acc: 0.3695920889987639, val_acc: 0.3399014778325123, train_loss: 1.483124681544687, val_loss: 1.4878683982811538 (30 / 100)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.3793103448275862, train_loss: 1.491090437979869, val_loss: 1.4329255508084602 (31 / 100)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.3694581280788177, train_loss: 1.4775976897464842, val_loss: 1.4593922287372534 (32 / 100)\n",
            "train_acc: 0.36711990111248455, val_acc: 0.41379310344827586, train_loss: 1.4651019229877127, val_loss: 1.4225289392941103 (33 / 100)\n",
            "train_acc: 0.35599505562422745, val_acc: 0.3645320197044335, train_loss: 1.4533088264712886, val_loss: 1.424196812319638 (34 / 100)\n",
            "train_acc: 0.3967861557478368, val_acc: 0.39408866995073893, train_loss: 1.4541224189682855, val_loss: 1.4072188444325489 (35 / 100)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.39901477832512317, train_loss: 1.432884059082003, val_loss: 1.4218825107724795 (36 / 100)\n",
            "train_acc: 0.3856613102595797, val_acc: 0.35960591133004927, train_loss: 1.4097544002002484, val_loss: 1.4313116173438838 (37 / 100)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.35960591133004927, train_loss: 1.4382734191137723, val_loss: 1.4134899442419042 (38 / 100)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.3793103448275862, train_loss: 1.4271525679471613, val_loss: 1.424003970446845 (39 / 100)\n",
            "train_acc: 0.40173053152039556, val_acc: 0.42857142857142855, train_loss: 1.428450406702841, val_loss: 1.377811147661632 (40 / 100)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.39901477832512317, train_loss: 1.396956944642462, val_loss: 1.3576337339842848 (41 / 100)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.4039408866995074, train_loss: 1.405928736711462, val_loss: 1.3595811175595363 (42 / 100)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.3842364532019704, train_loss: 1.3848136707643053, val_loss: 1.3786097368583303 (43 / 100)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.4236453201970443, train_loss: 1.3700653520885888, val_loss: 1.3499554771507902 (44 / 100)\n",
            "train_acc: 0.446229913473424, val_acc: 0.3891625615763547, train_loss: 1.3517530425664668, val_loss: 1.336894724756626 (45 / 100)\n",
            "train_acc: 0.4326328800988875, val_acc: 0.4236453201970443, train_loss: 1.3219228620434869, val_loss: 1.4158774910889236 (46 / 100)\n",
            "train_acc: 0.4647713226205192, val_acc: 0.4482758620689655, train_loss: 1.3257637654600392, val_loss: 1.3761434349520454 (47 / 100)\n",
            "train_acc: 0.45859085290482077, val_acc: 0.458128078817734, train_loss: 1.3123649755131328, val_loss: 1.361233451683533 (48 / 100)\n",
            "train_acc: 0.47342398022249693, val_acc: 0.4729064039408867, train_loss: 1.282755246710571, val_loss: 1.3314848874002843 (49 / 100)\n",
            "train_acc: 0.4783683559950556, val_acc: 0.458128078817734, train_loss: 1.3032461531524753, val_loss: 1.3281891181551178 (50 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.39901477832512317, train_loss: 1.279197435591218, val_loss: 1.3360355434746578 (51 / 100)\n",
            "train_acc: 0.49443757725587145, val_acc: 0.43842364532019706, train_loss: 1.268572748959875, val_loss: 1.2975852286874368 (52 / 100)\n",
            "train_acc: 0.4969097651421508, val_acc: 0.4482758620689655, train_loss: 1.2343275202219506, val_loss: 1.273424846785409 (53 / 100)\n",
            "train_acc: 0.4919653893695921, val_acc: 0.4827586206896552, train_loss: 1.2333409856954227, val_loss: 1.3075898110572928 (54 / 100)\n",
            "train_acc: 0.5030902348578492, val_acc: 0.4187192118226601, train_loss: 1.224392974774534, val_loss: 1.347468872962914 (55 / 100)\n",
            "train_acc: 0.5006180469715699, val_acc: 0.4630541871921182, train_loss: 1.2102851378460897, val_loss: 1.3108007931356946 (56 / 100)\n",
            "train_acc: 0.5142150803461063, val_acc: 0.4482758620689655, train_loss: 1.1942080714646612, val_loss: 1.321346582450303 (57 / 100)\n",
            "train_acc: 0.5265760197775031, val_acc: 0.43842364532019706, train_loss: 1.1718951855660664, val_loss: 1.2977585786669126 (58 / 100)\n",
            "train_acc: 0.5265760197775031, val_acc: 0.46798029556650245, train_loss: 1.1802341709620283, val_loss: 1.3268870552772372 (59 / 100)\n",
            "train_acc: 0.5735475896168108, val_acc: 0.47783251231527096, train_loss: 1.1199525940256154, val_loss: 1.337830789570738 (60 / 100)\n",
            "train_acc: 0.5661310259579728, val_acc: 0.4827586206896552, train_loss: 1.0656725924890327, val_loss: 1.3202153215267387 (61 / 100)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.49261083743842365, train_loss: 1.0854062668619993, val_loss: 1.3070618918376604 (62 / 100)\n",
            "train_acc: 0.5784919653893696, val_acc: 0.4827586206896552, train_loss: 1.0298078814602016, val_loss: 1.3064727577669868 (63 / 100)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.47783251231527096, train_loss: 1.0715215449280144, val_loss: 1.3041728864162427 (64 / 100)\n",
            "train_acc: 0.6032138442521632, val_acc: 0.47783251231527096, train_loss: 1.0801203915451012, val_loss: 1.2977234709439018 (65 / 100)\n",
            "train_acc: 0.6217552533992583, val_acc: 0.47783251231527096, train_loss: 1.0284365047188417, val_loss: 1.2961420930665115 (66 / 100)\n",
            "train_acc: 0.588380716934487, val_acc: 0.4729064039408867, train_loss: 1.059618371084093, val_loss: 1.295122396769782 (67 / 100)\n",
            "train_acc: 0.5920889987639061, val_acc: 0.47783251231527096, train_loss: 1.0528788184942803, val_loss: 1.2894698977470398 (68 / 100)\n",
            "train_acc: 0.5710754017305315, val_acc: 0.4729064039408867, train_loss: 1.0775019334184814, val_loss: 1.2865256652456198 (69 / 100)\n",
            "train_acc: 0.6044499381953028, val_acc: 0.4729064039408867, train_loss: 1.0326006518600899, val_loss: 1.2862080737875012 (70 / 100)\n",
            "train_acc: 0.6118665018541409, val_acc: 0.47783251231527096, train_loss: 1.0221995206639558, val_loss: 1.2882876302221138 (71 / 100)\n",
            "train_acc: 0.619283065512979, val_acc: 0.47783251231527096, train_loss: 1.033353226146533, val_loss: 1.2861599963286827 (72 / 100)\n",
            "train_acc: 0.6205191594561187, val_acc: 0.4729064039408867, train_loss: 0.9855448056947168, val_loss: 1.2867308473352141 (73 / 100)\n",
            "train_acc: 0.588380716934487, val_acc: 0.46798029556650245, train_loss: 1.0380183433278087, val_loss: 1.2837223504564446 (74 / 100)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.4729064039408867, train_loss: 1.017902004925076, val_loss: 1.2817397740086898 (75 / 100)\n",
            "train_acc: 0.6019777503090235, val_acc: 0.4729064039408867, train_loss: 0.9821261177841016, val_loss: 1.2863217192917622 (76 / 100)\n",
            "train_acc: 0.6019777503090235, val_acc: 0.47783251231527096, train_loss: 1.023525935463027, val_loss: 1.2869674607450738 (77 / 100)\n",
            "train_acc: 0.61557478368356, val_acc: 0.4827586206896552, train_loss: 1.0101917973260208, val_loss: 1.2909649716222227 (78 / 100)\n",
            "train_acc: 0.6056860321384425, val_acc: 0.47783251231527096, train_loss: 1.0222740916740172, val_loss: 1.2893279729218319 (79 / 100)\n",
            "train_acc: 0.6131025957972805, val_acc: 0.4876847290640394, train_loss: 1.038738663544319, val_loss: 1.28564546730718 (80 / 100)\n",
            "train_acc: 0.6365883807169345, val_acc: 0.47783251231527096, train_loss: 0.9950480263666405, val_loss: 1.2883565044168181 (81 / 100)\n",
            "train_acc: 0.6131025957972805, val_acc: 0.47783251231527096, train_loss: 0.988473687095312, val_loss: 1.2841350697531488 (82 / 100)\n",
            "train_acc: 0.6007416563658838, val_acc: 0.4729064039408867, train_loss: 1.0119144579830806, val_loss: 1.2794552101877523 (83 / 100)\n",
            "train_acc: 0.6217552533992583, val_acc: 0.4729064039408867, train_loss: 0.9848654917055656, val_loss: 1.2825565608264191 (84 / 100)\n",
            "train_acc: 0.6131025957972805, val_acc: 0.47783251231527096, train_loss: 1.003305890062094, val_loss: 1.2822397869502382 (85 / 100)\n",
            "train_acc: 0.6427688504326329, val_acc: 0.47783251231527096, train_loss: 0.9905433634892382, val_loss: 1.2838600307262589 (86 / 100)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.4827586206896552, train_loss: 0.9979837390194717, val_loss: 1.2862012042787863 (87 / 100)\n",
            "train_acc: 0.6131025957972805, val_acc: 0.4876847290640394, train_loss: 0.9946833557192564, val_loss: 1.2892598888556945 (88 / 100)\n",
            "train_acc: 0.622991347342398, val_acc: 0.49261083743842365, train_loss: 1.014927112864623, val_loss: 1.2859966452485823 (89 / 100)\n",
            "train_acc: 0.6489493201483313, val_acc: 0.4876847290640394, train_loss: 0.9802434216028975, val_loss: 1.287016691245469 (90 / 100)\n",
            "train_acc: 0.619283065512979, val_acc: 0.4729064039408867, train_loss: 0.9819803234967961, val_loss: 1.2852377427622603 (91 / 100)\n",
            "train_acc: 0.5982694684796045, val_acc: 0.47783251231527096, train_loss: 1.0145865843086808, val_loss: 1.2813611494496537 (92 / 100)\n",
            "train_acc: 0.6328800988875154, val_acc: 0.4827586206896552, train_loss: 0.9637179240602792, val_loss: 1.2808174423396295 (93 / 100)\n",
            "train_acc: 0.6242274412855378, val_acc: 0.4729064039408867, train_loss: 0.9793687258721577, val_loss: 1.279297319245456 (94 / 100)\n",
            "train_acc: 0.6266996291718171, val_acc: 0.49261083743842365, train_loss: 0.9582632862593247, val_loss: 1.2847976473164675 (95 / 100)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.4827586206896552, train_loss: 0.989684894294173, val_loss: 1.2851395201800493 (96 / 100)\n",
            "train_acc: 0.6266996291718171, val_acc: 0.4827586206896552, train_loss: 0.9586887648432747, val_loss: 1.2857492249000249 (97 / 100)\n",
            "train_acc: 0.6242274412855378, val_acc: 0.4827586206896552, train_loss: 0.984617411574563, val_loss: 1.282018936326351 (98 / 100)\n",
            "train_acc: 0.6254635352286774, val_acc: 0.4876847290640394, train_loss: 0.9950734891172393, val_loss: 1.2812725176364916 (99 / 100)\n",
            "train_acc: 0.6291718170580964, val_acc: 0.49261083743842365, train_loss: 1.0071809498871802, val_loss: 1.283952041799799 (100 / 100)\n",
            "({'lr': 0.00043660847130590896, 'batch_size': 10, 'weight_decay': 0.00025031720443271155, 'gamma': 0.011678955740792939}), val accuracy 0.49261083743842365, val loss 1.3070618918376604 [3 / 8]\n",
            "train_acc: 0.17181705809641531, val_acc: 0.18226600985221675, train_loss: 1.7896751214016795, val_loss: 1.7836480434304975 (1 / 100)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7805867887692635, val_loss: 1.7748527579706879 (2 / 100)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.7750417468574344, val_loss: 1.7672475159461862 (3 / 100)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7670053601707014, val_loss: 1.761302996738791 (4 / 100)\n",
            "train_acc: 0.207663782447466, val_acc: 0.17733990147783252, train_loss: 1.7656165451290287, val_loss: 1.7581329052084185 (5 / 100)\n",
            "train_acc: 0.17181705809641531, val_acc: 0.18226600985221675, train_loss: 1.7636665835221412, val_loss: 1.7542565743911442 (6 / 100)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7574671383104454, val_loss: 1.7505318372707648 (7 / 100)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.759245326993492, val_loss: 1.74786423755984 (8 / 100)\n",
            "train_acc: 0.20024721878862795, val_acc: 0.18226600985221675, train_loss: 1.7509824481087062, val_loss: 1.7430962770443243 (9 / 100)\n",
            "train_acc: 0.22249690976514216, val_acc: 0.18226600985221675, train_loss: 1.7490354300429412, val_loss: 1.7378453345134341 (10 / 100)\n",
            "train_acc: 0.2126081582200247, val_acc: 0.18719211822660098, train_loss: 1.7461120528255316, val_loss: 1.7318490843467524 (11 / 100)\n",
            "train_acc: 0.22620519159456118, val_acc: 0.1921182266009852, train_loss: 1.736988491858776, val_loss: 1.724260116445607 (12 / 100)\n",
            "underfit -> train_accuracy = 0.22620519159456118\n",
            "({'lr': 0.00027531434783290124, 'batch_size': 9, 'weight_decay': 4.3783604017624755e-06, 'gamma': 0.11844128056704877}), val accuracy 0.1921182266009852, val loss 1.724260116445607 [4 / 8]\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7861666676435246, val_loss: 1.7797195746980865 (1 / 100)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7770210115517615, val_loss: 1.7665413782514374 (2 / 100)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.76658577146872, val_loss: 1.7571329289469226 (3 / 100)\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7614888660103194, val_loss: 1.750835052851973 (4 / 100)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.2315270935960591, train_loss: 1.7579491670405762, val_loss: 1.746975199929599 (5 / 100)\n",
            "train_acc: 0.2126081582200247, val_acc: 0.18719211822660098, train_loss: 1.7496424293046533, val_loss: 1.7389411926269531 (6 / 100)\n",
            "train_acc: 0.2200247218788628, val_acc: 0.18719211822660098, train_loss: 1.7504148664521642, val_loss: 1.7292535839409664 (7 / 100)\n",
            "train_acc: 0.2323856613102596, val_acc: 0.2857142857142857, train_loss: 1.7432391289874856, val_loss: 1.7163076113010276 (8 / 100)\n",
            "train_acc: 0.2311495673671199, val_acc: 0.2660098522167488, train_loss: 1.7386996146332938, val_loss: 1.7062354745536015 (9 / 100)\n",
            "train_acc: 0.2843016069221261, val_acc: 0.3497536945812808, train_loss: 1.7100398910355068, val_loss: 1.653443406368124 (10 / 100)\n",
            "train_acc: 0.2904820766378245, val_acc: 0.33004926108374383, train_loss: 1.6846110346290768, val_loss: 1.599790651222755 (11 / 100)\n",
            "train_acc: 0.29913473423980225, val_acc: 0.23645320197044334, train_loss: 1.640645495009216, val_loss: 1.620114536120974 (12 / 100)\n",
            "train_acc: 0.315203955500618, val_acc: 0.2857142857142857, train_loss: 1.6389602580382885, val_loss: 1.6640174676632058 (13 / 100)\n",
            "train_acc: 0.2954264524103832, val_acc: 0.3251231527093596, train_loss: 1.6365082098910186, val_loss: 1.5494744489932883 (14 / 100)\n",
            "train_acc: 0.3176761433868974, val_acc: 0.3054187192118227, train_loss: 1.5896497151465587, val_loss: 1.5286641573083812 (15 / 100)\n",
            "train_acc: 0.3127317676143387, val_acc: 0.2857142857142857, train_loss: 1.611700435356687, val_loss: 1.5454529194996274 (16 / 100)\n",
            "train_acc: 0.30407911001236093, val_acc: 0.3103448275862069, train_loss: 1.5990852099855986, val_loss: 1.5186776580481693 (17 / 100)\n",
            "train_acc: 0.3053152039555006, val_acc: 0.3448275862068966, train_loss: 1.5791886141627327, val_loss: 1.4922222885592231 (18 / 100)\n",
            "train_acc: 0.3189122373300371, val_acc: 0.30049261083743845, train_loss: 1.5688163152583892, val_loss: 1.4945474287559246 (19 / 100)\n",
            "train_acc: 0.3411619283065513, val_acc: 0.30049261083743845, train_loss: 1.562727175772706, val_loss: 1.514878379887548 (20 / 100)\n",
            "train_acc: 0.34239802224969096, val_acc: 0.3645320197044335, train_loss: 1.4920681407041987, val_loss: 1.4502703444711094 (21 / 100)\n",
            "train_acc: 0.3522867737948084, val_acc: 0.3103448275862069, train_loss: 1.5418740111611535, val_loss: 1.5266990209447926 (22 / 100)\n",
            "train_acc: 0.3288009888751545, val_acc: 0.37438423645320196, train_loss: 1.5354687065364994, val_loss: 1.4599989118247196 (23 / 100)\n",
            "train_acc: 0.34981458590852904, val_acc: 0.2857142857142857, train_loss: 1.485948089616113, val_loss: 1.5392364181321243 (24 / 100)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.3103448275862069, train_loss: 1.5124971648523922, val_loss: 1.4574490210105633 (25 / 100)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.30049261083743845, train_loss: 1.500265139879196, val_loss: 1.4788668936696545 (26 / 100)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.3891625615763547, train_loss: 1.4898451901189476, val_loss: 1.4403603159148117 (27 / 100)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.3694581280788177, train_loss: 1.4690941519141933, val_loss: 1.4253489354561115 (28 / 100)\n",
            "train_acc: 0.37824474660074164, val_acc: 0.4088669950738916, train_loss: 1.4479748881496812, val_loss: 1.4070462975008735 (29 / 100)\n",
            "train_acc: 0.377008652657602, val_acc: 0.37438423645320196, train_loss: 1.4569640828593553, val_loss: 1.402541288014116 (30 / 100)\n",
            "train_acc: 0.37082818294190356, val_acc: 0.35960591133004927, train_loss: 1.440074267729251, val_loss: 1.4020368066327324 (31 / 100)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.3645320197044335, train_loss: 1.417559366143678, val_loss: 1.4018188632767776 (32 / 100)\n",
            "train_acc: 0.39555006180469715, val_acc: 0.4088669950738916, train_loss: 1.4061566134465786, val_loss: 1.4132417892587597 (33 / 100)\n",
            "train_acc: 0.4103831891223733, val_acc: 0.3645320197044335, train_loss: 1.4305003088690589, val_loss: 1.416400333930706 (34 / 100)\n",
            "train_acc: 0.4004944375772559, val_acc: 0.39408866995073893, train_loss: 1.4061868397060815, val_loss: 1.408303404676503 (35 / 100)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.35960591133004927, train_loss: 1.3933755906462226, val_loss: 1.4247817458777592 (36 / 100)\n",
            "train_acc: 0.4388133498145859, val_acc: 0.4187192118226601, train_loss: 1.3694114642325967, val_loss: 1.349530709200892 (37 / 100)\n",
            "train_acc: 0.43757725587144625, val_acc: 0.39901477832512317, train_loss: 1.365113873122207, val_loss: 1.3744847774505615 (38 / 100)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.43349753694581283, train_loss: 1.3617152243815778, val_loss: 1.353116039572091 (39 / 100)\n",
            "train_acc: 0.4252163164400494, val_acc: 0.41379310344827586, train_loss: 1.3410656323981078, val_loss: 1.375543606692347 (40 / 100)\n",
            "train_acc: 0.44746600741656367, val_acc: 0.4433497536945813, train_loss: 1.3427542926943936, val_loss: 1.3291631114893947 (41 / 100)\n",
            "train_acc: 0.4635352286773795, val_acc: 0.4236453201970443, train_loss: 1.3035583851216603, val_loss: 1.347035995845137 (42 / 100)\n",
            "train_acc: 0.484548825710754, val_acc: 0.43349753694581283, train_loss: 1.2813908402498337, val_loss: 1.4910176663563168 (43 / 100)\n",
            "train_acc: 0.48331273176761436, val_acc: 0.4630541871921182, train_loss: 1.2557095413007606, val_loss: 1.3312593123008465 (44 / 100)\n",
            "train_acc: 0.44499381953028433, val_acc: 0.4630541871921182, train_loss: 1.291922358411499, val_loss: 1.38007233471706 (45 / 100)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.4187192118226601, train_loss: 1.2486591891688381, val_loss: 1.3520402127298816 (46 / 100)\n",
            "train_acc: 0.4969097651421508, val_acc: 0.42857142857142855, train_loss: 1.1975854014878808, val_loss: 1.3440350417433113 (47 / 100)\n",
            "train_acc: 0.5216316440049443, val_acc: 0.4630541871921182, train_loss: 1.2019917209009894, val_loss: 1.3071334773096546 (48 / 100)\n",
            "train_acc: 0.5203955500618047, val_acc: 0.43349753694581283, train_loss: 1.1978855326090225, val_loss: 1.3005435631192963 (49 / 100)\n",
            "train_acc: 0.5278121137206427, val_acc: 0.47783251231527096, train_loss: 1.1684207923627459, val_loss: 1.3293308307384621 (50 / 100)\n",
            "train_acc: 0.5438813349814586, val_acc: 0.47783251231527096, train_loss: 1.1851064108210825, val_loss: 1.2618018594281426 (51 / 100)\n",
            "train_acc: 0.546353522867738, val_acc: 0.3891625615763547, train_loss: 1.1435144643111637, val_loss: 1.3558119782086075 (52 / 100)\n",
            "train_acc: 0.553770086526576, val_acc: 0.46798029556650245, train_loss: 1.1175305256884827, val_loss: 1.3537893829674557 (53 / 100)\n",
            "train_acc: 0.5611866501854141, val_acc: 0.4876847290640394, train_loss: 1.0911868378906815, val_loss: 1.441941129750219 (54 / 100)\n",
            "train_acc: 0.5710754017305315, val_acc: 0.43842364532019706, train_loss: 1.0898243342106069, val_loss: 1.3467519159974723 (55 / 100)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.4876847290640394, train_loss: 1.0543216319549806, val_loss: 1.3778414479617416 (56 / 100)\n",
            "train_acc: 0.6118665018541409, val_acc: 0.47783251231527096, train_loss: 1.0376828056478087, val_loss: 1.3004726377026787 (57 / 100)\n",
            "train_acc: 0.5784919653893696, val_acc: 0.4827586206896552, train_loss: 1.0598260622531432, val_loss: 1.3493875347334763 (58 / 100)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.4827586206896552, train_loss: 0.980230483932165, val_loss: 1.3492565463329185 (59 / 100)\n",
            "train_acc: 0.6266996291718171, val_acc: 0.4482758620689655, train_loss: 0.9627706161682921, val_loss: 1.3364048744070118 (60 / 100)\n",
            "train_acc: 0.6847960444993819, val_acc: 0.49261083743842365, train_loss: 0.8361430118051536, val_loss: 1.3428011187191666 (61 / 100)\n",
            "train_acc: 0.7255871446229913, val_acc: 0.5024630541871922, train_loss: 0.7838759533554426, val_loss: 1.3304991681000282 (62 / 100)\n",
            "train_acc: 0.6946847960444994, val_acc: 0.5073891625615764, train_loss: 0.7985873563475309, val_loss: 1.310949913386641 (63 / 100)\n",
            "train_acc: 0.7144622991347342, val_acc: 0.5073891625615764, train_loss: 0.7824390019636661, val_loss: 1.3298773457264077 (64 / 100)\n",
            "train_acc: 0.7218788627935723, val_acc: 0.5172413793103449, train_loss: 0.73653025960156, val_loss: 1.35627242614483 (65 / 100)\n",
            "train_acc: 0.7058096415327565, val_acc: 0.5270935960591133, train_loss: 0.7330933869695486, val_loss: 1.3716695925285076 (66 / 100)\n",
            "train_acc: 0.7268232385661311, val_acc: 0.5123152709359606, train_loss: 0.7632930639058315, val_loss: 1.349660688433154 (67 / 100)\n",
            "train_acc: 0.7391841779975278, val_acc: 0.5221674876847291, train_loss: 0.7166492982464756, val_loss: 1.3819818373384147 (68 / 100)\n",
            "train_acc: 0.7367119901112484, val_acc: 0.5221674876847291, train_loss: 0.7126379726254306, val_loss: 1.3682339520289981 (69 / 100)\n",
            "train_acc: 0.761433868974042, val_acc: 0.541871921182266, train_loss: 0.6970092858313335, val_loss: 1.408677471095118 (70 / 100)\n",
            "train_acc: 0.7428924598269468, val_acc: 0.5172413793103449, train_loss: 0.6994938392500647, val_loss: 1.3691240055807705 (71 / 100)\n",
            "train_acc: 0.7626699629171817, val_acc: 0.5172413793103449, train_loss: 0.697907956885761, val_loss: 1.424883431401746 (72 / 100)\n",
            "train_acc: 0.757725587144623, val_acc: 0.5270935960591133, train_loss: 0.6880681301813633, val_loss: 1.4193689268210838 (73 / 100)\n",
            "train_acc: 0.7688504326328801, val_acc: 0.541871921182266, train_loss: 0.6622876076969434, val_loss: 1.4288614536153859 (74 / 100)\n",
            "train_acc: 0.7589616810877626, val_acc: 0.5369458128078818, train_loss: 0.6602585435060989, val_loss: 1.413916713204877 (75 / 100)\n",
            "overfit -> train_accuracy 0.788627935723115, val_accuracy 0.5172413793103449\n",
            "({'lr': 0.0007220498435995008, 'batch_size': 14, 'weight_decay': 2.228552014354877e-05, 'gamma': 0.08113961287843949}), val accuracy 0.541871921182266, val loss 1.408677471095118 [5 / 8]\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7818211063613703, val_loss: 1.7647145305361067 (1 / 100)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7682946442968324, val_loss: 1.7513694264031396 (2 / 100)\n",
            "train_acc: 0.2138442521631644, val_acc: 0.23645320197044334, train_loss: 1.7560919831208746, val_loss: 1.749696900691892 (3 / 100)\n",
            "train_acc: 0.2249690976514215, val_acc: 0.21182266009852216, train_loss: 1.7565025212001446, val_loss: 1.730656121751945 (4 / 100)\n",
            "train_acc: 0.24969097651421507, val_acc: 0.2561576354679803, train_loss: 1.7366777002737754, val_loss: 1.716879397777501 (5 / 100)\n",
            "train_acc: 0.29171817058096416, val_acc: 0.3103448275862069, train_loss: 1.7009117376377025, val_loss: 1.6334610085182002 (6 / 100)\n",
            "train_acc: 0.2669962917181706, val_acc: 0.29064039408866993, train_loss: 1.721392325918813, val_loss: 1.724266503832023 (7 / 100)\n",
            "train_acc: 0.2954264524103832, val_acc: 0.3793103448275862, train_loss: 1.6643545520025662, val_loss: 1.568325483740257 (8 / 100)\n",
            "train_acc: 0.315203955500618, val_acc: 0.28078817733990147, train_loss: 1.63107955986254, val_loss: 1.6194747016934925 (9 / 100)\n",
            "train_acc: 0.3003708281829419, val_acc: 0.3054187192118227, train_loss: 1.6260548709202167, val_loss: 1.543332562070762 (10 / 100)\n",
            "train_acc: 0.2694684796044499, val_acc: 0.2561576354679803, train_loss: 1.6046412598217374, val_loss: 1.6182195277049625 (11 / 100)\n",
            "train_acc: 0.3226205191594561, val_acc: 0.2561576354679803, train_loss: 1.5813496310277686, val_loss: 1.7060172181998567 (12 / 100)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.3497536945812808, train_loss: 1.5656615345380804, val_loss: 1.4755384164490724 (13 / 100)\n",
            "train_acc: 0.34239802224969096, val_acc: 0.33497536945812806, train_loss: 1.5571980831502248, val_loss: 1.5338737594670262 (14 / 100)\n",
            "train_acc: 0.34981458590852904, val_acc: 0.35960591133004927, train_loss: 1.534064902068658, val_loss: 1.4758642459737843 (15 / 100)\n",
            "train_acc: 0.34487021013597036, val_acc: 0.3694581280788177, train_loss: 1.535166447920027, val_loss: 1.4489088810136166 (16 / 100)\n",
            "train_acc: 0.35599505562422745, val_acc: 0.3448275862068966, train_loss: 1.494519933634547, val_loss: 1.4668560309950354 (17 / 100)\n",
            "train_acc: 0.35599505562422745, val_acc: 0.35467980295566504, train_loss: 1.4828642811557124, val_loss: 1.4441546013789812 (18 / 100)\n",
            "train_acc: 0.3856613102595797, val_acc: 0.3399014778325123, train_loss: 1.4623146662753357, val_loss: 1.48071006307461 (19 / 100)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.3645320197044335, train_loss: 1.458672186352856, val_loss: 1.4758451824705001 (20 / 100)\n",
            "train_acc: 0.37824474660074164, val_acc: 0.39408866995073893, train_loss: 1.4200228822540737, val_loss: 1.4756163026898952 (21 / 100)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.3399014778325123, train_loss: 1.414640766728499, val_loss: 1.434823790794523 (22 / 100)\n",
            "train_acc: 0.3992583436341162, val_acc: 0.39408866995073893, train_loss: 1.3783538107229547, val_loss: 1.3883914301548097 (23 / 100)\n",
            "train_acc: 0.40667490729295425, val_acc: 0.41379310344827586, train_loss: 1.381150230636408, val_loss: 1.372382567433888 (24 / 100)\n",
            "train_acc: 0.3980222496909765, val_acc: 0.4236453201970443, train_loss: 1.3569961968988953, val_loss: 1.4425065188572324 (25 / 100)\n",
            "train_acc: 0.44870210135970334, val_acc: 0.35960591133004927, train_loss: 1.3258067593438958, val_loss: 1.3626915251680196 (26 / 100)\n",
            "train_acc: 0.446229913473424, val_acc: 0.39408866995073893, train_loss: 1.3157715815106783, val_loss: 1.3497652508355127 (27 / 100)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.3793103448275862, train_loss: 1.292302116917444, val_loss: 1.381402952330453 (28 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.458128078817734, train_loss: 1.2449283484917488, val_loss: 1.3569896494226503 (29 / 100)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.4088669950738916, train_loss: 1.2735979890057123, val_loss: 1.3047296701393691 (30 / 100)\n",
            "train_acc: 0.4956736711990111, val_acc: 0.4187192118226601, train_loss: 1.2002969949295552, val_loss: 1.3272160426736466 (31 / 100)\n",
            "train_acc: 0.5265760197775031, val_acc: 0.4433497536945813, train_loss: 1.1746609352427744, val_loss: 1.3452060466329452 (32 / 100)\n",
            "train_acc: 0.49938195302843014, val_acc: 0.41379310344827586, train_loss: 1.188621943312316, val_loss: 1.3155493237114892 (33 / 100)\n",
            "train_acc: 0.5216316440049443, val_acc: 0.4482758620689655, train_loss: 1.1495362595368375, val_loss: 1.2869190353478117 (34 / 100)\n",
            "train_acc: 0.553770086526576, val_acc: 0.43349753694581283, train_loss: 1.1336733917665422, val_loss: 1.3157043709543539 (35 / 100)\n",
            "train_acc: 0.5735475896168108, val_acc: 0.4236453201970443, train_loss: 1.074478788929905, val_loss: 1.3213418322830952 (36 / 100)\n",
            "train_acc: 0.5747836835599506, val_acc: 0.43349753694581283, train_loss: 1.0350455649850985, val_loss: 1.3961987539465204 (37 / 100)\n",
            "train_acc: 0.6390605686032138, val_acc: 0.45320197044334976, train_loss: 0.9894883716651625, val_loss: 1.4679504145542388 (38 / 100)\n",
            "train_acc: 0.5896168108776267, val_acc: 0.49261083743842365, train_loss: 1.0558017965300268, val_loss: 1.2173933687762086 (39 / 100)\n",
            "train_acc: 0.6415327564894932, val_acc: 0.4876847290640394, train_loss: 0.9066119650827792, val_loss: 1.3396403537007975 (40 / 100)\n",
            "train_acc: 0.6786155747836835, val_acc: 0.5172413793103449, train_loss: 0.8607781496271951, val_loss: 1.3202849993564811 (41 / 100)\n",
            "train_acc: 0.6860321384425216, val_acc: 0.5369458128078818, train_loss: 0.8167170461529707, val_loss: 1.2331452410796593 (42 / 100)\n",
            "train_acc: 0.6946847960444994, val_acc: 0.45320197044334976, train_loss: 0.7949860535092348, val_loss: 1.3899892746227716 (43 / 100)\n",
            "train_acc: 0.7268232385661311, val_acc: 0.5073891625615764, train_loss: 0.7124589523956714, val_loss: 1.2568137325676791 (44 / 100)\n",
            "train_acc: 0.7466007416563659, val_acc: 0.5123152709359606, train_loss: 0.7314317662577399, val_loss: 1.3832288269926174 (45 / 100)\n",
            "train_acc: 0.7490729295426453, val_acc: 0.5073891625615764, train_loss: 0.6745059854167943, val_loss: 1.382282345459379 (46 / 100)\n",
            "overfit -> train_accuracy 0.788627935723115, val_accuracy 0.5320197044334976\n",
            "({'lr': 0.0008377019231346562, 'batch_size': 8, 'weight_decay': 2.4427015675775187e-06, 'gamma': 0.00903130010323455}), val accuracy 0.5369458128078818, val loss 1.2331452410796593 [6 / 8]\n",
            "train_acc: 0.1631644004944376, val_acc: 0.18226600985221675, train_loss: 1.7766925758130765, val_loss: 1.7539880821857545 (1 / 100)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7596699351580682, val_loss: 1.7438263446826654 (2 / 100)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.18226600985221675, train_loss: 1.7552002472400077, val_loss: 1.7404161362812436 (3 / 100)\n",
            "train_acc: 0.23980222496909764, val_acc: 0.28078817733990147, train_loss: 1.7384143696432504, val_loss: 1.7143891151315473 (4 / 100)\n",
            "train_acc: 0.26452410383189123, val_acc: 0.30049261083743845, train_loss: 1.7122499581762534, val_loss: 1.6220136622489967 (5 / 100)\n",
            "train_acc: 0.2558714462299135, val_acc: 0.22660098522167488, train_loss: 1.709246553508254, val_loss: 1.7155815503867389 (6 / 100)\n",
            "train_acc: 0.28182941903584674, val_acc: 0.27586206896551724, train_loss: 1.66151555109083, val_loss: 1.5941098399937446 (7 / 100)\n",
            "train_acc: 0.29913473423980225, val_acc: 0.3103448275862069, train_loss: 1.653239645091506, val_loss: 1.603086013512071 (8 / 100)\n",
            "train_acc: 0.3127317676143387, val_acc: 0.31527093596059114, train_loss: 1.599515911380499, val_loss: 1.526085434876052 (9 / 100)\n",
            "train_acc: 0.34239802224969096, val_acc: 0.3399014778325123, train_loss: 1.5732850948871286, val_loss: 1.5342815163100294 (10 / 100)\n",
            "train_acc: 0.30778739184178, val_acc: 0.27586206896551724, train_loss: 1.5748609531353077, val_loss: 1.5317456810345202 (11 / 100)\n",
            "train_acc: 0.32509270704573545, val_acc: 0.31527093596059114, train_loss: 1.5477011520576713, val_loss: 1.5252825491534079 (12 / 100)\n",
            "train_acc: 0.3189122373300371, val_acc: 0.30049261083743845, train_loss: 1.5342656790989144, val_loss: 1.490433931350708 (13 / 100)\n",
            "train_acc: 0.36093943139678614, val_acc: 0.37438423645320196, train_loss: 1.5124600652711206, val_loss: 1.44714960030147 (14 / 100)\n",
            "train_acc: 0.3572311495673671, val_acc: 0.3645320197044335, train_loss: 1.4860571792305473, val_loss: 1.4459439363385655 (15 / 100)\n",
            "train_acc: 0.37082818294190356, val_acc: 0.35467980295566504, train_loss: 1.4675435104829861, val_loss: 1.4120907243249452 (16 / 100)\n",
            "train_acc: 0.37330037082818296, val_acc: 0.31527093596059114, train_loss: 1.461762725496469, val_loss: 1.482680203879408 (17 / 100)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.4039408866995074, train_loss: 1.4442971572003642, val_loss: 1.498663428381746 (18 / 100)\n",
            "train_acc: 0.4004944375772559, val_acc: 0.3694581280788177, train_loss: 1.4117927087104807, val_loss: 1.473797311630155 (19 / 100)\n",
            "train_acc: 0.4264524103831891, val_acc: 0.3793103448275862, train_loss: 1.3613594344578803, val_loss: 1.413333345516562 (20 / 100)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.4039408866995074, train_loss: 1.3928982415806377, val_loss: 1.3569276045108665 (21 / 100)\n",
            "train_acc: 0.411619283065513, val_acc: 0.43349753694581283, train_loss: 1.3785852050899132, val_loss: 1.342154352535755 (22 / 100)\n",
            "train_acc: 0.4437577255871446, val_acc: 0.39901477832512317, train_loss: 1.3111867198837996, val_loss: 1.3979129391938008 (23 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.35960591133004927, train_loss: 1.2748218254636332, val_loss: 1.4062273461243202 (24 / 100)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.4433497536945813, train_loss: 1.2341257921697477, val_loss: 1.4092442314025804 (25 / 100)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.46798029556650245, train_loss: 1.268781963325991, val_loss: 1.3221571204697558 (26 / 100)\n",
            "train_acc: 0.49814585908529047, val_acc: 0.4088669950738916, train_loss: 1.2048794194705996, val_loss: 1.3081591619646609 (27 / 100)\n",
            "train_acc: 0.5241038318912238, val_acc: 0.3694581280788177, train_loss: 1.1711495618148258, val_loss: 1.5107391315140748 (28 / 100)\n",
            "train_acc: 0.48331273176761436, val_acc: 0.4039408866995074, train_loss: 1.1932269041558867, val_loss: 1.4026355044595127 (29 / 100)\n",
            "train_acc: 0.5451174289245982, val_acc: 0.4482758620689655, train_loss: 1.1358668018772518, val_loss: 1.4358925073604865 (30 / 100)\n",
            "train_acc: 0.5636588380716935, val_acc: 0.41379310344827586, train_loss: 1.0778652382427447, val_loss: 1.3335260913289826 (31 / 100)\n",
            "train_acc: 0.5723114956736712, val_acc: 0.458128078817734, train_loss: 1.0747713683120106, val_loss: 1.334182380455468 (32 / 100)\n",
            "train_acc: 0.5772558714462299, val_acc: 0.4482758620689655, train_loss: 1.0337582761778672, val_loss: 1.3569612608754575 (33 / 100)\n",
            "train_acc: 0.6217552533992583, val_acc: 0.4729064039408867, train_loss: 0.9941877452345802, val_loss: 1.3541251644125125 (34 / 100)\n",
            "train_acc: 0.6402966625463535, val_acc: 0.4630541871921182, train_loss: 0.9266162067467556, val_loss: 1.399338897225892 (35 / 100)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.4482758620689655, train_loss: 0.9438230067454693, val_loss: 1.4723531595004604 (36 / 100)\n",
            "train_acc: 0.6526576019777504, val_acc: 0.4876847290640394, train_loss: 0.8838422512094524, val_loss: 1.2692955503322807 (37 / 100)\n",
            "train_acc: 0.657601977750309, val_acc: 0.49261083743842365, train_loss: 0.913736150644913, val_loss: 1.3009560190398117 (38 / 100)\n",
            "train_acc: 0.6662546353522868, val_acc: 0.49261083743842365, train_loss: 0.8611669599346826, val_loss: 1.5212284843322679 (39 / 100)\n",
            "train_acc: 0.7218788627935723, val_acc: 0.5172413793103449, train_loss: 0.7409040273635732, val_loss: 1.4521554445691884 (40 / 100)\n",
            "train_acc: 0.7255871446229913, val_acc: 0.5369458128078818, train_loss: 0.7377173862292239, val_loss: 1.343754017294334 (41 / 100)\n",
            "overfit -> train_accuracy 0.7725587144622992, val_accuracy 0.4827586206896552\n",
            "({'lr': 0.0010316163585472981, 'batch_size': 8, 'weight_decay': 1.8309942558988887e-05, 'gamma': 0.002673690056313373}), val accuracy 0.5369458128078818, val loss 1.343754017294334 [7 / 8]\n",
            "train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7771501181594227, val_loss: 1.7548545828006539 (1 / 100)\n",
            "train_acc: 0.21631644004944375, val_acc: 0.18226600985221675, train_loss: 1.7654944232426704, val_loss: 1.7531483108774195 (2 / 100)\n",
            "train_acc: 0.21013597033374537, val_acc: 0.18226600985221675, train_loss: 1.7499043114842532, val_loss: 1.727023784162963 (3 / 100)\n",
            "train_acc: 0.2360939431396786, val_acc: 0.28078817733990147, train_loss: 1.7288300428166525, val_loss: 1.6796269105573005 (4 / 100)\n",
            "train_acc: 0.2583436341161928, val_acc: 0.26108374384236455, train_loss: 1.7285280756661565, val_loss: 1.6600043485904563 (5 / 100)\n",
            "train_acc: 0.2843016069221261, val_acc: 0.28078817733990147, train_loss: 1.699261367836753, val_loss: 1.636972661676078 (6 / 100)\n",
            "train_acc: 0.24721878862793573, val_acc: 0.21674876847290642, train_loss: 1.672389029277712, val_loss: 1.669879928598263 (7 / 100)\n",
            "train_acc: 0.276885043263288, val_acc: 0.29064039408866993, train_loss: 1.6376952738932833, val_loss: 1.5834552977472691 (8 / 100)\n",
            "train_acc: 0.3374536464771323, val_acc: 0.33497536945812806, train_loss: 1.5902059505542807, val_loss: 1.5211945754553884 (9 / 100)\n",
            "train_acc: 0.3003708281829419, val_acc: 0.33004926108374383, train_loss: 1.578896675180593, val_loss: 1.5219833657072095 (10 / 100)\n",
            "train_acc: 0.3238566131025958, val_acc: 0.33497536945812806, train_loss: 1.5753891341323758, val_loss: 1.534960998690187 (11 / 100)\n",
            "train_acc: 0.3065512978986403, val_acc: 0.33497536945812806, train_loss: 1.576653564816205, val_loss: 1.4723006663064064 (12 / 100)\n",
            "train_acc: 0.32756489493201485, val_acc: 0.2955665024630542, train_loss: 1.5250024894259326, val_loss: 1.5390588714571423 (13 / 100)\n",
            "train_acc: 0.3522867737948084, val_acc: 0.32019704433497537, train_loss: 1.5144119754857275, val_loss: 1.4842161826899487 (14 / 100)\n",
            "train_acc: 0.3473423980222497, val_acc: 0.33497536945812806, train_loss: 1.487132582440512, val_loss: 1.4402958683192437 (15 / 100)\n",
            "train_acc: 0.37453646477132263, val_acc: 0.35960591133004927, train_loss: 1.4395528737045777, val_loss: 1.4646280998079648 (16 / 100)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.3054187192118227, train_loss: 1.4443730412071802, val_loss: 1.4728366576979313 (17 / 100)\n",
            "train_acc: 0.3967861557478368, val_acc: 0.39408866995073893, train_loss: 1.383141069093947, val_loss: 1.4096101915895058 (18 / 100)\n",
            "train_acc: 0.39555006180469715, val_acc: 0.3497536945812808, train_loss: 1.4027006299003535, val_loss: 1.4220207988334994 (19 / 100)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.3103448275862069, train_loss: 1.3607075843587058, val_loss: 1.4451146783499882 (20 / 100)\n",
            "train_acc: 0.415327564894932, val_acc: 0.3497536945812808, train_loss: 1.3524714808528886, val_loss: 1.421655544506505 (21 / 100)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.39901477832512317, train_loss: 1.276603008819599, val_loss: 1.3974204042862202 (22 / 100)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.3645320197044335, train_loss: 1.2597652163876887, val_loss: 1.411207056397875 (23 / 100)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.4039408866995074, train_loss: 1.2422341827409082, val_loss: 1.3416415652618032 (24 / 100)\n",
            "train_acc: 0.5067985166872683, val_acc: 0.458128078817734, train_loss: 1.1933199914335761, val_loss: 1.3026082762356461 (25 / 100)\n",
            "train_acc: 0.5253399258343634, val_acc: 0.39901477832512317, train_loss: 1.1758836630100962, val_loss: 1.4438523256719993 (26 / 100)\n",
            "train_acc: 0.5389369592088998, val_acc: 0.39901477832512317, train_loss: 1.1260489972471748, val_loss: 1.4162683287277598 (27 / 100)\n",
            "train_acc: 0.5550061804697157, val_acc: 0.4729064039408867, train_loss: 1.0764345004325744, val_loss: 1.260308179362067 (28 / 100)\n",
            "train_acc: 0.6044499381953028, val_acc: 0.4088669950738916, train_loss: 1.0364619052896393, val_loss: 1.2856104227122416 (29 / 100)\n",
            "train_acc: 0.580964153275649, val_acc: 0.4482758620689655, train_loss: 1.0357003559730256, val_loss: 1.2996841263888506 (30 / 100)\n",
            "train_acc: 0.5920889987639061, val_acc: 0.49261083743842365, train_loss: 1.0061518674433159, val_loss: 1.2977427631763403 (31 / 100)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.4827586206896552, train_loss: 0.9387034547638392, val_loss: 1.2701738561902727 (32 / 100)\n",
            "train_acc: 0.6860321384425216, val_acc: 0.49261083743842365, train_loss: 0.8141766475658629, val_loss: 1.405986180446418 (33 / 100)\n",
            "overfit -> train_accuracy 0.7021013597033374, val_accuracy 0.43842364532019706\n",
            "({'lr': 0.0016661746592012004, 'batch_size': 8, 'weight_decay': 3.3763075569909223e-06, 'gamma': 0.006052773438030023}), val accuracy 0.49261083743842365, val loss 1.2977427631763403 [8 / 8]\n",
            "\n",
            "\n",
            "({'lr': 0.0006444500508054211, 'batch_size': 14, 'weight_decay': 2.1280582227123365e-05, 'gamma': 0.19924404264743992}), best val accuracy 0.5566502463054187, best val loss 1.3541624217197812\n",
            "\n",
            "\n",
            "val_accuracies\n",
            "[0.5566502463054187, 0.5221674876847291, 0.49261083743842365, 0.1921182266009852, 0.541871921182266, 0.5369458128078818, 0.5369458128078818, 0.49261083743842365]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S1laZWm8Q0tm"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKl555WRQ1AF",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "# todo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jJGI06ylKePa"
      },
      "source": [
        "**Mean / std computation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDJptx12L1OL",
        "trusted": false,
        "outputId": "3faa7a3e-7dce-418a-8a16-5ac13471f165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec-672'\n",
        "pixel_mean = np.zeros(3)\n",
        "pixel_std = np.zeros(3)\n",
        "k = 1\n",
        "dataset, _ = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, [])\n",
        "for image, _ in tqdm(dataset, \"Computing mean/std\", len(dataset), unit=\"samples\"):\n",
        "    image = np.array(image)\n",
        "    pixels = image.reshape((-1, image.shape[2]))\n",
        "\n",
        "    for pixel in pixels:\n",
        "        diff = pixel - pixel_mean\n",
        "        pixel_mean += diff / k\n",
        "        pixel_std += diff * (pixel - pixel_mean)\n",
        "        k += 1\n",
        "\n",
        "pixel_std = np.sqrt(pixel_std / (k - 2))\n",
        "print(pixel_mean)\n",
        "print(pixel_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AIML_project'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 24421 (delta 28), reused 33 (delta 14), pack-reused 24373\u001b[K\n",
            "Receiving objects: 100% (24421/24421), 2.15 GiB | 48.26 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Checking out files: 100% (24638/24638), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Computing mean/std: 100%|| 1012/1012 [47:31<00:00,  2.82s/samples]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[45.6068733   0.81077038 57.85301916]\n",
            "[66.92374056  9.88349788 49.96761776]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}