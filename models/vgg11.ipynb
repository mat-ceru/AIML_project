{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MbtUcKfE_I4g"
      },
      "source": [
        "**Installs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzg4cO9xLvUG",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fxs_3zcG_NZd"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C7N0hU-VLx8W",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg11\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "#NUM_CLASSES = 102\n",
        "NUM_CLASSES = 6\n",
        "DEVICE = 'cuda'\n",
        "MOMENTUM = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uvABcepY_Vfe"
      },
      "source": [
        "**Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vztVCv3fQXjR",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "def get_datasets(train_data_dir, test_data_dir, compose=[transforms.Resize(224),\n",
        "                                                         transforms.CenterCrop(224),\n",
        "                                                         transforms.ToTensor()\n",
        "                                                         ]):\n",
        "    train_transform = transforms.Compose(compose)\n",
        "    eval_transform = transforms.Compose([\n",
        "          transforms.Resize(224),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor()#,\n",
        "          #transforms.Normalize((45.6068733, 0.81077038, 57.85301916), (66.92374056, 9.88349788, 49.96761776))\n",
        "          ])\n",
        "\n",
        "    '''\n",
        "    if not os.path.isdir('./Homework2-Caltech101'):\n",
        "        !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n",
        "\n",
        "    '''\n",
        "    if not os.path.isdir('./AIML_project'):\n",
        "        !git clone https://github.com/anphetamina/AIML_project.git\n",
        "    \n",
        "    train_dataset = torchvision.datasets.ImageFolder(train_data_dir, transform=train_transform)\n",
        "    test_dataset = torchvision.datasets.ImageFolder(test_data_dir, transform=eval_transform)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def test_network(net, test_dataset, batch_size):\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    net.train(False)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    sum_test_losses = 0.0\n",
        "    running_corrects = 0\n",
        "    for images, labels in test_dataloader:\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      test_loss = criterion(outputs, labels)\n",
        "      sum_test_losses += test_loss.item()*images.size(0)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "    # Calculate loss\n",
        "    test_loss = sum_test_losses / float(len(test_dataset))\n",
        "\n",
        "    return accuracy, test_loss\n",
        "\n",
        "def train_network(net, parameters_to_optimize, learning_rate, num_epochs, batch_size, weight_decay, step_size, gamma, train_dataset, val_dataset=None, verbosity=False, plot=False):\n",
        "  \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=False)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=learning_rate, momentum=MOMENTUM, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    net = net.to(DEVICE)\n",
        "    best_net = vgg11()\n",
        "    best_net = best_net.to(DEVICE)\n",
        "    best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "\n",
        "    current_step = 0\n",
        "    best_val_accuracy = 0.0\n",
        "    best_val_loss = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_running_corrects = 0\n",
        "        sum_train_losses = 0.0\n",
        "\n",
        "        for images, labels in train_dataloader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            net.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            train_running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "            loss = criterion(outputs, labels)\n",
        "            sum_train_losses += loss.item()*images.size(0)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            current_step += 1\n",
        "        \n",
        "        # Calculate accuracy on train set\n",
        "        train_accuracy = train_running_corrects / float(len(train_dataset))\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        if val_dataset is not None:\n",
        "            val_accuracy, val_loss = test_network(net, val_dataset, batch_size)\n",
        "            if val_accuracy > best_val_accuracy:\n",
        "                best_val_accuracy = val_accuracy\n",
        "                best_val_loss = val_loss\n",
        "                best_net.load_state_dict(net.state_dict())\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            val_losses.append(val_loss)\n",
        "            acc_diff = train_accuracy-val_accuracy\n",
        "            if acc_diff > 0.25:\n",
        "              print(\"overfit -> train_accuracy {}, val_accuracy {}\".format(train_accuracy, val_accuracy))\n",
        "              return best_net, best_val_accuracy, best_val_loss\n",
        "\n",
        "        \n",
        "\n",
        "        # Calculate loss on training set\n",
        "        train_loss = sum_train_losses/float(len(train_dataset))\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        if verbosity:\n",
        "            if val_dataset is not None:\n",
        "                print(\"train_acc: {}, val_acc: {}, train_loss: {}, val_loss: {} ({} / {})\".format(train_accuracy, val_accuracy, train_loss, val_loss, epoch+1, num_epochs))\n",
        "            else:\n",
        "                print(\"train_acc: {}, train_loss: {} ({} / {})\".format(train_accuracy, train_loss, epoch+1, num_epochs))\n",
        "        \n",
        "\n",
        "        if train_accuracy < 0.25 and epoch > num_epochs*0.1 or train_accuracy < 0.35 and epoch > num_epochs*0.5:\n",
        "          print(\"underfit -> train_accuracy = {}\".format(train_accuracy))\n",
        "          return best_net, best_val_accuracy, best_val_loss\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    if plot:\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        line1, = ax.plot(train_losses, label='Loss on training set')\n",
        "        line2, = ax.plot(train_accuracies, label='Accuracy on training set')\n",
        "        ax.legend()\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.show()\n",
        "\n",
        "        if val_dataset is not None:\n",
        "            fig, ax = plt.subplots()\n",
        "            line1, = ax.plot(val_accuracies, label='Accuracy on validation set', color='C2')\n",
        "            line2, = ax.plot(train_accuracies, label='Accuracy on training set', color='C3')\n",
        "            ax.legend()\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.show()\n",
        "        \n",
        "            fig, ax = plt.subplots()\n",
        "            line1, = ax.plot(val_losses, label='Loss on validation set', color='C1')\n",
        "            line2, = ax.plot(train_losses, label='Loss on training set', color='C7')\n",
        "            ax.legend()\n",
        "            plt.xlabel(\"Epochs\")\n",
        "            plt.show()\n",
        "\n",
        "    \n",
        "    return best_net, best_val_accuracy, best_val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6fTm2sD_BOt"
      },
      "source": [
        "**Train + validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XtBXC1cO_A6A",
        "outputId": "0813d032-f98a-42d6-e5af-86ebcda2f861",
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# lr 0.0042237987628595194, batch 15, decay 0.00042683917479004744, gamma 0.031192333743237592, val accuracy 0.4975369458128079, val loss 1.1257490495155598 [4 / 50]\n",
        "# {'lr': 0.0034817317117461864, 'batch_size': 9, 'weight_decay': 0.011757203744469723, 'gamma': 0.08397906547450418}\n",
        "BATCH_SIZE = 8\n",
        "LR = 0.003\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.02\n",
        "NUM_EPOCHS = 50\n",
        "STEP_SIZE = 30\n",
        "GAMMA = 0.08\n",
        "\n",
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec-672'\n",
        "#TRAIN_DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n",
        "compose=[transforms.Resize(224),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.RandomGrayscale(),\n",
        "         transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "         transforms.ToTensor()\n",
        "         ]\n",
        "train_dataset, val_dataset = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, compose)\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 5]\n",
        "val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 5]\n",
        "val_dataset = Subset(val_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))\n",
        "\n",
        "net = vgg11()\n",
        "net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "best_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True, plot=True)\n",
        "\n",
        "print('val accuracy {}'.format(val_accuracy))\n",
        "print('val loss {}'.format(val_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set 809\n",
            "validation set 203\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.779353533598637, val_loss: 1.7607205888907898 (1 / 50)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.19704433497536947, train_loss: 1.760152413612243, val_loss: 1.726006605942261 (2 / 50)\n",
            "train_acc: 0.22744128553770088, val_acc: 0.22167487684729065, train_loss: 1.7442417583889955, val_loss: 1.7161729940639927 (3 / 50)\n",
            "train_acc: 0.25710754017305315, val_acc: 0.3103448275862069, train_loss: 1.6953646019745816, val_loss: 1.6452216403237705 (4 / 50)\n",
            "train_acc: 0.3016069221260816, val_acc: 0.23645320197044334, train_loss: 1.6592598067520576, val_loss: 1.7206044426105294 (5 / 50)\n",
            "train_acc: 0.2978986402966625, val_acc: 0.3399014778325123, train_loss: 1.6601062877511212, val_loss: 1.527847541376875 (6 / 50)\n",
            "train_acc: 0.2830655129789864, val_acc: 0.32019704433497537, train_loss: 1.6179320464470184, val_loss: 1.7279341890306896 (7 / 50)\n",
            "train_acc: 0.33250927070457353, val_acc: 0.31527093596059114, train_loss: 1.5655036862612655, val_loss: 1.4962098639586876 (8 / 50)\n",
            "train_acc: 0.3164400494437577, val_acc: 0.33004926108374383, train_loss: 1.5691660845972257, val_loss: 1.4602862132593917 (9 / 50)\n",
            "train_acc: 0.34981458590852904, val_acc: 0.37438423645320196, train_loss: 1.5124627880761297, val_loss: 1.4995354061643478 (10 / 50)\n",
            "train_acc: 0.3683559950556242, val_acc: 0.35467980295566504, train_loss: 1.478119226409714, val_loss: 1.4940966532148163 (11 / 50)\n",
            "train_acc: 0.34487021013597036, val_acc: 0.37438423645320196, train_loss: 1.4700040798104739, val_loss: 1.405609208374775 (12 / 50)\n",
            "train_acc: 0.37330037082818296, val_acc: 0.31527093596059114, train_loss: 1.4691461225964673, val_loss: 1.5850158868164852 (13 / 50)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.3448275862068966, train_loss: 1.4705159398917067, val_loss: 1.4307451676852598 (14 / 50)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.43349753694581283, train_loss: 1.4176444465061937, val_loss: 1.4084776915940158 (15 / 50)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.37438423645320196, train_loss: 1.3768114262664568, val_loss: 1.419537316402191 (16 / 50)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.3694581280788177, train_loss: 1.4086436859904172, val_loss: 1.3636868223180911 (17 / 50)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.3842364532019704, train_loss: 1.382980784910129, val_loss: 1.4100395212032524 (18 / 50)\n",
            "train_acc: 0.44128553770086526, val_acc: 0.39901477832512317, train_loss: 1.3282899214104462, val_loss: 1.4232673415996757 (19 / 50)\n",
            "train_acc: 0.44128553770086526, val_acc: 0.43349753694581283, train_loss: 1.3352832807451303, val_loss: 1.2693794901147852 (20 / 50)\n",
            "train_acc: 0.446229913473424, val_acc: 0.4433497536945813, train_loss: 1.313489877838729, val_loss: 1.3911424147084428 (21 / 50)\n",
            "train_acc: 0.4363411619283066, val_acc: 0.42857142857142855, train_loss: 1.2978979813301077, val_loss: 1.2727257324557 (22 / 50)\n",
            "train_acc: 0.48702101359703337, val_acc: 0.37438423645320196, train_loss: 1.22468835903776, val_loss: 1.4323499331920606 (23 / 50)\n",
            "train_acc: 0.4758961681087763, val_acc: 0.43349753694581283, train_loss: 1.262905056013902, val_loss: 1.375212116488095 (24 / 50)\n",
            "train_acc: 0.4894932014833127, val_acc: 0.4482758620689655, train_loss: 1.249698081034223, val_loss: 1.2885054907775277 (25 / 50)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.4630541871921182, train_loss: 1.2736242710438883, val_loss: 1.2653444101070535 (26 / 50)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.43842364532019706, train_loss: 1.207345455921772, val_loss: 1.288880638301079 (27 / 50)\n",
            "train_acc: 0.4969097651421508, val_acc: 0.3399014778325123, train_loss: 1.2049727834788773, val_loss: 1.6169691320710582 (28 / 50)\n",
            "train_acc: 0.4919653893695921, val_acc: 0.42857142857142855, train_loss: 1.2405566833222605, val_loss: 1.267301765275119 (29 / 50)\n",
            "train_acc: 0.5414091470951793, val_acc: 0.5517241379310345, train_loss: 1.140572209882795, val_loss: 1.1823671932878166 (30 / 50)\n",
            "train_acc: 0.5772558714462299, val_acc: 0.5320197044334976, train_loss: 1.0218605561957814, val_loss: 1.1567935186066651 (31 / 50)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5320197044334976, train_loss: 0.9637914912220279, val_loss: 1.161686829158238 (32 / 50)\n",
            "train_acc: 0.619283065512979, val_acc: 0.5566502463054187, train_loss: 0.9561477612211324, val_loss: 1.1519732460599814 (33 / 50)\n",
            "train_acc: 0.6514215080346106, val_acc: 0.5517241379310345, train_loss: 0.9184959845430919, val_loss: 1.2353963188349908 (34 / 50)\n",
            "train_acc: 0.65389369592089, val_acc: 0.5517241379310345, train_loss: 0.879010093374217, val_loss: 1.1521129420238176 (35 / 50)\n",
            "train_acc: 0.6415327564894932, val_acc: 0.5615763546798029, train_loss: 0.8869051635633735, val_loss: 1.1543873337102053 (36 / 50)\n",
            "train_acc: 0.6514215080346106, val_acc: 0.5221674876847291, train_loss: 0.8588405902070374, val_loss: 1.2080408963076588 (37 / 50)\n",
            "train_acc: 0.6798516687268232, val_acc: 0.5665024630541872, train_loss: 0.8224939566165467, val_loss: 1.1637692727478854 (38 / 50)\n",
            "train_acc: 0.6823238566131026, val_acc: 0.5270935960591133, train_loss: 0.8150768766297103, val_loss: 1.2131320983905511 (39 / 50)\n",
            "train_acc: 0.69221260815822, val_acc: 0.5615763546798029, train_loss: 0.794620599970682, val_loss: 1.1129314784933193 (40 / 50)\n",
            "train_acc: 0.69221260815822, val_acc: 0.5615763546798029, train_loss: 0.7936648718358854, val_loss: 1.1210556629256074 (41 / 50)\n",
            "train_acc: 0.6971569839307787, val_acc: 0.5812807881773399, train_loss: 0.77628504434239, val_loss: 1.080905824459245 (42 / 50)\n",
            "train_acc: 0.715698393077874, val_acc: 0.5714285714285714, train_loss: 0.7255986936307514, val_loss: 1.1430520206836645 (43 / 50)\n",
            "train_acc: 0.73053152039555, val_acc: 0.5566502463054187, train_loss: 0.6968811987651735, val_loss: 1.1668779908729892 (44 / 50)\n",
            "train_acc: 0.7330037082818294, val_acc: 0.5763546798029556, train_loss: 0.6710003558431185, val_loss: 1.168790549480269 (45 / 50)\n",
            "train_acc: 0.7367119901112484, val_acc: 0.5517241379310345, train_loss: 0.6684570832659199, val_loss: 1.136658889613128 (46 / 50)\n",
            "train_acc: 0.7663782447466008, val_acc: 0.5960591133004927, train_loss: 0.6079379477813306, val_loss: 1.1822110373214842 (47 / 50)\n",
            "train_acc: 0.7564894932014833, val_acc: 0.6059113300492611, train_loss: 0.6405703673698698, val_loss: 1.1297464077108599 (48 / 50)\n",
            "train_acc: 0.7948084054388134, val_acc: 0.6206896551724138, train_loss: 0.5489810772967721, val_loss: 1.2119932565195808 (49 / 50)\n",
            "train_acc: 0.7836835599505563, val_acc: 0.5665024630541872, train_loss: 0.5960819062550077, val_loss: 1.240074037991721 (50 / 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVxRFyRJU0BVFUbIEFXRFUZQs4f8BV1wLCEDOiOcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4YJuZpYSLuhmZinhgm5mlhIu6GZmKfF/2AgMeEI7g+IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzcxKwgHdzKwkHNDNzErifxw/DFk3+KeNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "val accuracy 0.6206896551724138\n",
            "val loss 1.2119932565195808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1kgj76dvQxIJ"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_dKdDvgnQw7d",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "# todo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUIpGmogDgOC",
        "colab_type": "text"
      },
      "source": [
        "**Random search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1eOsPQVDgG6",
        "colab_type": "code",
        "outputId": "5cecbf4c-86e9-4a86-99c1-5d3c6ef6ede5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# best scores\n",
        "# \n",
        "best_accuracy = 0.0\n",
        "best_loss = 0.0\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "import random\n",
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec-672'\n",
        "#TRAIN_DATA_DIR = 'Homework2-Caltech101/101_ObjectCategories'\n",
        "compose=[transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.RandomGrayscale(),\n",
        "        transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "        transforms.ToTensor()\n",
        "        ]\n",
        "train_dataset, val_dataset = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, compose)\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 5]\n",
        "val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 5]\n",
        "val_dataset = Subset(val_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))\n",
        "best_net = vgg11()\n",
        "best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "best_net = best_net.to(DEVICE)\n",
        "best_set = {}\n",
        "N = 50\n",
        "for i in range(N):\n",
        "  BATCH_SIZE = int(random.uniform(8, 16))\n",
        "  LR = 10**random.uniform(-5, -3)\n",
        "  MOMENTUM = 0.9\n",
        "  WEIGHT_DECAY = 10**random.uniform(-6, -3)\n",
        "  NUM_EPOCHS = 80\n",
        "  STEP_SIZE = 48\n",
        "  GAMMA = 10**random.uniform(-2, 0)\n",
        "  set = {\"lr\": LR, \"batch_size\": BATCH_SIZE, \"weight_decay\": WEIGHT_DECAY, \"gamma\": GAMMA}\n",
        "  print(\"-------------------------------------\")\n",
        "  print(set)\n",
        "  net = vgg11()\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "  current_net, val_accuracy, val_loss = train_network(net, net.parameters(), LR, NUM_EPOCHS, BATCH_SIZE, WEIGHT_DECAY, STEP_SIZE, GAMMA, train_dataset, val_dataset=val_dataset, verbosity=True)\n",
        "\n",
        "  val_accuracies.append(val_accuracy)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  if val_accuracy > best_accuracy:\n",
        "    best_accuracy = val_accuracy\n",
        "    best_loss = val_loss\n",
        "    best_net = copy.deepcopy(current_net)\n",
        "    best_set = copy.deepcopy(set)\n",
        "\n",
        "  print(\"lr {}, batch {}, decay {}, gamma {}, val accuracy {}, val loss {} [{} / {}]\".format(LR, BATCH_SIZE, WEIGHT_DECAY, GAMMA, val_accuracy, val_loss, i+1, N))\n",
        "\n",
        "print(\"--------------------------------------------\")\n",
        "print(\"\\n{}, best val accuracy {}, best val loss {}\".format(best_set, best_accuracy, best_loss))\n",
        "print(\"val accuracies\\n{}\".format(val_accuracies))\n",
        "print(\"val losses\\n{}\".format(val_losses))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set 809\n",
            "validation set 203\n",
            "-------------------------------------\n",
            "{'lr': 6.87058653262615e-05, 'batch_size': 11, 'weight_decay': 0.0002907904402306258, 'gamma': 0.3452849738179014}\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7887585941144946, val_loss: 1.7877258355981611 (1 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.785982954782078, val_loss: 1.7848116212290497 (2 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.19704433497536947, train_loss: 1.7837268041886565, val_loss: 1.782029298138736 (3 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7821260082117558, val_loss: 1.77940324550779 (4 / 80)\n",
            "train_acc: 0.16563658838071693, val_acc: 0.18226600985221675, train_loss: 1.7808360979789857, val_loss: 1.7767154712395128 (5 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7786879697158398, val_loss: 1.774125811501677 (6 / 80)\n",
            "train_acc: 0.21631644004944375, val_acc: 0.18226600985221675, train_loss: 1.775055715268563, val_loss: 1.7714474465459438 (7 / 80)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.7734820630553332, val_loss: 1.7689472166775482 (8 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7741290254262823, val_loss: 1.7667156545986682 (9 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7707272959580675, val_loss: 1.764472509252614 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18046971569839307\n",
            "lr 6.87058653262615e-05, batch 11, decay 0.0002907904402306258, gamma 0.3452849738179014, val accuracy 0.19704433497536947, val loss 1.782029298138736 [1 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00032381259201563257, 'batch_size': 11, 'weight_decay': 0.00012890436972817662, 'gamma': 0.023213138693782234}\n",
            "train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.788759824519399, val_loss: 1.7848785581259892 (1 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7831569691965694, val_loss: 1.7776533294781087 (2 / 80)\n",
            "train_acc: 0.2088998763906057, val_acc: 0.18226600985221675, train_loss: 1.7766288991911596, val_loss: 1.7695574143837238 (3 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7676411016909832, val_loss: 1.7605839163211767 (4 / 80)\n",
            "train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7617450343958379, val_loss: 1.75310845328082 (5 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.755790419867366, val_loss: 1.7474357118747506 (6 / 80)\n",
            "train_acc: 0.20024721878862795, val_acc: 0.18226600985221675, train_loss: 1.7485467786399926, val_loss: 1.7410717292372229 (7 / 80)\n",
            "train_acc: 0.22373300370828184, val_acc: 0.18719211822660098, train_loss: 1.7451698794795203, val_loss: 1.7334059641279023 (8 / 80)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.24630541871921183, train_loss: 1.7307687411349548, val_loss: 1.721913890885602 (9 / 80)\n",
            "train_acc: 0.26328800988875156, val_acc: 0.19704433497536947, train_loss: 1.718468537584076, val_loss: 1.710659638414242 (10 / 80)\n",
            "train_acc: 0.27564894932014833, val_acc: 0.3399014778325123, train_loss: 1.7186331362895235, val_loss: 1.6872685771857576 (11 / 80)\n",
            "train_acc: 0.3226205191594561, val_acc: 0.27586206896551724, train_loss: 1.679874492664125, val_loss: 1.6422144855771745 (12 / 80)\n",
            "train_acc: 0.33250927070457353, val_acc: 0.3103448275862069, train_loss: 1.631629150641716, val_loss: 1.566756609038179 (13 / 80)\n",
            "train_acc: 0.3584672435105068, val_acc: 0.28078817733990147, train_loss: 1.5745559590413927, val_loss: 1.6197323147299254 (14 / 80)\n",
            "train_acc: 0.3522867737948084, val_acc: 0.3399014778325123, train_loss: 1.5650144181823258, val_loss: 1.4796721465481912 (15 / 80)\n",
            "train_acc: 0.3510506798516687, val_acc: 0.3399014778325123, train_loss: 1.5225654125803039, val_loss: 1.4708270503969616 (16 / 80)\n",
            "train_acc: 0.36093943139678614, val_acc: 0.3251231527093596, train_loss: 1.5149067360332191, val_loss: 1.4905134827045385 (17 / 80)\n",
            "train_acc: 0.37330037082818296, val_acc: 0.31527093596059114, train_loss: 1.4955695768074582, val_loss: 1.524568610003429 (18 / 80)\n",
            "train_acc: 0.34857849196538937, val_acc: 0.4236453201970443, train_loss: 1.4822473607045612, val_loss: 1.388589264724055 (19 / 80)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.3793103448275862, train_loss: 1.4546107345517398, val_loss: 1.3973773823583067 (20 / 80)\n",
            "train_acc: 0.38813349814585907, val_acc: 0.4088669950738916, train_loss: 1.4282049627327653, val_loss: 1.3735795041610455 (21 / 80)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.42857142857142855, train_loss: 1.4413129044109576, val_loss: 1.335635196692838 (22 / 80)\n",
            "train_acc: 0.415327564894932, val_acc: 0.3793103448275862, train_loss: 1.406516781327751, val_loss: 1.3972975608750517 (23 / 80)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.39901477832512317, train_loss: 1.4154124852311332, val_loss: 1.3766387407415606 (24 / 80)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.37438423645320196, train_loss: 1.401945825191894, val_loss: 1.3321593588796155 (25 / 80)\n",
            "train_acc: 0.40173053152039556, val_acc: 0.4236453201970443, train_loss: 1.403983973012719, val_loss: 1.3111175733246827 (26 / 80)\n",
            "train_acc: 0.3856613102595797, val_acc: 0.3793103448275862, train_loss: 1.4225272602587018, val_loss: 1.3415115267185156 (27 / 80)\n",
            "train_acc: 0.4400494437577256, val_acc: 0.43842364532019706, train_loss: 1.3754592304324043, val_loss: 1.3453965771374443 (28 / 80)\n",
            "train_acc: 0.4227441285537701, val_acc: 0.43349753694581283, train_loss: 1.3496772540072428, val_loss: 1.2862371535136783 (29 / 80)\n",
            "train_acc: 0.43510506798516685, val_acc: 0.43349753694581283, train_loss: 1.351313419115116, val_loss: 1.2706739059809982 (30 / 80)\n",
            "train_acc: 0.4004944375772559, val_acc: 0.43349753694581283, train_loss: 1.3788974368380675, val_loss: 1.2705026888495008 (31 / 80)\n",
            "train_acc: 0.4252163164400494, val_acc: 0.4729064039408867, train_loss: 1.3063695709991219, val_loss: 1.2802587107484564 (32 / 80)\n",
            "train_acc: 0.4363411619283066, val_acc: 0.4876847290640394, train_loss: 1.3295137229454974, val_loss: 1.2538200669687958 (33 / 80)\n",
            "train_acc: 0.4610630407911001, val_acc: 0.43842364532019706, train_loss: 1.3125536239928162, val_loss: 1.2372381311332064 (34 / 80)\n",
            "train_acc: 0.4746600741656366, val_acc: 0.458128078817734, train_loss: 1.2870506680350664, val_loss: 1.2268556494430956 (35 / 80)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.4187192118226601, train_loss: 1.3123210273655441, val_loss: 1.3045700330452379 (36 / 80)\n",
            "train_acc: 0.4783683559950556, val_acc: 0.458128078817734, train_loss: 1.2760416012169846, val_loss: 1.2551845376714696 (37 / 80)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.4630541871921182, train_loss: 1.2685753706064449, val_loss: 1.2768965102181646 (38 / 80)\n",
            "train_acc: 0.4758961681087763, val_acc: 0.4876847290640394, train_loss: 1.2668816015511124, val_loss: 1.2119491760953893 (39 / 80)\n",
            "train_acc: 0.48331273176761436, val_acc: 0.4433497536945813, train_loss: 1.2789893837735444, val_loss: 1.2570175886741413 (40 / 80)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.46798029556650245, train_loss: 1.2482793894038064, val_loss: 1.3703952628403462 (41 / 80)\n",
            "train_acc: 0.4894932014833127, val_acc: 0.4433497536945813, train_loss: 1.2394344774990058, val_loss: 1.234342000460977 (42 / 80)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.4876847290640394, train_loss: 1.2341456689557568, val_loss: 1.216400808888703 (43 / 80)\n",
            "train_acc: 0.5055624227441285, val_acc: 0.46798029556650245, train_loss: 1.2069260823564565, val_loss: 1.1647917205476996 (44 / 80)\n",
            "train_acc: 0.5067985166872683, val_acc: 0.4827586206896552, train_loss: 1.183660689478899, val_loss: 1.2856034698157475 (45 / 80)\n",
            "train_acc: 0.5290482076637825, val_acc: 0.45320197044334976, train_loss: 1.1838550482898471, val_loss: 1.2614115300436912 (46 / 80)\n",
            "train_acc: 0.5067985166872683, val_acc: 0.43349753694581283, train_loss: 1.2010821017847662, val_loss: 1.2713473069256749 (47 / 80)\n",
            "train_acc: 0.5253399258343634, val_acc: 0.47783251231527096, train_loss: 1.1804011419176024, val_loss: 1.1692165444637168 (48 / 80)\n",
            "train_acc: 0.5475896168108776, val_acc: 0.49261083743842365, train_loss: 1.1193662292435673, val_loss: 1.153833177289352 (49 / 80)\n",
            "train_acc: 0.5401730531520396, val_acc: 0.5024630541871922, train_loss: 1.1065636403186359, val_loss: 1.1519735041510295 (50 / 80)\n",
            "train_acc: 0.5166872682323856, val_acc: 0.5123152709359606, train_loss: 1.1309391584325632, val_loss: 1.142503213706275 (51 / 80)\n",
            "train_acc: 0.5562422744128553, val_acc: 0.5123152709359606, train_loss: 1.1112211085809913, val_loss: 1.141020753407126 (52 / 80)\n",
            "train_acc: 0.5488257107540173, val_acc: 0.5172413793103449, train_loss: 1.1289707660232988, val_loss: 1.1399509746746477 (53 / 80)\n",
            "train_acc: 0.5414091470951793, val_acc: 0.5172413793103449, train_loss: 1.107131454425925, val_loss: 1.1414725166823476 (54 / 80)\n",
            "train_acc: 0.5636588380716935, val_acc: 0.5123152709359606, train_loss: 1.1118016970909128, val_loss: 1.143481697061379 (55 / 80)\n",
            "train_acc: 0.5834363411619283, val_acc: 0.5024630541871922, train_loss: 1.0833184053046152, val_loss: 1.1368913844301196 (56 / 80)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.5073891625615764, train_loss: 1.0944939627046195, val_loss: 1.1364362040176768 (57 / 80)\n",
            "train_acc: 0.5525339925834364, val_acc: 0.5073891625615764, train_loss: 1.0837459521476358, val_loss: 1.1365543891643655 (58 / 80)\n",
            "train_acc: 0.546353522867738, val_acc: 0.5073891625615764, train_loss: 1.090269200144651, val_loss: 1.1376025635620644 (59 / 80)\n",
            "train_acc: 0.5451174289245982, val_acc: 0.5024630541871922, train_loss: 1.0875379037945467, val_loss: 1.141565755083056 (60 / 80)\n",
            "train_acc: 0.5500618046971569, val_acc: 0.5024630541871922, train_loss: 1.1008529789073505, val_loss: 1.1387394549224177 (61 / 80)\n",
            "train_acc: 0.5611866501854141, val_acc: 0.4975369458128079, train_loss: 1.1042318589460423, val_loss: 1.134943497885624 (62 / 80)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.5024630541871922, train_loss: 1.0962200186161235, val_loss: 1.134869571683442 (63 / 80)\n",
            "train_acc: 0.5673671199011124, val_acc: 0.5024630541871922, train_loss: 1.1014055059336318, val_loss: 1.1341883493174474 (64 / 80)\n",
            "train_acc: 0.5500618046971569, val_acc: 0.5024630541871922, train_loss: 1.0810771017935132, val_loss: 1.1320034576754265 (65 / 80)\n",
            "train_acc: 0.5772558714462299, val_acc: 0.4975369458128079, train_loss: 1.0715263902919991, val_loss: 1.1338478638033562 (66 / 80)\n",
            "train_acc: 0.5698393077873919, val_acc: 0.5024630541871922, train_loss: 1.0806852718366238, val_loss: 1.134693424983565 (67 / 80)\n",
            "train_acc: 0.5624227441285538, val_acc: 0.5073891625615764, train_loss: 1.0857884522126837, val_loss: 1.1354252355439323 (68 / 80)\n",
            "train_acc: 0.5599505562422744, val_acc: 0.5073891625615764, train_loss: 1.072859094228379, val_loss: 1.1330221862041305 (69 / 80)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.5024630541871922, train_loss: 1.1106265478287403, val_loss: 1.1326201120620878 (70 / 80)\n",
            "train_acc: 0.5512978986402967, val_acc: 0.5024630541871922, train_loss: 1.0774587395017314, val_loss: 1.1302289216976447 (71 / 80)\n",
            "train_acc: 0.5710754017305315, val_acc: 0.5024630541871922, train_loss: 1.0620560784569777, val_loss: 1.1288661918616647 (72 / 80)\n",
            "train_acc: 0.5723114956736712, val_acc: 0.5024630541871922, train_loss: 1.0738675069013248, val_loss: 1.134495311769946 (73 / 80)\n",
            "train_acc: 0.5562422744128553, val_acc: 0.5024630541871922, train_loss: 1.071173154821502, val_loss: 1.131242306068026 (74 / 80)\n",
            "train_acc: 0.584672435105068, val_acc: 0.5073891625615764, train_loss: 1.0578408160227337, val_loss: 1.1345570119730946 (75 / 80)\n",
            "train_acc: 0.5686032138442522, val_acc: 0.5024630541871922, train_loss: 1.0318389354441753, val_loss: 1.1384975590142123 (76 / 80)\n",
            "train_acc: 0.5673671199011124, val_acc: 0.5024630541871922, train_loss: 1.0628936851422484, val_loss: 1.1381935503682479 (77 / 80)\n",
            "train_acc: 0.5896168108776267, val_acc: 0.4975369458128079, train_loss: 1.0490736528881106, val_loss: 1.1383064703401087 (78 / 80)\n",
            "train_acc: 0.5970333745364648, val_acc: 0.4975369458128079, train_loss: 1.050982714657141, val_loss: 1.138759946588225 (79 / 80)\n",
            "train_acc: 0.5599505562422744, val_acc: 0.4975369458128079, train_loss: 1.0690692387640992, val_loss: 1.1355863454306654 (80 / 80)\n",
            "lr 0.00032381259201563257, batch 11, decay 0.00012890436972817662, gamma 0.023213138693782234, val accuracy 0.5172413793103449, val loss 1.1399509746746477 [2 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00010105746440697143, 'batch_size': 9, 'weight_decay': 7.168475186733455e-06, 'gamma': 0.6629497649106064}\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.789067440186207, val_loss: 1.7871722576066191 (1 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.1724137931034483, train_loss: 1.7864273537516742, val_loss: 1.7847367654293043 (2 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.17733990147783252, train_loss: 1.7838816944836686, val_loss: 1.7818673954808653 (3 / 80)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.20689655172413793, train_loss: 1.7812168431665163, val_loss: 1.7793491139200521 (4 / 80)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.20689655172413793, train_loss: 1.7786879157843194, val_loss: 1.7763204374924082 (5 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7768030318400474, val_loss: 1.7734263448292398 (6 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.774463122206359, val_loss: 1.7701088501314812 (7 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.771986294146374, val_loss: 1.7668335249858538 (8 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7676844714745898, val_loss: 1.7637061696921663 (9 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7645378316141325, val_loss: 1.7599302436330635 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1841779975278121\n",
            "lr 0.00010105746440697143, batch 9, decay 7.168475186733455e-06, gamma 0.6629497649106064, val accuracy 0.20689655172413793, val loss 1.7793491139200521 [3 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.000953839039916152, 'batch_size': 9, 'weight_decay': 0.00015319613136737035, 'gamma': 0.9262521929212527}\n",
            "train_acc: 0.1619283065512979, val_acc: 0.18226600985221675, train_loss: 1.7843165374067422, val_loss: 1.7708172668964404 (1 / 80)\n",
            "train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.7655035493105686, val_loss: 1.7479674604725954 (2 / 80)\n",
            "train_acc: 0.22991347342398022, val_acc: 0.3054187192118227, train_loss: 1.7520700083085428, val_loss: 1.7269958827295915 (3 / 80)\n",
            "train_acc: 0.2360939431396786, val_acc: 0.2512315270935961, train_loss: 1.7259858877608745, val_loss: 1.6628634677144694 (4 / 80)\n",
            "train_acc: 0.30778739184178, val_acc: 0.3103448275862069, train_loss: 1.6564312617769936, val_loss: 1.5577736816969998 (5 / 80)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.35467980295566504, train_loss: 1.6255239062609868, val_loss: 1.67831336571078 (6 / 80)\n",
            "train_acc: 0.3399258343634116, val_acc: 0.39901477832512317, train_loss: 1.5800065270755141, val_loss: 1.4451959597066117 (7 / 80)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.3891625615763547, train_loss: 1.5232941260738633, val_loss: 1.416880770857111 (8 / 80)\n",
            "train_acc: 0.3510506798516687, val_acc: 0.42857142857142855, train_loss: 1.4862068400836845, val_loss: 1.437072193094075 (9 / 80)\n",
            "train_acc: 0.3535228677379481, val_acc: 0.39901477832512317, train_loss: 1.4811254803123521, val_loss: 1.3871605854316298 (10 / 80)\n",
            "train_acc: 0.36711990111248455, val_acc: 0.32019704433497537, train_loss: 1.4483213461521087, val_loss: 1.4217302056368937 (11 / 80)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.33497536945812806, train_loss: 1.392251394956162, val_loss: 1.3533584220068795 (12 / 80)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.3793103448275862, train_loss: 1.3917552447878976, val_loss: 1.4445895179739139 (13 / 80)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.43349753694581283, train_loss: 1.3980080531760406, val_loss: 1.264357538646078 (14 / 80)\n",
            "train_acc: 0.41903584672435107, val_acc: 0.47783251231527096, train_loss: 1.3380119545645413, val_loss: 1.2456724725920578 (15 / 80)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.458128078817734, train_loss: 1.3532899060414365, val_loss: 1.2524754625235872 (16 / 80)\n",
            "train_acc: 0.45982694684796044, val_acc: 0.42857142857142855, train_loss: 1.2822939464718803, val_loss: 1.2742195728377168 (17 / 80)\n",
            "train_acc: 0.4388133498145859, val_acc: 0.43842364532019706, train_loss: 1.3062145637788054, val_loss: 1.2979426210736993 (18 / 80)\n",
            "train_acc: 0.446229913473424, val_acc: 0.4433497536945813, train_loss: 1.3008204361122235, val_loss: 1.2254492332195412 (19 / 80)\n",
            "train_acc: 0.44870210135970334, val_acc: 0.45320197044334976, train_loss: 1.2765251797415564, val_loss: 1.1887216465226536 (20 / 80)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.46798029556650245, train_loss: 1.220579541878883, val_loss: 1.2009116872190841 (21 / 80)\n",
            "train_acc: 0.47342398022249693, val_acc: 0.45320197044334976, train_loss: 1.2088771994388001, val_loss: 1.1675156146434729 (22 / 80)\n",
            "train_acc: 0.48084054388133496, val_acc: 0.5172413793103449, train_loss: 1.2321708146367585, val_loss: 1.1726773102295223 (23 / 80)\n",
            "train_acc: 0.5043263288009888, val_acc: 0.5172413793103449, train_loss: 1.1690235319184727, val_loss: 1.1385198490960258 (24 / 80)\n",
            "train_acc: 0.4932014833127318, val_acc: 0.5221674876847291, train_loss: 1.1694997893128318, val_loss: 1.1286629517677382 (25 / 80)\n",
            "train_acc: 0.5203955500618047, val_acc: 0.45320197044334976, train_loss: 1.1517868654247563, val_loss: 1.175784477165767 (26 / 80)\n",
            "train_acc: 0.5438813349814586, val_acc: 0.47783251231527096, train_loss: 1.1121123941483986, val_loss: 1.1247045186352846 (27 / 80)\n",
            "train_acc: 0.5451174289245982, val_acc: 0.5517241379310345, train_loss: 1.0523336397850027, val_loss: 1.219099949439758 (28 / 80)\n",
            "train_acc: 0.546353522867738, val_acc: 0.5665024630541872, train_loss: 1.0975021914439678, val_loss: 1.059812302366266 (29 / 80)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.4975369458128079, train_loss: 1.0614410124175775, val_loss: 1.0644254017933248 (30 / 80)\n",
            "train_acc: 0.5982694684796045, val_acc: 0.5517241379310345, train_loss: 0.9824870001105649, val_loss: 1.1160743897184362 (31 / 80)\n",
            "train_acc: 0.5970333745364648, val_acc: 0.3842364532019704, train_loss: 0.9544631979153094, val_loss: 1.3165174578798229 (32 / 80)\n",
            "train_acc: 0.5636588380716935, val_acc: 0.5665024630541872, train_loss: 0.9836446231758345, val_loss: 1.061188992021119 (33 / 80)\n",
            "train_acc: 0.65389369592089, val_acc: 0.541871921182266, train_loss: 0.8814733447633656, val_loss: 1.0081454135220627 (34 / 80)\n",
            "train_acc: 0.646477132262052, val_acc: 0.5714285714285714, train_loss: 0.8408737665569532, val_loss: 1.0947997106119918 (35 / 80)\n",
            "train_acc: 0.6440049443757726, val_acc: 0.6108374384236454, train_loss: 0.8527851865450737, val_loss: 0.965564813373124 (36 / 80)\n",
            "train_acc: 0.6773794808405439, val_acc: 0.5320197044334976, train_loss: 0.7801615160755233, val_loss: 1.1373400344637228 (37 / 80)\n",
            "train_acc: 0.6897404202719407, val_acc: 0.5517241379310345, train_loss: 0.746745854837196, val_loss: 1.3588215792414002 (38 / 80)\n",
            "train_acc: 0.7354758961681088, val_acc: 0.5763546798029556, train_loss: 0.6684505015022233, val_loss: 1.109989947139336 (39 / 80)\n",
            "train_acc: 0.7317676143386898, val_acc: 0.5812807881773399, train_loss: 0.6519573966434918, val_loss: 1.3616854762796111 (40 / 80)\n",
            "train_acc: 0.757725587144623, val_acc: 0.6059113300492611, train_loss: 0.6356143814929484, val_loss: 1.3224064221816698 (41 / 80)\n",
            "train_acc: 0.7799752781211372, val_acc: 0.6305418719211823, train_loss: 0.568807085784905, val_loss: 1.160102125679331 (42 / 80)\n",
            "train_acc: 0.8318912237330037, val_acc: 0.6403940886699507, train_loss: 0.4525507856045282, val_loss: 1.032604592994516 (43 / 80)\n",
            "train_acc: 0.8294190358467244, val_acc: 0.6354679802955665, train_loss: 0.46788043972238325, val_loss: 1.0510986533951876 (44 / 80)\n",
            "train_acc: 0.8665018541409147, val_acc: 0.6305418719211823, train_loss: 0.36674270030613, val_loss: 1.1558004913879145 (45 / 80)\n",
            "train_acc: 0.8529048207663782, val_acc: 0.6650246305418719, train_loss: 0.399278126610961, val_loss: 1.0745352168975792 (46 / 80)\n",
            "overfit -> train_accuracy-val_accuracy = 0.2754236514093298\n",
            "lr 0.000953839039916152, batch 9, decay 0.00015319613136737035, gamma 0.9262521929212527, val accuracy 0.6650246305418719, val loss 1.0745352168975792 [4 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0006444500508054211, 'batch_size': 14, 'weight_decay': 2.1280582227123365e-05, 'gamma': 0.19924404264743992}\n",
            "train_acc: 0.18541409147095178, val_acc: 0.2019704433497537, train_loss: 1.787222889798828, val_loss: 1.7792626043845867 (1 / 80)\n",
            "train_acc: 0.16563658838071693, val_acc: 0.1921182266009852, train_loss: 1.7763476816773858, val_loss: 1.7650453386635616 (2 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7601544238138846, val_loss: 1.7504996349071633 (3 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18719211822660098, train_loss: 1.7537464436848171, val_loss: 1.7423658001011815 (4 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18719211822660098, train_loss: 1.7451746896996043, val_loss: 1.7310849633710137 (5 / 80)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.22167487684729065, train_loss: 1.7384023071072157, val_loss: 1.7164962990530606 (6 / 80)\n",
            "train_acc: 0.25339925834363414, val_acc: 0.1921182266009852, train_loss: 1.7213749421394358, val_loss: 1.7045474175749153 (7 / 80)\n",
            "train_acc: 0.27812113720642767, val_acc: 0.3251231527093596, train_loss: 1.7036924455015563, val_loss: 1.6588450012535885 (8 / 80)\n",
            "train_acc: 0.3164400494437577, val_acc: 0.37438423645320196, train_loss: 1.649252489117079, val_loss: 1.5668467324355553 (9 / 80)\n",
            "train_acc: 0.3510506798516687, val_acc: 0.3399014778325123, train_loss: 1.5850570257868255, val_loss: 1.5274077160605068 (10 / 80)\n",
            "train_acc: 0.34857849196538937, val_acc: 0.37438423645320196, train_loss: 1.5427747144097892, val_loss: 1.5124577612712466 (11 / 80)\n",
            "train_acc: 0.35599505562422745, val_acc: 0.32019704433497537, train_loss: 1.5512995970411265, val_loss: 1.5868867800153534 (12 / 80)\n",
            "train_acc: 0.3473423980222497, val_acc: 0.3497536945812808, train_loss: 1.555502282526938, val_loss: 1.4769557755568932 (13 / 80)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.2561576354679803, train_loss: 1.4770200704909373, val_loss: 1.8925036561900173 (14 / 80)\n",
            "train_acc: 0.3535228677379481, val_acc: 0.3842364532019704, train_loss: 1.537875084411376, val_loss: 1.4012378002035206 (15 / 80)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.3399014778325123, train_loss: 1.4604699120975395, val_loss: 1.506527908917131 (16 / 80)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.35467980295566504, train_loss: 1.4340916007057256, val_loss: 1.5185675127752896 (17 / 80)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.3842364532019704, train_loss: 1.4395211963040573, val_loss: 1.3730327671971814 (18 / 80)\n",
            "train_acc: 0.4326328800988875, val_acc: 0.3891625615763547, train_loss: 1.3738763677175025, val_loss: 1.3312684831948116 (19 / 80)\n",
            "train_acc: 0.41903584672435107, val_acc: 0.4482758620689655, train_loss: 1.3767264574508289, val_loss: 1.3022156953811646 (20 / 80)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.42857142857142855, train_loss: 1.3915724064717334, val_loss: 1.299436984391048 (21 / 80)\n",
            "train_acc: 0.4103831891223733, val_acc: 0.45320197044334976, train_loss: 1.3846096771461855, val_loss: 1.2727831560989906 (22 / 80)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.4433497536945813, train_loss: 1.3467396804222513, val_loss: 1.265419943579312 (23 / 80)\n",
            "train_acc: 0.4289245982694685, val_acc: 0.3793103448275862, train_loss: 1.3601737239010108, val_loss: 1.3642062524269367 (24 / 80)\n",
            "train_acc: 0.4264524103831891, val_acc: 0.42857142857142855, train_loss: 1.335786685808038, val_loss: 1.2785878345884125 (25 / 80)\n",
            "train_acc: 0.43016069221260816, val_acc: 0.43842364532019706, train_loss: 1.3403208815712568, val_loss: 1.264502593155565 (26 / 80)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.4236453201970443, train_loss: 1.2789577535999426, val_loss: 1.2763923077747739 (27 / 80)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.3793103448275862, train_loss: 1.3201243102329476, val_loss: 1.3224709033966064 (28 / 80)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.4827586206896552, train_loss: 1.2736264495236618, val_loss: 1.228312714346524 (29 / 80)\n",
            "train_acc: 0.4561186650185414, val_acc: 0.4827586206896552, train_loss: 1.2923648538636632, val_loss: 1.1967953320207267 (30 / 80)\n",
            "train_acc: 0.4746600741656366, val_acc: 0.49261083743842365, train_loss: 1.2294056629220989, val_loss: 1.1863625090697716 (31 / 80)\n",
            "train_acc: 0.4783683559950556, val_acc: 0.4729064039408867, train_loss: 1.2671826901777714, val_loss: 1.2205022881770957 (32 / 80)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.45320197044334976, train_loss: 1.2245221230833434, val_loss: 1.2310690448201935 (33 / 80)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.49261083743842365, train_loss: 1.2072741372328901, val_loss: 1.2238941357053559 (34 / 80)\n",
            "train_acc: 0.5216316440049443, val_acc: 0.47783251231527096, train_loss: 1.1560617201260495, val_loss: 1.1895125035581917 (35 / 80)\n",
            "train_acc: 0.5080346106304079, val_acc: 0.45320197044334976, train_loss: 1.1739372881440209, val_loss: 1.1984874750005787 (36 / 80)\n",
            "train_acc: 0.515451174289246, val_acc: 0.4729064039408867, train_loss: 1.1849658424391587, val_loss: 1.182812657849542 (37 / 80)\n",
            "train_acc: 0.5488257107540173, val_acc: 0.4827586206896552, train_loss: 1.0996569210872957, val_loss: 1.1680588804442307 (38 / 80)\n",
            "train_acc: 0.5018541409147095, val_acc: 0.46798029556650245, train_loss: 1.179755399710463, val_loss: 1.156045173776561 (39 / 80)\n",
            "train_acc: 0.5339925834363412, val_acc: 0.4729064039408867, train_loss: 1.1203457184863475, val_loss: 1.132735803209502 (40 / 80)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.4433497536945813, train_loss: 1.1172435747825613, val_loss: 1.2293911226864518 (41 / 80)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.4827586206896552, train_loss: 1.1043445775181755, val_loss: 1.1996202551085373 (42 / 80)\n",
            "train_acc: 0.5475896168108776, val_acc: 0.5320197044334976, train_loss: 1.0737247892156963, val_loss: 1.0612037469600808 (43 / 80)\n",
            "train_acc: 0.5784919653893696, val_acc: 0.541871921182266, train_loss: 1.0420518560226828, val_loss: 1.0665424198939883 (44 / 80)\n",
            "train_acc: 0.5723114956736712, val_acc: 0.5073891625615764, train_loss: 1.0440986126992846, val_loss: 1.242678243538429 (45 / 80)\n",
            "train_acc: 0.595797280593325, val_acc: 0.5221674876847291, train_loss: 1.0113001997008164, val_loss: 1.0953919846436073 (46 / 80)\n",
            "train_acc: 0.588380716934487, val_acc: 0.5812807881773399, train_loss: 1.0109490018397826, val_loss: 1.0277004118623405 (47 / 80)\n",
            "train_acc: 0.595797280593325, val_acc: 0.5615763546798029, train_loss: 0.9877357492488158, val_loss: 1.0390881649379073 (48 / 80)\n",
            "train_acc: 0.6613102595797281, val_acc: 0.5566502463054187, train_loss: 0.9056266692866501, val_loss: 1.041390743748895 (49 / 80)\n",
            "train_acc: 0.6847960444993819, val_acc: 0.541871921182266, train_loss: 0.8282799451106558, val_loss: 1.0570352241910737 (50 / 80)\n",
            "train_acc: 0.6786155747836835, val_acc: 0.5665024630541872, train_loss: 0.8207695367751812, val_loss: 1.028143989628759 (51 / 80)\n",
            "train_acc: 0.6909765142150803, val_acc: 0.5615763546798029, train_loss: 0.8005277433781453, val_loss: 0.9992058790963272 (52 / 80)\n",
            "train_acc: 0.7082818294190358, val_acc: 0.5763546798029556, train_loss: 0.776104182041767, val_loss: 1.0135603814289487 (53 / 80)\n",
            "train_acc: 0.7107540173053152, val_acc: 0.5960591133004927, train_loss: 0.7555687693494507, val_loss: 1.0247759736817459 (54 / 80)\n",
            "train_acc: 0.6971569839307787, val_acc: 0.5615763546798029, train_loss: 0.757194256605707, val_loss: 1.0548285698068554 (55 / 80)\n",
            "train_acc: 0.7206427688504327, val_acc: 0.5665024630541872, train_loss: 0.7419022188714173, val_loss: 1.0752273551348983 (56 / 80)\n",
            "train_acc: 0.7070457354758962, val_acc: 0.6206896551724138, train_loss: 0.7512619475499072, val_loss: 1.0138117352436329 (57 / 80)\n",
            "train_acc: 0.723114956736712, val_acc: 0.5615763546798029, train_loss: 0.7123733114253163, val_loss: 1.1282346413053315 (58 / 80)\n",
            "train_acc: 0.7132262051915945, val_acc: 0.5911330049261084, train_loss: 0.7324233345033506, val_loss: 1.0951590229725015 (59 / 80)\n",
            "train_acc: 0.7317676143386898, val_acc: 0.5862068965517241, train_loss: 0.7002866710219602, val_loss: 1.0648832958320091 (60 / 80)\n",
            "train_acc: 0.7428924598269468, val_acc: 0.6059113300492611, train_loss: 0.678812905722407, val_loss: 1.0712166531332608 (61 / 80)\n",
            "train_acc: 0.7490729295426453, val_acc: 0.6059113300492611, train_loss: 0.6569333027113501, val_loss: 1.0585244647387801 (62 / 80)\n",
            "train_acc: 0.7552533992583437, val_acc: 0.5812807881773399, train_loss: 0.6231728311816901, val_loss: 1.0974382059327488 (63 / 80)\n",
            "train_acc: 0.7663782447466008, val_acc: 0.5960591133004927, train_loss: 0.6345368919178052, val_loss: 1.0251343044741401 (64 / 80)\n",
            "train_acc: 0.761433868974042, val_acc: 0.6305418719211823, train_loss: 0.6138986942205205, val_loss: 1.0403618915327664 (65 / 80)\n",
            "train_acc: 0.7775030902348579, val_acc: 0.5812807881773399, train_loss: 0.6046364948024561, val_loss: 1.0489437292362083 (66 / 80)\n",
            "train_acc: 0.7688504326328801, val_acc: 0.6157635467980296, train_loss: 0.6019679345144477, val_loss: 0.9912499353803438 (67 / 80)\n",
            "train_acc: 0.7935723114956736, val_acc: 0.6108374384236454, train_loss: 0.5833981561719708, val_loss: 1.0248023600413882 (68 / 80)\n",
            "train_acc: 0.7898640296662547, val_acc: 0.6206896551724138, train_loss: 0.5418609713373431, val_loss: 0.9862481807840282 (69 / 80)\n",
            "train_acc: 0.7911001236093943, val_acc: 0.5812807881773399, train_loss: 0.5428760911537189, val_loss: 1.1533048132370258 (70 / 80)\n",
            "train_acc: 0.8084054388133498, val_acc: 0.6305418719211823, train_loss: 0.5400103992599492, val_loss: 1.0390700451258956 (71 / 80)\n",
            "train_acc: 0.8034610630407911, val_acc: 0.5960591133004927, train_loss: 0.528687273689784, val_loss: 1.1654685119102741 (72 / 80)\n",
            "train_acc: 0.788627935723115, val_acc: 0.6157635467980296, train_loss: 0.5387095710992519, val_loss: 1.1068128532376782 (73 / 80)\n",
            "train_acc: 0.8084054388133498, val_acc: 0.5911330049261084, train_loss: 0.52215400697348, val_loss: 1.0810633075648342 (74 / 80)\n",
            "train_acc: 0.8182941903584673, val_acc: 0.6157635467980296, train_loss: 0.5075325077890024, val_loss: 1.0864405503560757 (75 / 80)\n",
            "train_acc: 0.823238566131026, val_acc: 0.6108374384236454, train_loss: 0.46471283370249056, val_loss: 1.1147774457931519 (76 / 80)\n",
            "train_acc: 0.8281829419035847, val_acc: 0.625615763546798, train_loss: 0.47437365454413244, val_loss: 1.0410206153475006 (77 / 80)\n",
            "train_acc: 0.8343634116192831, val_acc: 0.5960591133004927, train_loss: 0.44080895984865387, val_loss: 1.2352065731739175 (78 / 80)\n",
            "train_acc: 0.8417799752781211, val_acc: 0.6206896551724138, train_loss: 0.4301221124984425, val_loss: 1.0899376540348447 (79 / 80)\n",
            "train_acc: 0.8491965389369592, val_acc: 0.6108374384236454, train_loss: 0.4410361370948985, val_loss: 1.0437787154625202 (80 / 80)\n",
            "lr 0.0006444500508054211, batch 14, decay 2.1280582227123365e-05, gamma 0.19924404264743992, val accuracy 0.6305418719211823, val loss 1.0403618915327664 [5 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0001305446116361885, 'batch_size': 14, 'weight_decay': 0.00010370951313656249, 'gamma': 0.011122564036501074}\n",
            "train_acc: 0.17676143386897405, val_acc: 0.24630541871921183, train_loss: 1.7911253729915737, val_loss: 1.7889860991773934 (1 / 80)\n",
            "train_acc: 0.16563658838071693, val_acc: 0.18226600985221675, train_loss: 1.7896173013597543, val_loss: 1.786014540442105 (2 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7852197245997463, val_loss: 1.7829572784489598 (3 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7827301821101582, val_loss: 1.7797618282252345 (4 / 80)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.779560592472185, val_loss: 1.7766350055563038 (5 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7776200956997676, val_loss: 1.773538770346806 (6 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7759767928435863, val_loss: 1.7698548785571395 (7 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7714564200532157, val_loss: 1.76623168484918 (8 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7690501071614004, val_loss: 1.7626149901028336 (9 / 80)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.763103476266189, val_loss: 1.7586268638742382 (10 / 80)\n",
            "underfit -> train_accuracy = 0.19283065512978986\n",
            "lr 0.0001305446116361885, batch 14, decay 0.00010370951313656249, gamma 0.011122564036501074, val accuracy 0.24630541871921183, val loss 1.7889860991773934 [6 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 2.3278612890785774e-05, 'batch_size': 14, 'weight_decay': 7.589757136798515e-06, 'gamma': 0.09659098336434087}\n",
            "train_acc: 0.19901112484548825, val_acc: 0.16748768472906403, train_loss: 1.791314975587635, val_loss: 1.791052135927924 (1 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.1625615763546798, train_loss: 1.7908138889906875, val_loss: 1.7907539441667755 (2 / 80)\n",
            "train_acc: 0.16934487021013597, val_acc: 0.1724137931034483, train_loss: 1.7903956602176718, val_loss: 1.7904509388167282 (3 / 80)\n",
            "train_acc: 0.1668726823238566, val_acc: 0.19704433497536947, train_loss: 1.7909288083667072, val_loss: 1.7901354493765995 (4 / 80)\n",
            "train_acc: 0.17676143386897405, val_acc: 0.1921182266009852, train_loss: 1.7902444212044715, val_loss: 1.7898006233675727 (5 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.20689655172413793, train_loss: 1.7899467200961778, val_loss: 1.7895073068553005 (6 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.2019704433497537, train_loss: 1.789289941451753, val_loss: 1.7891784495320813 (7 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.19704433497536947, train_loss: 1.7894069895019342, val_loss: 1.7888674530489692 (8 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.2019704433497537, train_loss: 1.789302989637896, val_loss: 1.7885767262557457 (9 / 80)\n",
            "train_acc: 0.1668726823238566, val_acc: 0.19704433497536947, train_loss: 1.7886143996482726, val_loss: 1.7882703131642834 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1668726823238566\n",
            "lr 2.3278612890785774e-05, batch 14, decay 7.589757136798515e-06, gamma 0.09659098336434087, val accuracy 0.20689655172413793, val loss 1.7895073068553005 [7 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0009634199776520317, 'batch_size': 15, 'weight_decay': 1.9424564221617045e-05, 'gamma': 0.07349147747689912}\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.786665228152894, val_loss: 1.774316380763876 (1 / 80)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7669209924115534, val_loss: 1.7544501285834853 (2 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7585920702541715, val_loss: 1.7445145217068676 (3 / 80)\n",
            "train_acc: 0.21878862793572312, val_acc: 0.3645320197044335, train_loss: 1.7492692757890604, val_loss: 1.7311473339062018 (4 / 80)\n",
            "train_acc: 0.26452410383189123, val_acc: 0.23645320197044334, train_loss: 1.7323195565910952, val_loss: 1.7026332822339287 (5 / 80)\n",
            "train_acc: 0.2719406674907293, val_acc: 0.29064039408866993, train_loss: 1.704760705584796, val_loss: 1.6675647356240033 (6 / 80)\n",
            "train_acc: 0.315203955500618, val_acc: 0.3251231527093596, train_loss: 1.639566122527765, val_loss: 1.607150181173691 (7 / 80)\n",
            "train_acc: 0.33250927070457353, val_acc: 0.37438423645320196, train_loss: 1.5878085108711044, val_loss: 1.5244458032946282 (8 / 80)\n",
            "train_acc: 0.33250927070457353, val_acc: 0.2955665024630542, train_loss: 1.5556854061202154, val_loss: 1.579487159921618 (9 / 80)\n",
            "train_acc: 0.3535228677379481, val_acc: 0.29064039408866993, train_loss: 1.5216060648447798, val_loss: 1.6251128066349498 (10 / 80)\n",
            "train_acc: 0.36711990111248455, val_acc: 0.39901477832512317, train_loss: 1.4753642908869626, val_loss: 1.4044798706552666 (11 / 80)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.37438423645320196, train_loss: 1.4392656518885467, val_loss: 1.3746400390352522 (12 / 80)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.3399014778325123, train_loss: 1.4287441637371614, val_loss: 1.3460592659823414 (13 / 80)\n",
            "train_acc: 0.3980222496909765, val_acc: 0.31527093596059114, train_loss: 1.4014112048302356, val_loss: 1.4415982885313738 (14 / 80)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.4236453201970443, train_loss: 1.4576514377287497, val_loss: 1.3479073059382698 (15 / 80)\n",
            "train_acc: 0.3943139678615575, val_acc: 0.3842364532019704, train_loss: 1.41751869389389, val_loss: 1.316191076645123 (16 / 80)\n",
            "train_acc: 0.4042027194066749, val_acc: 0.3645320197044335, train_loss: 1.3771791864826595, val_loss: 1.3648258735393655 (17 / 80)\n",
            "train_acc: 0.4215080346106304, val_acc: 0.41379310344827586, train_loss: 1.349396808701481, val_loss: 1.2892836802111471 (18 / 80)\n",
            "train_acc: 0.40667490729295425, val_acc: 0.32019704433497537, train_loss: 1.375269352063555, val_loss: 1.4318600288165615 (19 / 80)\n",
            "train_acc: 0.4363411619283066, val_acc: 0.4236453201970443, train_loss: 1.3248776650546654, val_loss: 1.288839027799409 (20 / 80)\n",
            "train_acc: 0.4647713226205192, val_acc: 0.41379310344827586, train_loss: 1.3024861326323747, val_loss: 1.2919705607033716 (21 / 80)\n",
            "train_acc: 0.4400494437577256, val_acc: 0.458128078817734, train_loss: 1.3233630705822825, val_loss: 1.2453127794077832 (22 / 80)\n",
            "train_acc: 0.453646477132262, val_acc: 0.4433497536945813, train_loss: 1.2745911145976507, val_loss: 1.225804489821636 (23 / 80)\n",
            "train_acc: 0.4437577255871446, val_acc: 0.4630541871921182, train_loss: 1.262476594766374, val_loss: 1.2328006296322263 (24 / 80)\n",
            "train_acc: 0.45488257107540175, val_acc: 0.43349753694581283, train_loss: 1.2560162710760374, val_loss: 1.2900656664312766 (25 / 80)\n",
            "train_acc: 0.48825710754017304, val_acc: 0.4827586206896552, train_loss: 1.2552694027002427, val_loss: 1.2109088621703274 (26 / 80)\n",
            "train_acc: 0.47095179233621753, val_acc: 0.43842364532019706, train_loss: 1.2827296893587807, val_loss: 1.2183743867968104 (27 / 80)\n",
            "train_acc: 0.48084054388133496, val_acc: 0.5024630541871922, train_loss: 1.2298771252738236, val_loss: 1.1668280383636211 (28 / 80)\n",
            "train_acc: 0.5092707045735476, val_acc: 0.4630541871921182, train_loss: 1.2149536407774841, val_loss: 1.2436247600123214 (29 / 80)\n",
            "train_acc: 0.5067985166872683, val_acc: 0.4876847290640394, train_loss: 1.179842229826636, val_loss: 1.1917519939356838 (30 / 80)\n",
            "train_acc: 0.48331273176761436, val_acc: 0.47783251231527096, train_loss: 1.177790672655306, val_loss: 1.1682661478155352 (31 / 80)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.49261083743842365, train_loss: 1.1074474403088999, val_loss: 1.1727781800800943 (32 / 80)\n",
            "train_acc: 0.5067985166872683, val_acc: 0.4876847290640394, train_loss: 1.1572481330451918, val_loss: 1.218942379716582 (33 / 80)\n",
            "train_acc: 0.5426452410383189, val_acc: 0.4975369458128079, train_loss: 1.0709693049470927, val_loss: 1.1478605863496 (34 / 80)\n",
            "train_acc: 0.5414091470951793, val_acc: 0.49261083743842365, train_loss: 1.093535809714361, val_loss: 1.1059578862683526 (35 / 80)\n",
            "train_acc: 0.5772558714462299, val_acc: 0.49261083743842365, train_loss: 1.0416084498498583, val_loss: 1.0926710082392388 (36 / 80)\n",
            "train_acc: 0.588380716934487, val_acc: 0.4876847290640394, train_loss: 1.0093203555373533, val_loss: 1.16413760655032 (37 / 80)\n",
            "train_acc: 0.5698393077873919, val_acc: 0.5024630541871922, train_loss: 1.0421229617852068, val_loss: 1.0747496277240698 (38 / 80)\n",
            "train_acc: 0.584672435105068, val_acc: 0.5369458128078818, train_loss: 0.9593849137185972, val_loss: 1.1949351051170838 (39 / 80)\n",
            "train_acc: 0.6069221260815822, val_acc: 0.49261083743842365, train_loss: 1.0171635249636524, val_loss: 1.1452754841649473 (40 / 80)\n",
            "train_acc: 0.6143386897404203, val_acc: 0.5073891625615764, train_loss: 0.9526032627144614, val_loss: 1.109876732814488 (41 / 80)\n",
            "train_acc: 0.6180469715698393, val_acc: 0.5172413793103449, train_loss: 0.9361037322410989, val_loss: 1.0476969348386003 (42 / 80)\n",
            "train_acc: 0.6588380716934487, val_acc: 0.5073891625615764, train_loss: 0.8547297485531924, val_loss: 1.2476113623586194 (43 / 80)\n",
            "train_acc: 0.6761433868974042, val_acc: 0.5172413793103449, train_loss: 0.8458620588328547, val_loss: 1.153444245237435 (44 / 80)\n",
            "train_acc: 0.6674907292954264, val_acc: 0.5911330049261084, train_loss: 0.835431748805441, val_loss: 1.0313909115462467 (45 / 80)\n",
            "train_acc: 0.6749072929542645, val_acc: 0.5270935960591133, train_loss: 0.8040402977737714, val_loss: 1.0488723375527143 (46 / 80)\n",
            "train_acc: 0.7206427688504327, val_acc: 0.6108374384236454, train_loss: 0.7424383254074786, val_loss: 1.106118556901152 (47 / 80)\n",
            "train_acc: 0.7391841779975278, val_acc: 0.5763546798029556, train_loss: 0.6779079132233327, val_loss: 1.123381285831846 (48 / 80)\n",
            "train_acc: 0.788627935723115, val_acc: 0.6403940886699507, train_loss: 0.5921969447648717, val_loss: 0.9407640683827142 (49 / 80)\n",
            "train_acc: 0.8170580964153276, val_acc: 0.625615763546798, train_loss: 0.508535963422141, val_loss: 0.9636944830417633 (50 / 80)\n",
            "train_acc: 0.8331273176761433, val_acc: 0.6009852216748769, train_loss: 0.5013664025348256, val_loss: 1.0107342264628763 (51 / 80)\n",
            "train_acc: 0.8294190358467244, val_acc: 0.5960591133004927, train_loss: 0.46167221361465005, val_loss: 1.008301425156335 (52 / 80)\n",
            "overfit -> train_accuracy-val_accuracy = 0.2518830642951524\n",
            "lr 0.0009634199776520317, batch 15, decay 1.9424564221617045e-05, gamma 0.07349147747689912, val accuracy 0.6403940886699507, val loss 0.9407640683827142 [8 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00010189554549947586, 'batch_size': 13, 'weight_decay': 3.7938384472088194e-05, 'gamma': 0.09548688661589924}\n",
            "train_acc: 0.1841779975278121, val_acc: 0.15763546798029557, train_loss: 1.7910205140544104, val_loss: 1.7904848882130213 (1 / 80)\n",
            "train_acc: 0.17428924598269468, val_acc: 0.1724137931034483, train_loss: 1.7898006425947135, val_loss: 1.7883698035930764 (2 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.1724137931034483, train_loss: 1.7881470473646675, val_loss: 1.7866255150639951 (3 / 80)\n",
            "train_acc: 0.16440049443757726, val_acc: 0.15763546798029557, train_loss: 1.7856369581446512, val_loss: 1.7845543229521201 (4 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.16748768472906403, train_loss: 1.7839533962926404, val_loss: 1.7826210407200704 (5 / 80)\n",
            "train_acc: 0.16440049443757726, val_acc: 0.19704433497536947, train_loss: 1.783540470933148, val_loss: 1.7806962082538698 (6 / 80)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.19704433497536947, train_loss: 1.7807572909132365, val_loss: 1.7786100926657615 (7 / 80)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.18226600985221675, train_loss: 1.7794604294084944, val_loss: 1.7765302499526827 (8 / 80)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.19704433497536947, train_loss: 1.7768926784046206, val_loss: 1.7742538716405483 (9 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18719211822660098, train_loss: 1.7750525310985532, val_loss: 1.7719405954107275 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18788627935723115\n",
            "lr 0.00010189554549947586, batch 13, decay 3.7938384472088194e-05, gamma 0.09548688661589924, val accuracy 0.19704433497536947, val loss 1.7806962082538698 [9 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00017744405536419445, 'batch_size': 9, 'weight_decay': 0.00012317359545354354, 'gamma': 0.845852187456105}\n",
            "train_acc: 0.17181705809641531, val_acc: 0.18226600985221675, train_loss: 1.7916947212737924, val_loss: 1.7892627322615073 (1 / 80)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.21674876847290642, train_loss: 1.7872253833211986, val_loss: 1.784052419545028 (2 / 80)\n",
            "train_acc: 0.20395550061804696, val_acc: 0.18226600985221675, train_loss: 1.7816941341746724, val_loss: 1.7787476090962075 (3 / 80)\n",
            "train_acc: 0.21878862793572312, val_acc: 0.19704433497536947, train_loss: 1.7790260401880196, val_loss: 1.7734342320212002 (4 / 80)\n",
            "train_acc: 0.2138442521631644, val_acc: 0.19704433497536947, train_loss: 1.7736998784674702, val_loss: 1.7675559491359543 (5 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.19704433497536947, train_loss: 1.7673112196739584, val_loss: 1.7611320112726372 (6 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7639878858594575, val_loss: 1.7561942661924315 (7 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7599243861341063, val_loss: 1.7515102730596006 (8 / 80)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.18719211822660098, train_loss: 1.7611129714178362, val_loss: 1.7480444502947954 (9 / 80)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.19704433497536947, train_loss: 1.7528144352515638, val_loss: 1.7441711848592523 (10 / 80)\n",
            "underfit -> train_accuracy = 0.2027194066749073\n",
            "lr 0.00017744405536419445, batch 9, decay 0.00012317359545354354, gamma 0.845852187456105, val accuracy 0.21674876847290642, val loss 1.784052419545028 [10 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0002363879745266706, 'batch_size': 15, 'weight_decay': 1.2859972654017638e-06, 'gamma': 0.0727587904402118}\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7891807382274765, val_loss: 1.7869080610463184 (1 / 80)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.1921182266009852, train_loss: 1.785039386878939, val_loss: 1.7816807748061683 (2 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7809068900250975, val_loss: 1.7759456141241665 (3 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.775704463715901, val_loss: 1.7704732394570788 (4 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7718106013145964, val_loss: 1.7648393602794028 (5 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7657020041910179, val_loss: 1.758992694281592 (6 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7593911507515736, val_loss: 1.7534237236812198 (7 / 80)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7582281615737048, val_loss: 1.7492405293610296 (8 / 80)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.18226600985221675, train_loss: 1.7539863371289115, val_loss: 1.7456974325508907 (9 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.751209494209997, val_loss: 1.741910687221095 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1915945611866502\n",
            "lr 0.0002363879745266706, batch 15, decay 1.2859972654017638e-06, gamma 0.0727587904402118, val accuracy 0.1921182266009852, val loss 1.7816807748061683 [11 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0005465334588837794, 'batch_size': 8, 'weight_decay': 1.711446646796338e-06, 'gamma': 0.584732316549937}\n",
            "train_acc: 0.1668726823238566, val_acc: 0.18719211822660098, train_loss: 1.7836723259853933, val_loss: 1.7714930473290054 (1 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18719211822660098, train_loss: 1.7695924120279414, val_loss: 1.7545736851950584 (2 / 80)\n",
            "train_acc: 0.21508034610630408, val_acc: 0.2512315270935961, train_loss: 1.7630677853880177, val_loss: 1.742377163741389 (3 / 80)\n",
            "train_acc: 0.21631644004944375, val_acc: 0.2019704433497537, train_loss: 1.7446508775977476, val_loss: 1.7219984584253998 (4 / 80)\n",
            "train_acc: 0.2558714462299135, val_acc: 0.3054187192118227, train_loss: 1.7237332441898152, val_loss: 1.6890023347779448 (5 / 80)\n",
            "train_acc: 0.2954264524103832, val_acc: 0.2955665024630542, train_loss: 1.6778607658756384, val_loss: 1.6109397311515996 (6 / 80)\n",
            "train_acc: 0.33498145859085293, val_acc: 0.3103448275862069, train_loss: 1.608709555768554, val_loss: 1.5618932135586667 (7 / 80)\n",
            "train_acc: 0.33868974042027195, val_acc: 0.2413793103448276, train_loss: 1.5818598880461325, val_loss: 1.8779087865293906 (8 / 80)\n",
            "train_acc: 0.32509270704573545, val_acc: 0.33497536945812806, train_loss: 1.5870682897025488, val_loss: 1.5722151237168336 (9 / 80)\n",
            "train_acc: 0.35970333745364647, val_acc: 0.35960591133004927, train_loss: 1.5160533712879836, val_loss: 1.4858352809116757 (10 / 80)\n",
            "train_acc: 0.34363411619283063, val_acc: 0.35960591133004927, train_loss: 1.5284189279058809, val_loss: 1.4786360205100675 (11 / 80)\n",
            "train_acc: 0.3856613102595797, val_acc: 0.3694581280788177, train_loss: 1.4754494512626355, val_loss: 1.4851975323531428 (12 / 80)\n",
            "train_acc: 0.3856613102595797, val_acc: 0.35467980295566504, train_loss: 1.4907377729899214, val_loss: 1.5056374008432398 (13 / 80)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.35960591133004927, train_loss: 1.4421964519840236, val_loss: 1.4627277792380948 (14 / 80)\n",
            "train_acc: 0.3621755253399258, val_acc: 0.35467980295566504, train_loss: 1.4651580714472145, val_loss: 1.4609584156515563 (15 / 80)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.4236453201970443, train_loss: 1.4553415719304597, val_loss: 1.358055942164266 (16 / 80)\n",
            "train_acc: 0.411619283065513, val_acc: 0.3842364532019704, train_loss: 1.3844294357948161, val_loss: 1.3460659886815864 (17 / 80)\n",
            "train_acc: 0.40173053152039556, val_acc: 0.41379310344827586, train_loss: 1.3872943897035124, val_loss: 1.3264535242700812 (18 / 80)\n",
            "train_acc: 0.4004944375772559, val_acc: 0.37438423645320196, train_loss: 1.367954702400896, val_loss: 1.3701828744611129 (19 / 80)\n",
            "train_acc: 0.4004944375772559, val_acc: 0.3448275862068966, train_loss: 1.376709552866272, val_loss: 1.479715694347626 (20 / 80)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.4482758620689655, train_loss: 1.3540345452477227, val_loss: 1.2934694994846587 (21 / 80)\n",
            "train_acc: 0.45859085290482077, val_acc: 0.458128078817734, train_loss: 1.3375207390567134, val_loss: 1.3508591757619322 (22 / 80)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.4187192118226601, train_loss: 1.3246232720623794, val_loss: 1.328382266566084 (23 / 80)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.4187192118226601, train_loss: 1.2731987735986414, val_loss: 1.3638565340652842 (24 / 80)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.4187192118226601, train_loss: 1.2845582263578739, val_loss: 1.3952607421452188 (25 / 80)\n",
            "train_acc: 0.47713226205191595, val_acc: 0.4729064039408867, train_loss: 1.2337107755639791, val_loss: 1.3143641003246964 (26 / 80)\n",
            "train_acc: 0.4561186650185414, val_acc: 0.458128078817734, train_loss: 1.2466795317764188, val_loss: 1.268025020954057 (27 / 80)\n",
            "train_acc: 0.4857849196538937, val_acc: 0.4088669950738916, train_loss: 1.2094642563420261, val_loss: 1.3368873508105725 (28 / 80)\n",
            "train_acc: 0.5105067985166872, val_acc: 0.4827586206896552, train_loss: 1.2045944730195186, val_loss: 1.2167328666583659 (29 / 80)\n",
            "train_acc: 0.5006180469715699, val_acc: 0.4975369458128079, train_loss: 1.1824299499337252, val_loss: 1.1383853128978185 (30 / 80)\n",
            "train_acc: 0.511742892459827, val_acc: 0.4975369458128079, train_loss: 1.1705051429192126, val_loss: 1.1762634580358495 (31 / 80)\n",
            "train_acc: 0.5253399258343634, val_acc: 0.458128078817734, train_loss: 1.1404973420283409, val_loss: 1.39688052890336 (32 / 80)\n",
            "train_acc: 0.5438813349814586, val_acc: 0.49261083743842365, train_loss: 1.1272150258935427, val_loss: 1.1048166875181527 (33 / 80)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.5369458128078818, train_loss: 1.0772371799011609, val_loss: 1.0812879278154797 (34 / 80)\n",
            "train_acc: 0.5822002472187886, val_acc: 0.5024630541871922, train_loss: 1.0555276461083751, val_loss: 1.0840413176954673 (35 / 80)\n",
            "train_acc: 0.6032138442521632, val_acc: 0.5221674876847291, train_loss: 1.0096365928060487, val_loss: 1.0834341119662882 (36 / 80)\n",
            "train_acc: 0.6143386897404203, val_acc: 0.5320197044334976, train_loss: 0.9831471213304216, val_loss: 1.0630706266816614 (37 / 80)\n",
            "train_acc: 0.6365883807169345, val_acc: 0.4975369458128079, train_loss: 0.9130582859548562, val_loss: 1.1709376520711212 (38 / 80)\n",
            "train_acc: 0.5908529048207664, val_acc: 0.5566502463054187, train_loss: 1.005147853504742, val_loss: 1.1434824936495627 (39 / 80)\n",
            "train_acc: 0.6291718170580964, val_acc: 0.5024630541871922, train_loss: 0.9283118987702321, val_loss: 1.1501974877465535 (40 / 80)\n",
            "train_acc: 0.6724351050679852, val_acc: 0.5566502463054187, train_loss: 0.8353607654571533, val_loss: 1.067342251098802 (41 / 80)\n",
            "train_acc: 0.6328800988875154, val_acc: 0.5024630541871922, train_loss: 0.8628874850656253, val_loss: 1.1586129862099446 (42 / 80)\n",
            "train_acc: 0.7008652657601978, val_acc: 0.5911330049261084, train_loss: 0.7434219278128097, val_loss: 0.9477890726967986 (43 / 80)\n",
            "train_acc: 0.7354758961681088, val_acc: 0.5763546798029556, train_loss: 0.706371915355159, val_loss: 1.0445652237079415 (44 / 80)\n",
            "train_acc: 0.7503090234857849, val_acc: 0.6305418719211823, train_loss: 0.641275816411701, val_loss: 0.9919763516469542 (45 / 80)\n",
            "train_acc: 0.765142150803461, val_acc: 0.6354679802955665, train_loss: 0.6334164693712157, val_loss: 1.0427736769168836 (46 / 80)\n",
            "train_acc: 0.8133498145859085, val_acc: 0.6157635467980296, train_loss: 0.547641984759214, val_loss: 1.0603813656738825 (47 / 80)\n",
            "train_acc: 0.7898640296662547, val_acc: 0.5862068965517241, train_loss: 0.552942528860236, val_loss: 1.0078091777016964 (48 / 80)\n",
            "train_acc: 0.8405438813349815, val_acc: 0.6059113300492611, train_loss: 0.4203287769749079, val_loss: 1.1467087944152907 (49 / 80)\n",
            "train_acc: 0.8800988875154512, val_acc: 0.645320197044335, train_loss: 0.35731089188819765, val_loss: 1.0584736058277449 (50 / 80)\n",
            "train_acc: 0.8677379480840544, val_acc: 0.6305418719211823, train_loss: 0.3403515865835183, val_loss: 1.2486861762154866 (51 / 80)\n",
            "overfit -> train_accuracy-val_accuracy = 0.2507565747410596\n",
            "lr 0.0005465334588837794, batch 8, decay 1.711446646796338e-06, gamma 0.584732316549937, val accuracy 0.645320197044335, val loss 1.0584736058277449 [12 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0008670113918889819, 'batch_size': 11, 'weight_decay': 0.0003892040977696076, 'gamma': 0.02508334820776559}\n",
            "train_acc: 0.207663782447466, val_acc: 0.18226600985221675, train_loss: 1.7783415731599805, val_loss: 1.7632855987313933 (1 / 80)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.21182266009852216, train_loss: 1.7647181528313052, val_loss: 1.7519341519313494 (2 / 80)\n",
            "train_acc: 0.22249690976514216, val_acc: 0.18226600985221675, train_loss: 1.7495299511403766, val_loss: 1.7333315634375135 (3 / 80)\n",
            "train_acc: 0.2373300370828183, val_acc: 0.26108374384236455, train_loss: 1.732881354165755, val_loss: 1.7121631635233687 (4 / 80)\n",
            "train_acc: 0.2719406674907293, val_acc: 0.2019704433497537, train_loss: 1.6923270184265815, val_loss: 1.8312842017911337 (5 / 80)\n",
            "train_acc: 0.30778739184178, val_acc: 0.3399014778325123, train_loss: 1.6624587747753035, val_loss: 1.5463856811006669 (6 / 80)\n",
            "train_acc: 0.34610630407911, val_acc: 0.3399014778325123, train_loss: 1.594323554056684, val_loss: 1.5236128885757747 (7 / 80)\n",
            "train_acc: 0.34363411619283063, val_acc: 0.35960591133004927, train_loss: 1.5680036423969623, val_loss: 1.481627111951706 (8 / 80)\n",
            "train_acc: 0.380716934487021, val_acc: 0.28078817733990147, train_loss: 1.5002506162386446, val_loss: 1.566476924078805 (9 / 80)\n",
            "train_acc: 0.37330037082818296, val_acc: 0.3251231527093596, train_loss: 1.50512472351343, val_loss: 1.469620294171601 (10 / 80)\n",
            "train_acc: 0.38813349814585907, val_acc: 0.3793103448275862, train_loss: 1.4730796800702994, val_loss: 1.40218427322181 (11 / 80)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.3793103448275862, train_loss: 1.4468655945786144, val_loss: 1.3995197236244314 (12 / 80)\n",
            "train_acc: 0.39555006180469715, val_acc: 0.37438423645320196, train_loss: 1.489084980691172, val_loss: 1.3805601849344564 (13 / 80)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.3251231527093596, train_loss: 1.4098156391469157, val_loss: 1.3892290210488982 (14 / 80)\n",
            "train_acc: 0.40173053152039556, val_acc: 0.4187192118226601, train_loss: 1.4067143565792088, val_loss: 1.383583631127926 (15 / 80)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.4039408866995074, train_loss: 1.385444401589548, val_loss: 1.3232481045088744 (16 / 80)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.4433497536945813, train_loss: 1.3761696170817495, val_loss: 1.2858427548643403 (17 / 80)\n",
            "train_acc: 0.415327564894932, val_acc: 0.4630541871921182, train_loss: 1.3421032635773658, val_loss: 1.2543461701552856 (18 / 80)\n",
            "train_acc: 0.4215080346106304, val_acc: 0.4482758620689655, train_loss: 1.3305145552927542, val_loss: 1.2893194418235365 (19 / 80)\n",
            "train_acc: 0.4326328800988875, val_acc: 0.45320197044334976, train_loss: 1.297895181532696, val_loss: 1.2630238163060155 (20 / 80)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.42857142857142855, train_loss: 1.306460084225545, val_loss: 1.29711537701743 (21 / 80)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.5024630541871922, train_loss: 1.2721348597181152, val_loss: 1.219912954151924 (22 / 80)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.43842364532019706, train_loss: 1.2346195761293945, val_loss: 1.2631108056148286 (23 / 80)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.42857142857142855, train_loss: 1.2729093651983736, val_loss: 1.2328880728759202 (24 / 80)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.5073891625615764, train_loss: 1.2104548503942927, val_loss: 1.149159736821217 (25 / 80)\n",
            "train_acc: 0.48702101359703337, val_acc: 0.4236453201970443, train_loss: 1.2123926821980988, val_loss: 1.2748114173048235 (26 / 80)\n",
            "train_acc: 0.48825710754017304, val_acc: 0.4975369458128079, train_loss: 1.1801144205890273, val_loss: 1.2099745065120642 (27 / 80)\n",
            "train_acc: 0.5092707045735476, val_acc: 0.5073891625615764, train_loss: 1.1720385376101519, val_loss: 1.1498881613679708 (28 / 80)\n",
            "train_acc: 0.5030902348578492, val_acc: 0.5073891625615764, train_loss: 1.1594995442589076, val_loss: 1.135923074090422 (29 / 80)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.5270935960591133, train_loss: 1.1271902963021776, val_loss: 1.1496396443526733 (30 / 80)\n",
            "train_acc: 0.511742892459827, val_acc: 0.5566502463054187, train_loss: 1.117115535164351, val_loss: 1.1036048117529582 (31 / 80)\n",
            "train_acc: 0.5710754017305315, val_acc: 0.5221674876847291, train_loss: 1.0751097761509003, val_loss: 1.0736036250743959 (32 / 80)\n",
            "train_acc: 0.553770086526576, val_acc: 0.4975369458128079, train_loss: 1.0722036153188594, val_loss: 1.201435373040843 (33 / 80)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.5320197044334976, train_loss: 1.0635257375844478, val_loss: 1.1671878669062272 (34 / 80)\n",
            "train_acc: 0.5920889987639061, val_acc: 0.4729064039408867, train_loss: 1.0029864924210405, val_loss: 1.3422290433216564 (35 / 80)\n",
            "train_acc: 0.6143386897404203, val_acc: 0.5172413793103449, train_loss: 0.9471875280694407, val_loss: 1.066661687613708 (36 / 80)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.541871921182266, train_loss: 0.9486126831568068, val_loss: 1.0393799334911291 (37 / 80)\n",
            "train_acc: 0.6440049443757726, val_acc: 0.5320197044334976, train_loss: 0.8966210511028398, val_loss: 1.0827014364045242 (38 / 80)\n",
            "train_acc: 0.6489493201483313, val_acc: 0.541871921182266, train_loss: 0.8638875182759482, val_loss: 1.1010513340898336 (39 / 80)\n",
            "train_acc: 0.6526576019777504, val_acc: 0.4876847290640394, train_loss: 0.8737510092399913, val_loss: 1.173086239492952 (40 / 80)\n",
            "train_acc: 0.681087762669963, val_acc: 0.5911330049261084, train_loss: 0.8306006381405593, val_loss: 1.0060809524775727 (41 / 80)\n",
            "train_acc: 0.6860321384425216, val_acc: 0.5566502463054187, train_loss: 0.7897210343843337, val_loss: 1.0611242624045594 (42 / 80)\n",
            "train_acc: 0.7144622991347342, val_acc: 0.5960591133004927, train_loss: 0.7394109100140217, val_loss: 1.0160163582252164 (43 / 80)\n",
            "train_acc: 0.7404202719406675, val_acc: 0.6157635467980296, train_loss: 0.6973929629742878, val_loss: 1.0746936818649029 (44 / 80)\n",
            "train_acc: 0.7466007416563659, val_acc: 0.6009852216748769, train_loss: 0.6457612467342608, val_loss: 1.0078718637304354 (45 / 80)\n",
            "train_acc: 0.7700865265760197, val_acc: 0.5911330049261084, train_loss: 0.5776814613007791, val_loss: 1.056036905114874 (46 / 80)\n",
            "overfit -> train_accuracy-val_accuracy = 0.25419084559786154\n",
            "lr 0.0008670113918889819, batch 11, decay 0.0003892040977696076, gamma 0.02508334820776559, val accuracy 0.6157635467980296, val loss 1.0746936818649029 [13 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 3.3042208394929954e-05, 'batch_size': 9, 'weight_decay': 2.2544953140818472e-05, 'gamma': 0.1726830675807015}\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7915298629308072, val_loss: 1.7906943829776032 (1 / 80)\n",
            "train_acc: 0.16069221260815822, val_acc: 0.18226600985221675, train_loss: 1.79069139046781, val_loss: 1.7897180571344686 (2 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7900462038584781, val_loss: 1.7887903827751799 (3 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7891896599155421, val_loss: 1.7878265739074481 (4 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7886277745179693, val_loss: 1.7868581587457892 (5 / 80)\n",
            "train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7884019938918068, val_loss: 1.7859662242710883 (6 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.786817290726934, val_loss: 1.7849784420041614 (7 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7862159759358807, val_loss: 1.7840587669992682 (8 / 80)\n",
            "train_acc: 0.22744128553770088, val_acc: 0.18226600985221675, train_loss: 1.784831752146425, val_loss: 1.7829637644913396 (9 / 80)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.18719211822660098, train_loss: 1.783462997125312, val_loss: 1.7819292927023225 (10 / 80)\n",
            "underfit -> train_accuracy = 0.20519159456118666\n",
            "lr 3.3042208394929954e-05, batch 9, decay 2.2544953140818472e-05, gamma 0.1726830675807015, val accuracy 0.18719211822660098, val loss 1.7819292927023225 [14 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 3.197395076331845e-05, 'batch_size': 9, 'weight_decay': 3.558941299187794e-05, 'gamma': 0.019386212704505884}\n",
            "train_acc: 0.16440049443757726, val_acc: 0.18226600985221675, train_loss: 1.7924532119659027, val_loss: 1.7920219998054316 (1 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7919292393956696, val_loss: 1.790701379916938 (2 / 80)\n",
            "train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7905474725848223, val_loss: 1.7895015372431338 (3 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7894555118086901, val_loss: 1.7883586642777392 (4 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.7882258606192798, val_loss: 1.7872381239689041 (5 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.1921182266009852, train_loss: 1.7877407831373557, val_loss: 1.7861061947686332 (6 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.2019704433497537, train_loss: 1.7866976237562295, val_loss: 1.7849850437324035 (7 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.1921182266009852, train_loss: 1.7855170708798949, val_loss: 1.7838412652461988 (8 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.2413793103448276, train_loss: 1.784582667651371, val_loss: 1.7827796489734369 (9 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.2315270935960591, train_loss: 1.7830326754613623, val_loss: 1.7816130486615185 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1841779975278121\n",
            "lr 3.197395076331845e-05, batch 9, decay 3.558941299187794e-05, gamma 0.019386212704505884, val accuracy 0.2413793103448276, val loss 1.7827796489734369 [15 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0008953725585146094, 'batch_size': 14, 'weight_decay': 0.000203492637897574, 'gamma': 0.04966470353985619}\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.787415564133888, val_loss: 1.7756503162712887 (1 / 80)\n",
            "train_acc: 0.1668726823238566, val_acc: 0.18226600985221675, train_loss: 1.770330794220065, val_loss: 1.7558711027276928 (2 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.2561576354679803, train_loss: 1.757707850894763, val_loss: 1.7435344210986434 (3 / 80)\n",
            "train_acc: 0.2311495673671199, val_acc: 0.18719211822660098, train_loss: 1.746915886811774, val_loss: 1.7283571300835445 (4 / 80)\n",
            "train_acc: 0.23980222496909764, val_acc: 0.19704433497536947, train_loss: 1.739060390864963, val_loss: 1.7119896288575798 (5 / 80)\n",
            "train_acc: 0.24721878862793573, val_acc: 0.2660098522167488, train_loss: 1.6966830701262163, val_loss: 1.653645922397745 (6 / 80)\n",
            "train_acc: 0.30778739184178, val_acc: 0.2512315270935961, train_loss: 1.6333165970367318, val_loss: 1.6327147072759167 (7 / 80)\n",
            "train_acc: 0.3362175525339926, val_acc: 0.2857142857142857, train_loss: 1.5876114814920979, val_loss: 1.6017859023192833 (8 / 80)\n",
            "train_acc: 0.3522867737948084, val_acc: 0.39408866995073893, train_loss: 1.5605416918272437, val_loss: 1.4567518110932975 (9 / 80)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.35467980295566504, train_loss: 1.4850983266040627, val_loss: 1.4743560552597046 (10 / 80)\n",
            "train_acc: 0.33250927070457353, val_acc: 0.3793103448275862, train_loss: 1.5068980922510362, val_loss: 1.4520978475439137 (11 / 80)\n",
            "train_acc: 0.3992583436341162, val_acc: 0.4088669950738916, train_loss: 1.4274535445848708, val_loss: 1.3615094587720673 (12 / 80)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.41379310344827586, train_loss: 1.4223714851773124, val_loss: 1.3596570615110726 (13 / 80)\n",
            "train_acc: 0.3980222496909765, val_acc: 0.43349753694581283, train_loss: 1.4326310436274714, val_loss: 1.3216071375485123 (14 / 80)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.41379310344827586, train_loss: 1.444635716887428, val_loss: 1.3487221208111992 (15 / 80)\n",
            "train_acc: 0.40173053152039556, val_acc: 0.41379310344827586, train_loss: 1.4019482053254533, val_loss: 1.3270318878108058 (16 / 80)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.3694581280788177, train_loss: 1.3823514941006863, val_loss: 1.3214745480438759 (17 / 80)\n",
            "train_acc: 0.415327564894932, val_acc: 0.37438423645320196, train_loss: 1.3656235413144633, val_loss: 1.308371848073499 (18 / 80)\n",
            "train_acc: 0.44746600741656367, val_acc: 0.4433497536945813, train_loss: 1.3287267583262345, val_loss: 1.3242356242804691 (19 / 80)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.43842364532019706, train_loss: 1.341282256306765, val_loss: 1.2390337319209659 (20 / 80)\n",
            "train_acc: 0.449938195302843, val_acc: 0.43349753694581283, train_loss: 1.3022530345598464, val_loss: 1.3566006915322666 (21 / 80)\n",
            "train_acc: 0.43757725587144625, val_acc: 0.43349753694581283, train_loss: 1.292161051952942, val_loss: 1.3499034766493172 (22 / 80)\n",
            "train_acc: 0.4252163164400494, val_acc: 0.4236453201970443, train_loss: 1.3299484490169435, val_loss: 1.2663237925233513 (23 / 80)\n",
            "train_acc: 0.4647713226205192, val_acc: 0.458128078817734, train_loss: 1.2724302780200878, val_loss: 1.1923482582486908 (24 / 80)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.46798029556650245, train_loss: 1.252830580226865, val_loss: 1.1688316073910943 (25 / 80)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.458128078817734, train_loss: 1.2270899947700453, val_loss: 1.1708993829529861 (26 / 80)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.4827586206896552, train_loss: 1.2687320856582396, val_loss: 1.2291586481291672 (27 / 80)\n",
            "train_acc: 0.48825710754017304, val_acc: 0.4876847290640394, train_loss: 1.206395809376343, val_loss: 1.1784535584778622 (28 / 80)\n",
            "train_acc: 0.49443757725587145, val_acc: 0.4975369458128079, train_loss: 1.2019392051567106, val_loss: 1.1564478771439914 (29 / 80)\n",
            "train_acc: 0.5030902348578492, val_acc: 0.43842364532019706, train_loss: 1.1723808355178174, val_loss: 1.1881667992164349 (30 / 80)\n",
            "train_acc: 0.5092707045735476, val_acc: 0.4630541871921182, train_loss: 1.1415808744572002, val_loss: 1.2314749462851162 (31 / 80)\n",
            "train_acc: 0.49443757725587145, val_acc: 0.458128078817734, train_loss: 1.182304038252907, val_loss: 1.3218758805044766 (32 / 80)\n",
            "train_acc: 0.511742892459827, val_acc: 0.5024630541871922, train_loss: 1.1465580912101991, val_loss: 1.1459572993475815 (33 / 80)\n",
            "train_acc: 0.49814585908529047, val_acc: 0.4630541871921182, train_loss: 1.1473055473806242, val_loss: 1.2037130799786797 (34 / 80)\n",
            "train_acc: 0.5377008652657602, val_acc: 0.47783251231527096, train_loss: 1.116285782367249, val_loss: 1.2355582385227597 (35 / 80)\n",
            "train_acc: 0.5389369592088998, val_acc: 0.4975369458128079, train_loss: 1.1022970828196617, val_loss: 1.2752575977095242 (36 / 80)\n",
            "train_acc: 0.553770086526576, val_acc: 0.5320197044334976, train_loss: 1.0694759581675488, val_loss: 1.1210231205512737 (37 / 80)\n",
            "train_acc: 0.5747836835599506, val_acc: 0.5073891625615764, train_loss: 1.0366843348675223, val_loss: 1.1419818298570041 (38 / 80)\n",
            "train_acc: 0.5772558714462299, val_acc: 0.5270935960591133, train_loss: 0.999210163158893, val_loss: 1.1661884702485183 (39 / 80)\n",
            "train_acc: 0.588380716934487, val_acc: 0.47783251231527096, train_loss: 1.0100532989714144, val_loss: 1.1681699074547867 (40 / 80)\n",
            "train_acc: 0.5933250927070457, val_acc: 0.5812807881773399, train_loss: 1.0168186483630732, val_loss: 1.0441231028787021 (41 / 80)\n",
            "train_acc: 0.622991347342398, val_acc: 0.5812807881773399, train_loss: 0.9386046972351404, val_loss: 1.0400537314086125 (42 / 80)\n",
            "train_acc: 0.6613102595797281, val_acc: 0.5714285714285714, train_loss: 0.8804272951463834, val_loss: 1.1277720928192139 (43 / 80)\n",
            "train_acc: 0.6217552533992583, val_acc: 0.541871921182266, train_loss: 0.9207747448212726, val_loss: 1.1285162136472504 (44 / 80)\n",
            "train_acc: 0.6662546353522868, val_acc: 0.5911330049261084, train_loss: 0.8544420566782815, val_loss: 1.0964395342202022 (45 / 80)\n",
            "train_acc: 0.6761433868974042, val_acc: 0.5320197044334976, train_loss: 0.853040805447971, val_loss: 1.0764849021516998 (46 / 80)\n",
            "train_acc: 0.6897404202719407, val_acc: 0.5812807881773399, train_loss: 0.8051347997780931, val_loss: 1.0548291781852985 (47 / 80)\n",
            "train_acc: 0.6909765142150803, val_acc: 0.6206896551724138, train_loss: 0.7858065099297702, val_loss: 0.9601221865621107 (48 / 80)\n",
            "train_acc: 0.7379480840543882, val_acc: 0.645320197044335, train_loss: 0.6991468897119293, val_loss: 0.8902628134036886 (49 / 80)\n",
            "train_acc: 0.7898640296662547, val_acc: 0.6551724137931034, train_loss: 0.5949805986380253, val_loss: 0.9024412734755154 (50 / 80)\n",
            "train_acc: 0.7861557478368356, val_acc: 0.6502463054187192, train_loss: 0.5721001311197269, val_loss: 0.9060757858999844 (51 / 80)\n",
            "train_acc: 0.7812113720642769, val_acc: 0.6354679802955665, train_loss: 0.586283428873061, val_loss: 0.8987611655531258 (52 / 80)\n",
            "train_acc: 0.8121137206427689, val_acc: 0.6403940886699507, train_loss: 0.5152946879306741, val_loss: 0.9180378975539372 (53 / 80)\n",
            "train_acc: 0.8022249690976514, val_acc: 0.6206896551724138, train_loss: 0.5634213156988948, val_loss: 0.9302061590655096 (54 / 80)\n",
            "train_acc: 0.7985166872682324, val_acc: 0.6403940886699507, train_loss: 0.5359824541841066, val_loss: 0.9233844752969413 (55 / 80)\n",
            "train_acc: 0.8158220024721878, val_acc: 0.645320197044335, train_loss: 0.5135496058334379, val_loss: 0.9221451755227714 (56 / 80)\n",
            "train_acc: 0.8084054388133498, val_acc: 0.6551724137931034, train_loss: 0.4997240919412582, val_loss: 0.9296531923886003 (57 / 80)\n",
            "train_acc: 0.8294190358467244, val_acc: 0.645320197044335, train_loss: 0.4761414296547474, val_loss: 0.9388067537340624 (58 / 80)\n",
            "train_acc: 0.8281829419035847, val_acc: 0.625615763546798, train_loss: 0.48682703742727507, val_loss: 0.9763080354394584 (59 / 80)\n",
            "train_acc: 0.8158220024721878, val_acc: 0.6354679802955665, train_loss: 0.48625512073007593, val_loss: 0.9724553083551342 (60 / 80)\n",
            "train_acc: 0.8207663782447466, val_acc: 0.6305418719211823, train_loss: 0.47333962846966404, val_loss: 0.9849876288709969 (61 / 80)\n",
            "train_acc: 0.8121137206427689, val_acc: 0.6403940886699507, train_loss: 0.4930337985012823, val_loss: 0.9632591214673273 (62 / 80)\n",
            "train_acc: 0.8294190358467244, val_acc: 0.625615763546798, train_loss: 0.46177852035894973, val_loss: 0.9884029513803022 (63 / 80)\n",
            "train_acc: 0.823238566131026, val_acc: 0.6403940886699507, train_loss: 0.4685932102766261, val_loss: 1.0060558771264965 (64 / 80)\n",
            "train_acc: 0.8281829419035847, val_acc: 0.6354679802955665, train_loss: 0.438939513544217, val_loss: 0.9815003419744557 (65 / 80)\n",
            "train_acc: 0.8590852904820766, val_acc: 0.6354679802955665, train_loss: 0.42789524387480743, val_loss: 1.0090743262192299 (66 / 80)\n",
            "train_acc: 0.861557478368356, val_acc: 0.6305418719211823, train_loss: 0.4090430800088109, val_loss: 0.9809014488910807 (67 / 80)\n",
            "train_acc: 0.8504326328800988, val_acc: 0.6305418719211823, train_loss: 0.40116065729533784, val_loss: 0.9977589578464113 (68 / 80)\n",
            "train_acc: 0.8566131025957973, val_acc: 0.625615763546798, train_loss: 0.430286761433144, val_loss: 1.0251140717802376 (69 / 80)\n",
            "train_acc: 0.8393077873918418, val_acc: 0.6403940886699507, train_loss: 0.42102183543412736, val_loss: 1.0241043505997494 (70 / 80)\n",
            "train_acc: 0.8417799752781211, val_acc: 0.6403940886699507, train_loss: 0.4173261045083422, val_loss: 0.9891311567405174 (71 / 80)\n",
            "train_acc: 0.8417799752781211, val_acc: 0.6305418719211823, train_loss: 0.44632277730737246, val_loss: 1.0030901719783913 (72 / 80)\n",
            "overfit -> train_accuracy-val_accuracy = 0.25077484213923407\n",
            "lr 0.0008953725585146094, batch 14, decay 0.000203492637897574, gamma 0.04966470353985619, val accuracy 0.6551724137931034, val loss 0.9024412734755154 [16 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.6848678710363327e-05, 'batch_size': 13, 'weight_decay': 2.434939383279698e-05, 'gamma': 0.8043372181468511}\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7895991986407926, val_loss: 1.7894282710963283 (1 / 80)\n",
            "train_acc: 0.1631644004944376, val_acc: 0.18226600985221675, train_loss: 1.7894321033185434, val_loss: 1.7887662936901223 (2 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7891038642972892, val_loss: 1.7881003588878464 (3 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7889707117941236, val_loss: 1.7875428005979566 (4 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18719211822660098, train_loss: 1.7875054267192505, val_loss: 1.786891032909525 (5 / 80)\n",
            "train_acc: 0.17181705809641531, val_acc: 0.20689655172413793, train_loss: 1.7882221506906233, val_loss: 1.786285328160366 (6 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.22167487684729065, train_loss: 1.7876705911162463, val_loss: 1.7856773619581325 (7 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.2512315270935961, train_loss: 1.7863168135856373, val_loss: 1.7851064140573512 (8 / 80)\n",
            "train_acc: 0.17181705809641531, val_acc: 0.2857142857142857, train_loss: 1.7852237161216689, val_loss: 1.784512523359853 (9 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.2561576354679803, train_loss: 1.7848315443774532, val_loss: 1.7838956857549733 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18046971569839307\n",
            "lr 1.6848678710363327e-05, batch 13, decay 2.434939383279698e-05, gamma 0.8043372181468511, val accuracy 0.2857142857142857, val loss 1.784512523359853 [17 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 4.783176617531722e-05, 'batch_size': 10, 'weight_decay': 0.0009642418472764449, 'gamma': 0.03561973625955027}\n",
            "train_acc: 0.1668726823238566, val_acc: 0.2019704433497537, train_loss: 1.7896279705174922, val_loss: 1.7886661238271027 (1 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.19704433497536947, train_loss: 1.7885034353977671, val_loss: 1.7877183935325134 (2 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.21674876847290642, train_loss: 1.7882599864518833, val_loss: 1.7868574845967033 (3 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18719211822660098, train_loss: 1.7860787015026078, val_loss: 1.7858835646671614 (4 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7862809827212793, val_loss: 1.7849680778428252 (5 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7849221876436758, val_loss: 1.78396161260276 (6 / 80)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.18226600985221675, train_loss: 1.7843121241580129, val_loss: 1.7830340339632458 (7 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.784400378081059, val_loss: 1.7821194802599 (8 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7832948828509771, val_loss: 1.781144921415545 (9 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7828001223034853, val_loss: 1.7802673073237754 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18170580964153277\n",
            "lr 4.783176617531722e-05, batch 10, decay 0.0009642418472764449, gamma 0.03561973625955027, val accuracy 0.21674876847290642, val loss 1.7868574845967033 [18 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0002531721137862798, 'batch_size': 8, 'weight_decay': 0.0009655920272047319, 'gamma': 0.15488009464632205}\n",
            "train_acc: 0.18294190358467244, val_acc: 0.3054187192118227, train_loss: 1.789054440184783, val_loss: 1.7843526437364776 (1 / 80)\n",
            "train_acc: 0.22867737948084055, val_acc: 0.18226600985221675, train_loss: 1.7816456823030715, val_loss: 1.7761704557634928 (2 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7754314595600877, val_loss: 1.767180021760499 (3 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.765440572177524, val_loss: 1.7567397649652265 (4 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7581094434146387, val_loss: 1.7490777851912775 (5 / 80)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.18226600985221675, train_loss: 1.7578396468581021, val_loss: 1.7432840663224018 (6 / 80)\n",
            "train_acc: 0.207663782447466, val_acc: 0.19704433497536947, train_loss: 1.7535008460246442, val_loss: 1.737018322709746 (7 / 80)\n",
            "train_acc: 0.2311495673671199, val_acc: 0.29064039408866993, train_loss: 1.7387481115951822, val_loss: 1.7277642194860674 (8 / 80)\n",
            "train_acc: 0.26823238566131025, val_acc: 0.33004926108374383, train_loss: 1.7345173322078766, val_loss: 1.7141256074012794 (9 / 80)\n",
            "train_acc: 0.28553770086526575, val_acc: 0.2413793103448276, train_loss: 1.7129745944616084, val_loss: 1.6908158856659687 (10 / 80)\n",
            "train_acc: 0.29913473423980225, val_acc: 0.2857142857142857, train_loss: 1.690199688869, val_loss: 1.636707287703829 (11 / 80)\n",
            "train_acc: 0.3399258343634116, val_acc: 0.3251231527093596, train_loss: 1.632972244131845, val_loss: 1.5848777006412376 (12 / 80)\n",
            "train_acc: 0.3362175525339926, val_acc: 0.3399014778325123, train_loss: 1.6032216148117122, val_loss: 1.5376284539405936 (13 / 80)\n",
            "train_acc: 0.32756489493201485, val_acc: 0.3103448275862069, train_loss: 1.5741397995589248, val_loss: 1.6105776449729656 (14 / 80)\n",
            "train_acc: 0.3473423980222497, val_acc: 0.35960591133004927, train_loss: 1.59932220998889, val_loss: 1.5170055375310587 (15 / 80)\n",
            "train_acc: 0.33498145859085293, val_acc: 0.3399014778325123, train_loss: 1.5544735265455965, val_loss: 1.4908969290738034 (16 / 80)\n",
            "train_acc: 0.3572311495673671, val_acc: 0.3448275862068966, train_loss: 1.522565576879881, val_loss: 1.5345609675487275 (17 / 80)\n",
            "train_acc: 0.34610630407911, val_acc: 0.33497536945812806, train_loss: 1.5117590338986353, val_loss: 1.4851932525634766 (18 / 80)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.35467980295566504, train_loss: 1.4987435332040115, val_loss: 1.454722967053869 (19 / 80)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.3842364532019704, train_loss: 1.4626893840997268, val_loss: 1.4561962488249605 (20 / 80)\n",
            "train_acc: 0.3683559950556242, val_acc: 0.35467980295566504, train_loss: 1.521939418224528, val_loss: 1.4407730219986639 (21 / 80)\n",
            "train_acc: 0.3930778739184178, val_acc: 0.35467980295566504, train_loss: 1.4535022618891429, val_loss: 1.4627476754446922 (22 / 80)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.41379310344827586, train_loss: 1.4439881558176613, val_loss: 1.4280078032333863 (23 / 80)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.39901477832512317, train_loss: 1.428920632681829, val_loss: 1.4459364575705504 (24 / 80)\n",
            "train_acc: 0.39555006180469715, val_acc: 0.3842364532019704, train_loss: 1.44481319373264, val_loss: 1.3868786968621127 (25 / 80)\n",
            "train_acc: 0.3868974042027194, val_acc: 0.37438423645320196, train_loss: 1.4469472317229979, val_loss: 1.3776721032382233 (26 / 80)\n",
            "train_acc: 0.40667490729295425, val_acc: 0.3793103448275862, train_loss: 1.3940344419703348, val_loss: 1.397299778285285 (27 / 80)\n",
            "train_acc: 0.41903584672435107, val_acc: 0.39901477832512317, train_loss: 1.3589537610819078, val_loss: 1.342888882007505 (28 / 80)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.4187192118226601, train_loss: 1.4159452602801423, val_loss: 1.3340356426285993 (29 / 80)\n",
            "train_acc: 0.4215080346106304, val_acc: 0.4482758620689655, train_loss: 1.376444389410455, val_loss: 1.3205630973054858 (30 / 80)\n",
            "train_acc: 0.4388133498145859, val_acc: 0.3842364532019704, train_loss: 1.3633285380706504, val_loss: 1.3920504136625769 (31 / 80)\n",
            "train_acc: 0.42398022249690975, val_acc: 0.4729064039408867, train_loss: 1.359041180392573, val_loss: 1.295046733517952 (32 / 80)\n",
            "train_acc: 0.4326328800988875, val_acc: 0.3793103448275862, train_loss: 1.3684176532240822, val_loss: 1.344804361535998 (33 / 80)\n",
            "train_acc: 0.411619283065513, val_acc: 0.42857142857142855, train_loss: 1.387951353129704, val_loss: 1.2940462780703466 (34 / 80)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.39901477832512317, train_loss: 1.346973545324375, val_loss: 1.3040526982011467 (35 / 80)\n",
            "train_acc: 0.43139678615574784, val_acc: 0.3399014778325123, train_loss: 1.339445559144462, val_loss: 1.370599028512175 (36 / 80)\n",
            "train_acc: 0.4363411619283066, val_acc: 0.4729064039408867, train_loss: 1.3215143211987168, val_loss: 1.2611518734194376 (37 / 80)\n",
            "train_acc: 0.4684796044499382, val_acc: 0.4187192118226601, train_loss: 1.3015599645701859, val_loss: 1.2635611589319014 (38 / 80)\n",
            "train_acc: 0.4721878862793572, val_acc: 0.46798029556650245, train_loss: 1.2820683000114261, val_loss: 1.2456508246548657 (39 / 80)\n",
            "train_acc: 0.4561186650185414, val_acc: 0.37438423645320196, train_loss: 1.2885912913179811, val_loss: 1.3625852268904888 (40 / 80)\n",
            "train_acc: 0.4894932014833127, val_acc: 0.46798029556650245, train_loss: 1.2707391330426645, val_loss: 1.205890939153474 (41 / 80)\n",
            "train_acc: 0.48825710754017304, val_acc: 0.3891625615763547, train_loss: 1.2454068959864462, val_loss: 1.3970345182371844 (42 / 80)\n",
            "train_acc: 0.5105067985166872, val_acc: 0.46798029556650245, train_loss: 1.2318289724946465, val_loss: 1.2982136063975067 (43 / 80)\n",
            "train_acc: 0.5055624227441285, val_acc: 0.43349753694581283, train_loss: 1.246812275814627, val_loss: 1.218558172287025 (44 / 80)\n",
            "train_acc: 0.4969097651421508, val_acc: 0.4827586206896552, train_loss: 1.20597825961001, val_loss: 1.2008642383984156 (45 / 80)\n",
            "train_acc: 0.48702101359703337, val_acc: 0.4975369458128079, train_loss: 1.2114606610924117, val_loss: 1.1791857381172368 (46 / 80)\n",
            "train_acc: 0.5092707045735476, val_acc: 0.4975369458128079, train_loss: 1.1912728156088603, val_loss: 1.314421798208077 (47 / 80)\n",
            "train_acc: 0.49443757725587145, val_acc: 0.5172413793103449, train_loss: 1.177028500842223, val_loss: 1.168079918241266 (48 / 80)\n",
            "train_acc: 0.546353522867738, val_acc: 0.5221674876847291, train_loss: 1.1236788315885, val_loss: 1.1751735735996602 (49 / 80)\n",
            "train_acc: 0.5512978986402967, val_acc: 0.5320197044334976, train_loss: 1.104209935561983, val_loss: 1.1580818873908132 (50 / 80)\n",
            "train_acc: 0.5512978986402967, val_acc: 0.5221674876847291, train_loss: 1.0912479657325227, val_loss: 1.1983618284093922 (51 / 80)\n",
            "train_acc: 0.5562422744128553, val_acc: 0.5123152709359606, train_loss: 1.0847943963756963, val_loss: 1.1532148607258725 (52 / 80)\n",
            "train_acc: 0.5760197775030902, val_acc: 0.5221674876847291, train_loss: 1.0667636527119224, val_loss: 1.1452814913148364 (53 / 80)\n",
            "train_acc: 0.5834363411619283, val_acc: 0.4975369458128079, train_loss: 1.0839065208717977, val_loss: 1.137929481532186 (54 / 80)\n",
            "train_acc: 0.5871446229913473, val_acc: 0.5024630541871922, train_loss: 1.0429119223864618, val_loss: 1.1620496696439282 (55 / 80)\n",
            "train_acc: 0.5698393077873919, val_acc: 0.5073891625615764, train_loss: 1.041378605351018, val_loss: 1.1465212227088477 (56 / 80)\n",
            "train_acc: 0.5760197775030902, val_acc: 0.5369458128078818, train_loss: 1.045298169068854, val_loss: 1.1617277737321525 (57 / 80)\n",
            "train_acc: 0.5735475896168108, val_acc: 0.49261083743842365, train_loss: 1.0310620427573713, val_loss: 1.161767637641559 (58 / 80)\n",
            "train_acc: 0.5822002472187886, val_acc: 0.5073891625615764, train_loss: 1.0508521808534677, val_loss: 1.170350309956837 (59 / 80)\n",
            "train_acc: 0.5834363411619283, val_acc: 0.5123152709359606, train_loss: 1.048866125208191, val_loss: 1.1291136083931759 (60 / 80)\n",
            "train_acc: 0.5982694684796045, val_acc: 0.5270935960591133, train_loss: 1.0157268091538927, val_loss: 1.1373830728342968 (61 / 80)\n",
            "train_acc: 0.5970333745364648, val_acc: 0.5123152709359606, train_loss: 1.0023694249107162, val_loss: 1.128943940101586 (62 / 80)\n",
            "train_acc: 0.6069221260815822, val_acc: 0.5172413793103449, train_loss: 1.0168085429812834, val_loss: 1.1268905142845191 (63 / 80)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5221674876847291, train_loss: 0.9833448779008297, val_loss: 1.1459653988260354 (64 / 80)\n",
            "train_acc: 0.6205191594561187, val_acc: 0.5270935960591133, train_loss: 0.9850073246784941, val_loss: 1.1229854079302897 (65 / 80)\n",
            "train_acc: 0.6093943139678616, val_acc: 0.5123152709359606, train_loss: 0.9923739978497933, val_loss: 1.2482002072146374 (66 / 80)\n",
            "train_acc: 0.6044499381953028, val_acc: 0.5270935960591133, train_loss: 0.9946838810358413, val_loss: 1.135863816356424 (67 / 80)\n",
            "train_acc: 0.6069221260815822, val_acc: 0.5221674876847291, train_loss: 1.0003854409136494, val_loss: 1.1240946483142271 (68 / 80)\n",
            "train_acc: 0.61557478368356, val_acc: 0.5123152709359606, train_loss: 0.9791652142485818, val_loss: 1.1817280153922847 (69 / 80)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5467980295566502, train_loss: 0.9886539778102313, val_loss: 1.117496659603025 (70 / 80)\n",
            "train_acc: 0.6378244746600742, val_acc: 0.5123152709359606, train_loss: 0.955635701475391, val_loss: 1.170106169331837 (71 / 80)\n",
            "train_acc: 0.6081582200247219, val_acc: 0.5467980295566502, train_loss: 0.9765714050370772, val_loss: 1.1125595358204958 (72 / 80)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.5123152709359606, train_loss: 0.9644300059423458, val_loss: 1.152000006783772 (73 / 80)\n",
            "train_acc: 0.6390605686032138, val_acc: 0.5517241379310345, train_loss: 0.9233744778061385, val_loss: 1.1015644120465358 (74 / 80)\n",
            "train_acc: 0.6402966625463535, val_acc: 0.5073891625615764, train_loss: 0.9521179968406007, val_loss: 1.1700292035863904 (75 / 80)\n",
            "train_acc: 0.6242274412855378, val_acc: 0.541871921182266, train_loss: 0.9454403006102158, val_loss: 1.1266387524863182 (76 / 80)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.5369458128078818, train_loss: 0.9383655793881976, val_loss: 1.1959145703339225 (77 / 80)\n",
            "train_acc: 0.6328800988875154, val_acc: 0.5467980295566502, train_loss: 0.9270373345600218, val_loss: 1.139235940179214 (78 / 80)\n",
            "train_acc: 0.6118665018541409, val_acc: 0.5172413793103449, train_loss: 0.9692476349796442, val_loss: 1.1845936270182944 (79 / 80)\n",
            "train_acc: 0.6501854140914709, val_acc: 0.5566502463054187, train_loss: 0.9163771433058127, val_loss: 1.1655204400346784 (80 / 80)\n",
            "lr 0.0002531721137862798, batch 8, decay 0.0009655920272047319, gamma 0.15488009464632205, val accuracy 0.5566502463054187, val loss 1.1655204400346784 [19 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.905352466839929e-05, 'batch_size': 14, 'weight_decay': 8.696974751849823e-05, 'gamma': 0.04630137073797267}\n",
            "train_acc: 0.1557478368355995, val_acc: 0.18226600985221675, train_loss: 1.790477024315314, val_loss: 1.7890354066059506 (1 / 80)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.18226600985221675, train_loss: 1.7889568214805518, val_loss: 1.7886278588196327 (2 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7898984197043666, val_loss: 1.7882286680155788 (3 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.7893896434451506, val_loss: 1.7878210256839622 (4 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7890916440926024, val_loss: 1.787447789619709 (5 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18719211822660098, train_loss: 1.7885848157043363, val_loss: 1.78703733970379 (6 / 80)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.18719211822660098, train_loss: 1.7870276687466464, val_loss: 1.7866613453832165 (7 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.1921182266009852, train_loss: 1.788017912759769, val_loss: 1.7862822639531102 (8 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.1921182266009852, train_loss: 1.7868480725105673, val_loss: 1.7859024919312576 (9 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.2019704433497537, train_loss: 1.7879698787837741, val_loss: 1.7855143999231273 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1841779975278121\n",
            "lr 1.905352466839929e-05, batch 14, decay 8.696974751849823e-05, gamma 0.04630137073797267, val accuracy 0.2019704433497537, val loss 1.7855143999231273 [20 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00037657242094956675, 'batch_size': 14, 'weight_decay': 0.0006370553781871713, 'gamma': 0.0812292656012972}\n",
            "train_acc: 0.17058096415327564, val_acc: 0.19704433497536947, train_loss: 1.792964639121434, val_loss: 1.7871162357001469 (1 / 80)\n",
            "train_acc: 0.2249690976514215, val_acc: 0.18226600985221675, train_loss: 1.786119551682207, val_loss: 1.7808319905708576 (2 / 80)\n",
            "train_acc: 0.207663782447466, val_acc: 0.20689655172413793, train_loss: 1.7800710531336121, val_loss: 1.7730719262155994 (3 / 80)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.18226600985221675, train_loss: 1.7724212011683858, val_loss: 1.7644403556297565 (4 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7650907133948817, val_loss: 1.755947441890322 (5 / 80)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.759258204396487, val_loss: 1.7498310763260414 (6 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7608038402163644, val_loss: 1.7453570283692459 (7 / 80)\n",
            "train_acc: 0.21755253399258342, val_acc: 0.2019704433497537, train_loss: 1.753454827997092, val_loss: 1.7398754769358142 (8 / 80)\n",
            "train_acc: 0.21755253399258342, val_acc: 0.22167487684729065, train_loss: 1.7480005425192664, val_loss: 1.7332626827831925 (9 / 80)\n",
            "train_acc: 0.22991347342398022, val_acc: 0.22167487684729065, train_loss: 1.7385284420291631, val_loss: 1.7215363362739826 (10 / 80)\n",
            "underfit -> train_accuracy = 0.22991347342398022\n",
            "lr 0.00037657242094956675, batch 14, decay 0.0006370553781871713, gamma 0.0812292656012972, val accuracy 0.22167487684729065, val loss 1.7332626827831925 [21 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0002055106070295775, 'batch_size': 10, 'weight_decay': 2.792640215163258e-05, 'gamma': 0.6679608767136781}\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7912129636748022, val_loss: 1.7894050892937947 (1 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.787606562465908, val_loss: 1.7858272338735646 (2 / 80)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.23645320197044334, train_loss: 1.7843774088528306, val_loss: 1.7826859375526165 (3 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.781033476291982, val_loss: 1.7788196637712677 (4 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7786532612459915, val_loss: 1.7750854110482879 (5 / 80)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.18226600985221675, train_loss: 1.772522323653194, val_loss: 1.7704382506497387 (6 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7707996842887699, val_loss: 1.765560981088084 (7 / 80)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.76552544182399, val_loss: 1.76116462587723 (8 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7601813976785308, val_loss: 1.7567053892342328 (9 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7592298577830876, val_loss: 1.7529475929701857 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18046971569839307\n",
            "lr 0.0002055106070295775, batch 10, decay 2.792640215163258e-05, gamma 0.6679608767136781, val accuracy 0.23645320197044334, val loss 1.7826859375526165 [22 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00012041850834594425, 'batch_size': 11, 'weight_decay': 2.0271215230368365e-05, 'gamma': 0.042630499910263776}\n",
            "train_acc: 0.17676143386897405, val_acc: 0.1625615763546798, train_loss: 1.792549537787184, val_loss: 1.7912093306997139 (1 / 80)\n",
            "train_acc: 0.16934487021013597, val_acc: 0.1477832512315271, train_loss: 1.7907904981830065, val_loss: 1.788928284433675 (2 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.270935960591133, train_loss: 1.7881625402990466, val_loss: 1.786708316779489 (3 / 80)\n",
            "train_acc: 0.17552533992583436, val_acc: 0.2413793103448276, train_loss: 1.786511115002249, val_loss: 1.7845563042927257 (4 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.24630541871921183, train_loss: 1.7851231970510022, val_loss: 1.7823121124887702 (5 / 80)\n",
            "train_acc: 0.22249690976514216, val_acc: 0.2019704433497537, train_loss: 1.783069941994581, val_loss: 1.7801735336557398 (6 / 80)\n",
            "train_acc: 0.21013597033374537, val_acc: 0.18226600985221675, train_loss: 1.7792985160801702, val_loss: 1.7776191586931351 (7 / 80)\n",
            "train_acc: 0.21508034610630408, val_acc: 0.18226600985221675, train_loss: 1.778994392405629, val_loss: 1.774961579609387 (8 / 80)\n",
            "train_acc: 0.22991347342398022, val_acc: 0.18226600985221675, train_loss: 1.7738786147463013, val_loss: 1.7720296705884886 (9 / 80)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.18226600985221675, train_loss: 1.7730262889555564, val_loss: 1.7689976709816844 (10 / 80)\n",
            "underfit -> train_accuracy = 0.20148331273176762\n",
            "lr 0.00012041850834594425, batch 11, decay 2.0271215230368365e-05, gamma 0.042630499910263776, val accuracy 0.270935960591133, val loss 1.786708316779489 [23 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 2.2379438732413894e-05, 'batch_size': 11, 'weight_decay': 7.883912666714595e-06, 'gamma': 0.29274769663507344}\n",
            "train_acc: 0.14585908529048208, val_acc: 0.10344827586206896, train_loss: 1.7930757482208044, val_loss: 1.7922773149800417 (1 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.18719211822660098, train_loss: 1.792243747540251, val_loss: 1.7917129006879082 (2 / 80)\n",
            "train_acc: 0.15451174289245984, val_acc: 0.2019704433497537, train_loss: 1.791853348610575, val_loss: 1.791147052360873 (3 / 80)\n",
            "train_acc: 0.16934487021013597, val_acc: 0.22660098522167488, train_loss: 1.7913818042564156, val_loss: 1.7906180473384012 (4 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.21674876847290642, train_loss: 1.7911705879404753, val_loss: 1.7900925023215157 (5 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.2413793103448276, train_loss: 1.7898380803531415, val_loss: 1.7895808959829396 (6 / 80)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.23645320197044334, train_loss: 1.789481217840546, val_loss: 1.7890240435529812 (7 / 80)\n",
            "train_acc: 0.17428924598269468, val_acc: 0.2413793103448276, train_loss: 1.7902858560842696, val_loss: 1.78850945874388 (8 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.24630541871921183, train_loss: 1.7881288836411993, val_loss: 1.7879804173126597 (9 / 80)\n",
            "train_acc: 0.17676143386897405, val_acc: 0.23645320197044334, train_loss: 1.78863177090257, val_loss: 1.7874526595834441 (10 / 80)\n",
            "underfit -> train_accuracy = 0.17676143386897405\n",
            "lr 2.2379438732413894e-05, batch 11, decay 7.883912666714595e-06, gamma 0.29274769663507344, val accuracy 0.24630541871921183, val loss 1.7879804173126597 [24 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 2.996216955726189e-05, 'batch_size': 10, 'weight_decay': 5.670661568395657e-06, 'gamma': 0.6007000313731713}\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18719211822660098, train_loss: 1.7903237045179634, val_loss: 1.7890934409766361 (1 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.20689655172413793, train_loss: 1.7892668035033312, val_loss: 1.788133008139474 (2 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.21674876847290642, train_loss: 1.788569912774896, val_loss: 1.7871808894162107 (3 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.2315270935960591, train_loss: 1.7876057032454293, val_loss: 1.786216427539957 (4 / 80)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.24630541871921183, train_loss: 1.7872999088725878, val_loss: 1.7853131300122866 (5 / 80)\n",
            "train_acc: 0.21137206427688504, val_acc: 0.2413793103448276, train_loss: 1.7855439258299592, val_loss: 1.7843019639329958 (6 / 80)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.24630541871921183, train_loss: 1.7847676324313886, val_loss: 1.7833733517548134 (7 / 80)\n",
            "train_acc: 0.2126081582200247, val_acc: 0.2413793103448276, train_loss: 1.7832985089352753, val_loss: 1.7823802291466098 (8 / 80)\n",
            "train_acc: 0.22373300370828184, val_acc: 0.22660098522167488, train_loss: 1.7821050117572836, val_loss: 1.7814334519390989 (9 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.21182266009852216, train_loss: 1.7827060381767923, val_loss: 1.780527304545999 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18665018541409148\n",
            "lr 2.996216955726189e-05, batch 10, decay 5.670661568395657e-06, gamma 0.6007000313731713, val accuracy 0.24630541871921183, val loss 1.7853131300122866 [25 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 5.995488393472277e-05, 'batch_size': 13, 'weight_decay': 0.00012064524987386579, 'gamma': 0.6958797886028412}\n",
            "train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7923434980130755, val_loss: 1.7911480559504092 (1 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7905060741014327, val_loss: 1.7901057316164666 (2 / 80)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.18226600985221675, train_loss: 1.7900737914520377, val_loss: 1.7890593905754277 (3 / 80)\n",
            "train_acc: 0.20024721878862795, val_acc: 0.18719211822660098, train_loss: 1.788598641624262, val_loss: 1.7880795031345535 (4 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18719211822660098, train_loss: 1.7884649695513128, val_loss: 1.7870584150840496 (5 / 80)\n",
            "train_acc: 0.20395550061804696, val_acc: 0.19704433497536947, train_loss: 1.787133121962011, val_loss: 1.786107160774945 (6 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.22167487684729065, train_loss: 1.78682032857454, val_loss: 1.785096594265529 (7 / 80)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.22167487684729065, train_loss: 1.7859037007920084, val_loss: 1.7841444773039794 (8 / 80)\n",
            "train_acc: 0.2088998763906057, val_acc: 0.2413793103448276, train_loss: 1.7847974739499086, val_loss: 1.7831120872732453 (9 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.2512315270935961, train_loss: 1.7835114637323009, val_loss: 1.7821070014549594 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1965389369592089\n",
            "lr 5.995488393472277e-05, batch 13, decay 0.00012064524987386579, gamma 0.6958797886028412, val accuracy 0.2512315270935961, val loss 1.7821070014549594 [26 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0002675993513121015, 'batch_size': 15, 'weight_decay': 2.2029421346186062e-06, 'gamma': 0.12487823074737385}\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.789440775389135, val_loss: 1.7872180650974143 (1 / 80)\n",
            "train_acc: 0.20024721878862795, val_acc: 0.2019704433497537, train_loss: 1.7866482121688032, val_loss: 1.7839031495484226 (2 / 80)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.2413793103448276, train_loss: 1.783963984405744, val_loss: 1.7808871592206907 (3 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18719211822660098, train_loss: 1.780119381080599, val_loss: 1.7772946334237536 (4 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7778425844402337, val_loss: 1.7735321609844714 (5 / 80)\n",
            "train_acc: 0.21508034610630408, val_acc: 0.18226600985221675, train_loss: 1.7741894876706439, val_loss: 1.7693386224690328 (6 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7702956933468323, val_loss: 1.7643736459938764 (7 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7662352305849638, val_loss: 1.7604945486989514 (8 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7634515078607094, val_loss: 1.7558644346415704 (9 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7554671189693645, val_loss: 1.7518606197657844 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18170580964153277\n",
            "lr 0.0002675993513121015, batch 15, decay 2.2029421346186062e-06, gamma 0.12487823074737385, val accuracy 0.2413793103448276, val loss 1.7808871592206907 [27 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00013201408595811797, 'batch_size': 15, 'weight_decay': 0.0008423099611946554, 'gamma': 0.07363128886597509}\n",
            "train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.7885736938754766, val_loss: 1.7875813492413224 (1 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.17733990147783252, train_loss: 1.7867762494293515, val_loss: 1.7850503128737651 (2 / 80)\n",
            "train_acc: 0.1631644004944376, val_acc: 0.18226600985221675, train_loss: 1.7856618255855716, val_loss: 1.7828014942225565 (3 / 80)\n",
            "train_acc: 0.1631644004944376, val_acc: 0.17733990147783252, train_loss: 1.7835409938920708, val_loss: 1.7805004965495594 (4 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.16748768472906403, train_loss: 1.7802320860224985, val_loss: 1.778071934366461 (5 / 80)\n",
            "train_acc: 0.2088998763906057, val_acc: 0.13793103448275862, train_loss: 1.778160168891784, val_loss: 1.7755755315273267 (6 / 80)\n",
            "train_acc: 0.16440049443757726, val_acc: 0.16748768472906403, train_loss: 1.7765854337748845, val_loss: 1.7729688319079395 (7 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7737261447387809, val_loss: 1.7701868259260807 (8 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7719885822574346, val_loss: 1.767380592271025 (9 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7692077426592117, val_loss: 1.7645380068295107 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1915945611866502\n",
            "lr 0.00013201408595811797, batch 15, decay 0.0008423099611946554, gamma 0.07363128886597509, val accuracy 0.18226600985221675, val loss 1.7875813492413224 [28 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.3143892945388419e-05, 'batch_size': 10, 'weight_decay': 1.9203760627025567e-06, 'gamma': 0.03335634522775482}\n",
            "train_acc: 0.15822002472187885, val_acc: 0.1625615763546798, train_loss: 1.7923226054430892, val_loss: 1.792053003616521 (1 / 80)\n",
            "train_acc: 0.15327564894932014, val_acc: 0.1625615763546798, train_loss: 1.792616332564572, val_loss: 1.791713618879835 (2 / 80)\n",
            "train_acc: 0.14091470951792337, val_acc: 0.09359605911330049, train_loss: 1.79258643667247, val_loss: 1.7913775567350716 (3 / 80)\n",
            "train_acc: 0.15451174289245984, val_acc: 0.09359605911330049, train_loss: 1.7927947019322399, val_loss: 1.7910394169426904 (4 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.11822660098522167, train_loss: 1.790465960249176, val_loss: 1.7907165717608824 (5 / 80)\n",
            "train_acc: 0.15945611866501855, val_acc: 0.12807881773399016, train_loss: 1.7915009675715556, val_loss: 1.7903897315997797 (6 / 80)\n",
            "train_acc: 0.16069221260815822, val_acc: 0.12807881773399016, train_loss: 1.7906161045703959, val_loss: 1.7900824969625238 (7 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.1477832512315271, train_loss: 1.789972129209964, val_loss: 1.7897561465578127 (8 / 80)\n",
            "train_acc: 0.1668726823238566, val_acc: 0.1724137931034483, train_loss: 1.7906595636504543, val_loss: 1.7894364712860784 (9 / 80)\n",
            "train_acc: 0.16440049443757726, val_acc: 0.17733990147783252, train_loss: 1.7897932164011838, val_loss: 1.789126704479086 (10 / 80)\n",
            "underfit -> train_accuracy = 0.16440049443757726\n",
            "lr 1.3143892945388419e-05, batch 10, decay 1.9203760627025567e-06, gamma 0.03335634522775482, val accuracy 0.17733990147783252, val loss 1.789126704479086 [29 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 2.1085429508322516e-05, 'batch_size': 15, 'weight_decay': 9.705207707771695e-05, 'gamma': 0.011606333970593474}\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7915245732211948, val_loss: 1.7914933612193968 (1 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.7914812242144855, val_loss: 1.7912060139801702 (2 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7905706149538603, val_loss: 1.790916427015671 (3 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7906499894794043, val_loss: 1.7906431693748888 (4 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7909508790309703, val_loss: 1.7903463171033436 (5 / 80)\n",
            "train_acc: 0.21013597033374537, val_acc: 0.18226600985221675, train_loss: 1.7897395726629477, val_loss: 1.7900766069665919 (6 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7892998803236575, val_loss: 1.7897902715382317 (7 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.789698251098284, val_loss: 1.789504631399521 (8 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7897162738040882, val_loss: 1.789235137953547 (9 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18719211822660098, train_loss: 1.7896169676032143, val_loss: 1.7889673110886748 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1915945611866502\n",
            "lr 2.1085429508322516e-05, batch 15, decay 9.705207707771695e-05, gamma 0.011606333970593474, val accuracy 0.18719211822660098, val loss 1.7889673110886748 [30 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 3.883937077429172e-05, 'batch_size': 11, 'weight_decay': 0.00018999372579880553, 'gamma': 0.43087269959236435}\n",
            "train_acc: 0.12484548825710753, val_acc: 0.08866995073891626, train_loss: 1.794301411719493, val_loss: 1.7936250382456287 (1 / 80)\n",
            "train_acc: 0.15203955500618047, val_acc: 0.20689655172413793, train_loss: 1.7933033194913264, val_loss: 1.7925488778523035 (2 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.791791564307195, val_loss: 1.7914894044105643 (3 / 80)\n",
            "train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.791463270912359, val_loss: 1.7904578876025572 (4 / 80)\n",
            "train_acc: 0.1619283065512979, val_acc: 0.18226600985221675, train_loss: 1.790903668615815, val_loss: 1.7894233653110823 (5 / 80)\n",
            "train_acc: 0.16440049443757726, val_acc: 0.18226600985221675, train_loss: 1.7885386861298966, val_loss: 1.788465787037253 (6 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.788704072147129, val_loss: 1.7874867140953177 (7 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7872472086412503, val_loss: 1.7865410138820779 (8 / 80)\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7855913818840339, val_loss: 1.7855855414432844 (9 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7861148453466087, val_loss: 1.7845964754743529 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1792336217552534\n",
            "lr 3.883937077429172e-05, batch 11, decay 0.00018999372579880553, gamma 0.43087269959236435, val accuracy 0.20689655172413793, val loss 1.7925488778523035 [31 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 3.599327095348746e-05, 'batch_size': 9, 'weight_decay': 4.620437981705745e-06, 'gamma': 0.5441579856142484}\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7903038801162587, val_loss: 1.789510677600729 (1 / 80)\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7903637741640561, val_loss: 1.7883795488056877 (2 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.789259526137221, val_loss: 1.7874711458318926 (3 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7871661915619972, val_loss: 1.7864242269487804 (4 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7869992991460415, val_loss: 1.785475764955793 (5 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7852324287145778, val_loss: 1.784562998217315 (6 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7853642168681614, val_loss: 1.7835972467666776 (7 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7839785507789205, val_loss: 1.7825771828590355 (8 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.784311771982238, val_loss: 1.7816898141588484 (9 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7826340994227803, val_loss: 1.780665792855136 (10 / 80)\n",
            "underfit -> train_accuracy = 0.1965389369592089\n",
            "lr 3.599327095348746e-05, batch 9, decay 4.620437981705745e-06, gamma 0.5441579856142484, val accuracy 0.18226600985221675, val loss 1.789510677600729 [32 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 3.198340929319736e-05, 'batch_size': 8, 'weight_decay': 3.1784350343264584e-05, 'gamma': 0.6970499109344321}\n",
            "train_acc: 0.15451174289245984, val_acc: 0.18226600985221675, train_loss: 1.7928273123775336, val_loss: 1.7924881515831783 (1 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7922214862000663, val_loss: 1.7912744412868482 (2 / 80)\n",
            "train_acc: 0.173053152039555, val_acc: 0.18226600985221675, train_loss: 1.7924582082939384, val_loss: 1.7901339108133552 (3 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7902392411261465, val_loss: 1.7890180507904203 (4 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7884125431034856, val_loss: 1.7877352055657674 (5 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7879710526047885, val_loss: 1.786680124663367 (6 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7865245489608519, val_loss: 1.7855473686321615 (7 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7856711953767888, val_loss: 1.7843824648504774 (8 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.785597116602366, val_loss: 1.783299290487919 (9 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.784121125531285, val_loss: 1.7821250032321574 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18541409147095178\n",
            "lr 3.198340929319736e-05, batch 8, decay 3.1784350343264584e-05, gamma 0.6970499109344321, val accuracy 0.18226600985221675, val loss 1.7924881515831783 [33 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0004399454097313953, 'batch_size': 11, 'weight_decay': 3.263890362796817e-05, 'gamma': 0.028474867178585227}\n",
            "train_acc: 0.18294190358467244, val_acc: 0.22167487684729065, train_loss: 1.786174948194855, val_loss: 1.7808288787973339 (1 / 80)\n",
            "train_acc: 0.21137206427688504, val_acc: 0.270935960591133, train_loss: 1.7776872016885519, val_loss: 1.7687880534843858 (2 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18719211822660098, train_loss: 1.7658059137271274, val_loss: 1.7556556422134926 (3 / 80)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.27586206896551724, train_loss: 1.7610154384735635, val_loss: 1.7482037033353532 (4 / 80)\n",
            "train_acc: 0.2126081582200247, val_acc: 0.2413793103448276, train_loss: 1.7498694127805448, val_loss: 1.739262146315551 (5 / 80)\n",
            "train_acc: 0.22373300370828184, val_acc: 0.27586206896551724, train_loss: 1.7482927093105056, val_loss: 1.726956122027242 (6 / 80)\n",
            "train_acc: 0.24598269468479605, val_acc: 0.2413793103448276, train_loss: 1.7343539387098201, val_loss: 1.7036950323969273 (7 / 80)\n",
            "train_acc: 0.28553770086526575, val_acc: 0.24630541871921183, train_loss: 1.7095959328013681, val_loss: 1.686984711679919 (8 / 80)\n",
            "train_acc: 0.3016069221260816, val_acc: 0.3054187192118227, train_loss: 1.6659774316108713, val_loss: 1.5769165214059389 (9 / 80)\n",
            "train_acc: 0.3288009888751545, val_acc: 0.3251231527093596, train_loss: 1.605791806585267, val_loss: 1.5711847625929733 (10 / 80)\n",
            "train_acc: 0.30778739184178, val_acc: 0.32019704433497537, train_loss: 1.5653303758470325, val_loss: 1.5019696986146749 (11 / 80)\n",
            "train_acc: 0.34487021013597036, val_acc: 0.3645320197044335, train_loss: 1.54336654756803, val_loss: 1.6111215065265525 (12 / 80)\n",
            "train_acc: 0.3411619283065513, val_acc: 0.3251231527093596, train_loss: 1.5341585961496285, val_loss: 1.4745340106522509 (13 / 80)\n",
            "train_acc: 0.3535228677379481, val_acc: 0.3645320197044335, train_loss: 1.5260404295326016, val_loss: 1.4710213750454004 (14 / 80)\n",
            "train_acc: 0.3695920889987639, val_acc: 0.2955665024630542, train_loss: 1.5162324850874571, val_loss: 1.531145482814958 (15 / 80)\n",
            "train_acc: 0.35970333745364647, val_acc: 0.3645320197044335, train_loss: 1.4857536969874492, val_loss: 1.4574556526879372 (16 / 80)\n",
            "train_acc: 0.35970333745364647, val_acc: 0.39408866995073893, train_loss: 1.5002803102559301, val_loss: 1.4244163617711936 (17 / 80)\n",
            "train_acc: 0.37082818294190356, val_acc: 0.41379310344827586, train_loss: 1.4500172297356302, val_loss: 1.4028443168536784 (18 / 80)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.3891625615763547, train_loss: 1.4843863841187674, val_loss: 1.416801592986572 (19 / 80)\n",
            "train_acc: 0.377008652657602, val_acc: 0.3645320197044335, train_loss: 1.4639962198413052, val_loss: 1.4153139514876116 (20 / 80)\n",
            "train_acc: 0.41903584672435107, val_acc: 0.4088669950738916, train_loss: 1.4358703169893423, val_loss: 1.4060507043829105 (21 / 80)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.43349753694581283, train_loss: 1.4140312637769985, val_loss: 1.3938969461788684 (22 / 80)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.37438423645320196, train_loss: 1.423202447455068, val_loss: 1.374310329042632 (23 / 80)\n",
            "train_acc: 0.40667490729295425, val_acc: 0.4039408866995074, train_loss: 1.4092237488153692, val_loss: 1.3654413707737851 (24 / 80)\n",
            "train_acc: 0.37824474660074164, val_acc: 0.43842364532019706, train_loss: 1.4175471169102767, val_loss: 1.342031460090224 (25 / 80)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.4187192118226601, train_loss: 1.3809293593994914, val_loss: 1.322366358611384 (26 / 80)\n",
            "train_acc: 0.4177997527812114, val_acc: 0.3842364532019704, train_loss: 1.382617865195233, val_loss: 1.3168861824890663 (27 / 80)\n",
            "train_acc: 0.3980222496909765, val_acc: 0.42857142857142855, train_loss: 1.3480592064273962, val_loss: 1.324254757665061 (28 / 80)\n",
            "train_acc: 0.415327564894932, val_acc: 0.41379310344827586, train_loss: 1.3335929773793969, val_loss: 1.3677612375743284 (29 / 80)\n",
            "train_acc: 0.40914709517923364, val_acc: 0.4630541871921182, train_loss: 1.346611248842718, val_loss: 1.2886023060441605 (30 / 80)\n",
            "train_acc: 0.4215080346106304, val_acc: 0.4187192118226601, train_loss: 1.3319973962563372, val_loss: 1.3461772256296844 (31 / 80)\n",
            "train_acc: 0.4264524103831891, val_acc: 0.43842364532019706, train_loss: 1.3539280892155225, val_loss: 1.276640339437964 (32 / 80)\n",
            "train_acc: 0.4511742892459827, val_acc: 0.4876847290640394, train_loss: 1.3005333492723472, val_loss: 1.2663202467810344 (33 / 80)\n",
            "train_acc: 0.44746600741656367, val_acc: 0.47783251231527096, train_loss: 1.302263988184251, val_loss: 1.2794148772220892 (34 / 80)\n",
            "train_acc: 0.449938195302843, val_acc: 0.4187192118226601, train_loss: 1.3023242492168885, val_loss: 1.2681040942962534 (35 / 80)\n",
            "train_acc: 0.44870210135970334, val_acc: 0.47783251231527096, train_loss: 1.2969016912399027, val_loss: 1.2572762191002005 (36 / 80)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.5073891625615764, train_loss: 1.270869871668232, val_loss: 1.2298295856109394 (37 / 80)\n",
            "train_acc: 0.44252163164400493, val_acc: 0.47783251231527096, train_loss: 1.2780724841231026, val_loss: 1.210747679759716 (38 / 80)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.4827586206896552, train_loss: 1.267361853193441, val_loss: 1.2433313471930367 (39 / 80)\n",
            "train_acc: 0.4857849196538937, val_acc: 0.4827586206896552, train_loss: 1.2292726305860229, val_loss: 1.2155800417726264 (40 / 80)\n",
            "train_acc: 0.511742892459827, val_acc: 0.4433497536945813, train_loss: 1.208809780470373, val_loss: 1.2495703773545515 (41 / 80)\n",
            "train_acc: 0.49938195302843014, val_acc: 0.42857142857142855, train_loss: 1.2132038846151496, val_loss: 1.1847744308081753 (42 / 80)\n",
            "train_acc: 0.5105067985166872, val_acc: 0.4827586206896552, train_loss: 1.1973712358103399, val_loss: 1.1748961489188847 (43 / 80)\n",
            "train_acc: 0.5006180469715699, val_acc: 0.45320197044334976, train_loss: 1.194709660759373, val_loss: 1.2745383111714141 (44 / 80)\n",
            "train_acc: 0.4956736711990111, val_acc: 0.4433497536945813, train_loss: 1.1802830388136347, val_loss: 1.2082030641034318 (45 / 80)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.46798029556650245, train_loss: 1.143044337943399, val_loss: 1.1855918512555765 (46 / 80)\n",
            "train_acc: 0.5451174289245982, val_acc: 0.5024630541871922, train_loss: 1.127315458614835, val_loss: 1.1577239071794332 (47 / 80)\n",
            "train_acc: 0.5414091470951793, val_acc: 0.4827586206896552, train_loss: 1.123570757463335, val_loss: 1.14254816823405 (48 / 80)\n",
            "train_acc: 0.5550061804697157, val_acc: 0.5073891625615764, train_loss: 1.0736619057997197, val_loss: 1.1287611370603439 (49 / 80)\n",
            "train_acc: 0.6044499381953028, val_acc: 0.5073891625615764, train_loss: 1.035278143414166, val_loss: 1.1250845000074414 (50 / 80)\n",
            "train_acc: 0.5636588380716935, val_acc: 0.5073891625615764, train_loss: 1.050646655491757, val_loss: 1.131116128613796 (51 / 80)\n",
            "train_acc: 0.5920889987639061, val_acc: 0.5123152709359606, train_loss: 1.028489035935278, val_loss: 1.1269027106280398 (52 / 80)\n",
            "train_acc: 0.5933250927070457, val_acc: 0.5221674876847291, train_loss: 1.0269387605311107, val_loss: 1.1265894488748072 (53 / 80)\n",
            "train_acc: 0.5871446229913473, val_acc: 0.5270935960591133, train_loss: 1.02478223383353, val_loss: 1.1229615669532362 (54 / 80)\n",
            "train_acc: 0.5920889987639061, val_acc: 0.5172413793103449, train_loss: 1.0224977455416187, val_loss: 1.1230847571283726 (55 / 80)\n",
            "train_acc: 0.5933250927070457, val_acc: 0.5221674876847291, train_loss: 1.0059685161882925, val_loss: 1.123695067290602 (56 / 80)\n",
            "train_acc: 0.6007416563658838, val_acc: 0.5123152709359606, train_loss: 1.010048722454585, val_loss: 1.1328006295735025 (57 / 80)\n",
            "train_acc: 0.5723114956736712, val_acc: 0.5172413793103449, train_loss: 1.0095272776664999, val_loss: 1.12807629202387 (58 / 80)\n",
            "train_acc: 0.5945611866501854, val_acc: 0.5320197044334976, train_loss: 1.0105207125910722, val_loss: 1.1262729640664726 (59 / 80)\n",
            "train_acc: 0.5995055624227441, val_acc: 0.5172413793103449, train_loss: 1.009282129435663, val_loss: 1.1226033286508081 (60 / 80)\n",
            "train_acc: 0.6131025957972805, val_acc: 0.5172413793103449, train_loss: 0.9894624489494248, val_loss: 1.1316954343776984 (61 / 80)\n",
            "train_acc: 0.5896168108776267, val_acc: 0.5073891625615764, train_loss: 1.0077499527719023, val_loss: 1.122639370669285 (62 / 80)\n",
            "train_acc: 0.5982694684796045, val_acc: 0.5172413793103449, train_loss: 0.9903117130949117, val_loss: 1.1170176434986696 (63 / 80)\n",
            "train_acc: 0.622991347342398, val_acc: 0.5320197044334976, train_loss: 0.9711798623259489, val_loss: 1.1205902915870027 (64 / 80)\n",
            "train_acc: 0.6081582200247219, val_acc: 0.5369458128078818, train_loss: 0.9860505831919435, val_loss: 1.124489773670441 (65 / 80)\n",
            "train_acc: 0.6093943139678616, val_acc: 0.5369458128078818, train_loss: 0.9847337041045591, val_loss: 1.125073643740762 (66 / 80)\n",
            "train_acc: 0.6056860321384425, val_acc: 0.5369458128078818, train_loss: 0.9943706465445283, val_loss: 1.1233507253853559 (67 / 80)\n",
            "train_acc: 0.5995055624227441, val_acc: 0.5270935960591133, train_loss: 0.9948651758201043, val_loss: 1.1209997032663506 (68 / 80)\n",
            "train_acc: 0.6254635352286774, val_acc: 0.5172413793103449, train_loss: 0.9478679495335215, val_loss: 1.1194363339193936 (69 / 80)\n",
            "train_acc: 0.6415327564894932, val_acc: 0.5270935960591133, train_loss: 0.9509896490865644, val_loss: 1.1238298853629916 (70 / 80)\n",
            "train_acc: 0.6032138442521632, val_acc: 0.5369458128078818, train_loss: 0.9662105225956779, val_loss: 1.118209304774336 (71 / 80)\n",
            "train_acc: 0.6390605686032138, val_acc: 0.541871921182266, train_loss: 0.9593249957257944, val_loss: 1.1236164769515615 (72 / 80)\n",
            "train_acc: 0.6044499381953028, val_acc: 0.5172413793103449, train_loss: 0.9908442782383178, val_loss: 1.12382774781711 (73 / 80)\n",
            "train_acc: 0.61557478368356, val_acc: 0.5270935960591133, train_loss: 0.9493398034602072, val_loss: 1.1342344372143298 (74 / 80)\n",
            "train_acc: 0.6007416563658838, val_acc: 0.5369458128078818, train_loss: 0.9813566204938664, val_loss: 1.1290425505544164 (75 / 80)\n",
            "train_acc: 0.619283065512979, val_acc: 0.5369458128078818, train_loss: 0.9760363628749352, val_loss: 1.120019984069129 (76 / 80)\n",
            "train_acc: 0.619283065512979, val_acc: 0.5320197044334976, train_loss: 0.9467313009669961, val_loss: 1.124017425652208 (77 / 80)\n",
            "train_acc: 0.622991347342398, val_acc: 0.5270935960591133, train_loss: 0.9475437888993026, val_loss: 1.1271809404119482 (78 / 80)\n",
            "train_acc: 0.6044499381953028, val_acc: 0.5270935960591133, train_loss: 0.9654337595066123, val_loss: 1.1222557821884531 (79 / 80)\n",
            "train_acc: 0.6353522867737948, val_acc: 0.5270935960591133, train_loss: 0.9480480616555373, val_loss: 1.1230442444679185 (80 / 80)\n",
            "lr 0.0004399454097313953, batch 11, decay 3.263890362796817e-05, gamma 0.028474867178585227, val accuracy 0.541871921182266, val loss 1.1236164769515615 [34 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00036180239778692643, 'batch_size': 12, 'weight_decay': 2.5691244557863772e-05, 'gamma': 0.7369094218288808}\n",
            "train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.790940792657831, val_loss: 1.7864045568287665 (1 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.18226600985221675, train_loss: 1.7855782557476878, val_loss: 1.7799846333235942 (2 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.780024998120236, val_loss: 1.7741656015659202 (3 / 80)\n",
            "train_acc: 0.20024721878862795, val_acc: 0.18226600985221675, train_loss: 1.7723952103014782, val_loss: 1.7665244393748016 (4 / 80)\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7656263006927055, val_loss: 1.7580997151107036 (5 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7629583163373403, val_loss: 1.7523001726037764 (6 / 80)\n",
            "train_acc: 0.2088998763906057, val_acc: 0.18226600985221675, train_loss: 1.7553530035561182, val_loss: 1.7466757550028158 (7 / 80)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.18226600985221675, train_loss: 1.7568423158600834, val_loss: 1.741093640844223 (8 / 80)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7459801563668458, val_loss: 1.734224874985042 (9 / 80)\n",
            "train_acc: 0.2249690976514215, val_acc: 0.3103448275862069, train_loss: 1.7435742529714653, val_loss: 1.7242449345847068 (10 / 80)\n",
            "underfit -> train_accuracy = 0.2249690976514215\n",
            "lr 0.00036180239778692643, batch 12, decay 2.5691244557863772e-05, gamma 0.7369094218288808, val accuracy 0.3103448275862069, val loss 1.7242449345847068 [35 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 5.628622738889949e-05, 'batch_size': 11, 'weight_decay': 2.3498178071729428e-05, 'gamma': 0.02075261186968871}\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7898308451302414, val_loss: 1.7888959986822945 (1 / 80)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.1921182266009852, train_loss: 1.788151834892254, val_loss: 1.787287297507225 (2 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.27586206896551724, train_loss: 1.7873030638665293, val_loss: 1.7857967808916064 (3 / 80)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.2315270935960591, train_loss: 1.786762634520183, val_loss: 1.7842531744482482 (4 / 80)\n",
            "train_acc: 0.21013597033374537, val_acc: 0.21674876847290642, train_loss: 1.7845484427969593, val_loss: 1.7826549161243908 (5 / 80)\n",
            "train_acc: 0.2088998763906057, val_acc: 0.20689655172413793, train_loss: 1.7841918419848561, val_loss: 1.7809355300048302 (6 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.1921182266009852, train_loss: 1.782309803149314, val_loss: 1.7793796215151332 (7 / 80)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.1921182266009852, train_loss: 1.7809634364874314, val_loss: 1.7777826046121532 (8 / 80)\n",
            "train_acc: 0.2126081582200247, val_acc: 0.18226600985221675, train_loss: 1.7784683349253367, val_loss: 1.7759712458831336 (9 / 80)\n",
            "train_acc: 0.207663782447466, val_acc: 0.18226600985221675, train_loss: 1.7766911793403484, val_loss: 1.7741898192560732 (10 / 80)\n",
            "underfit -> train_accuracy = 0.207663782447466\n",
            "lr 5.628622738889949e-05, batch 11, decay 2.3498178071729428e-05, gamma 0.02075261186968871, val accuracy 0.27586206896551724, val loss 1.7857967808916064 [36 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0008652828929293144, 'batch_size': 12, 'weight_decay': 0.00015810245434232114, 'gamma': 0.056134226755877197}\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18719211822660098, train_loss: 1.7843857736905808, val_loss: 1.77100556237357 (1 / 80)\n",
            "train_acc: 0.18170580964153277, val_acc: 0.21182266009852216, train_loss: 1.7645570083955309, val_loss: 1.7492770537954245 (2 / 80)\n",
            "train_acc: 0.207663782447466, val_acc: 0.1921182266009852, train_loss: 1.7560460518847585, val_loss: 1.7381467284827397 (3 / 80)\n",
            "train_acc: 0.21631644004944375, val_acc: 0.18226600985221675, train_loss: 1.733821499627659, val_loss: 1.7206286062747973 (4 / 80)\n",
            "train_acc: 0.26081582200247216, val_acc: 0.270935960591133, train_loss: 1.7120504806598715, val_loss: 1.6625629880745423 (5 / 80)\n",
            "train_acc: 0.31025957972805934, val_acc: 0.31527093596059114, train_loss: 1.6491083347311126, val_loss: 1.5536053362738322 (6 / 80)\n",
            "train_acc: 0.2904820766378245, val_acc: 0.2857142857142857, train_loss: 1.6200772487041535, val_loss: 1.6014699636421768 (7 / 80)\n",
            "train_acc: 0.33250927070457353, val_acc: 0.35467980295566504, train_loss: 1.590594791806083, val_loss: 1.5212817843911683 (8 / 80)\n",
            "train_acc: 0.34487021013597036, val_acc: 0.2857142857142857, train_loss: 1.5438696664697014, val_loss: 1.4846553732021688 (9 / 80)\n",
            "train_acc: 0.3473423980222497, val_acc: 0.3251231527093596, train_loss: 1.5151075764256443, val_loss: 1.4589152212800651 (10 / 80)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.35467980295566504, train_loss: 1.4694900385970975, val_loss: 1.4407652634117991 (11 / 80)\n",
            "train_acc: 0.377008652657602, val_acc: 0.39408866995073893, train_loss: 1.4461339069680024, val_loss: 1.4375635667387487 (12 / 80)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.43842364532019706, train_loss: 1.434220207636377, val_loss: 1.3582016399928503 (13 / 80)\n",
            "train_acc: 0.37330037082818296, val_acc: 0.3842364532019704, train_loss: 1.4857000432291492, val_loss: 1.3707127870597275 (14 / 80)\n",
            "train_acc: 0.39060568603213847, val_acc: 0.42857142857142855, train_loss: 1.433487373170511, val_loss: 1.4189287118723828 (15 / 80)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.46798029556650245, train_loss: 1.388946308046395, val_loss: 1.325936642186395 (16 / 80)\n",
            "train_acc: 0.4227441285537701, val_acc: 0.4039408866995074, train_loss: 1.3906634011581005, val_loss: 1.356097498550791 (17 / 80)\n",
            "train_acc: 0.44499381953028433, val_acc: 0.43842364532019706, train_loss: 1.327712007299784, val_loss: 1.2920915099787595 (18 / 80)\n",
            "train_acc: 0.4326328800988875, val_acc: 0.39901477832512317, train_loss: 1.3432538917539145, val_loss: 1.4147894499924383 (19 / 80)\n",
            "train_acc: 0.41903584672435107, val_acc: 0.458128078817734, train_loss: 1.328394416383524, val_loss: 1.2798945222582137 (20 / 80)\n",
            "train_acc: 0.4252163164400494, val_acc: 0.4729064039408867, train_loss: 1.3225058473379563, val_loss: 1.2417410641468216 (21 / 80)\n",
            "train_acc: 0.446229913473424, val_acc: 0.4236453201970443, train_loss: 1.2970434490623521, val_loss: 1.2878327093688138 (22 / 80)\n",
            "train_acc: 0.4289245982694685, val_acc: 0.46798029556650245, train_loss: 1.3066334314782482, val_loss: 1.220058940314307 (23 / 80)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.5024630541871922, train_loss: 1.2890165768683473, val_loss: 1.255091988981651 (24 / 80)\n",
            "train_acc: 0.4561186650185414, val_acc: 0.47783251231527096, train_loss: 1.2523242557888714, val_loss: 1.2437462642275054 (25 / 80)\n",
            "train_acc: 0.4647713226205192, val_acc: 0.4729064039408867, train_loss: 1.2660097268367432, val_loss: 1.2665851122052798 (26 / 80)\n",
            "train_acc: 0.48331273176761436, val_acc: 0.5024630541871922, train_loss: 1.2450143207873048, val_loss: 1.213479634576243 (27 / 80)\n",
            "train_acc: 0.5253399258343634, val_acc: 0.49261083743842365, train_loss: 1.1850928474857723, val_loss: 1.166396155733193 (28 / 80)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.541871921182266, train_loss: 1.2106214151688943, val_loss: 1.1437185515323882 (29 / 80)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.4433497536945813, train_loss: 1.188889549158707, val_loss: 1.2454943974029842 (30 / 80)\n",
            "train_acc: 0.5290482076637825, val_acc: 0.4827586206896552, train_loss: 1.142151001136294, val_loss: 1.145571494924611 (31 / 80)\n",
            "train_acc: 0.515451174289246, val_acc: 0.5024630541871922, train_loss: 1.1449763079361508, val_loss: 1.1314082997185844 (32 / 80)\n",
            "train_acc: 0.5488257107540173, val_acc: 0.49261083743842365, train_loss: 1.106393680731652, val_loss: 1.1596733267084132 (33 / 80)\n",
            "train_acc: 0.5377008652657602, val_acc: 0.4729064039408867, train_loss: 1.0839582082662358, val_loss: 1.1841983160949106 (34 / 80)\n",
            "train_acc: 0.5611866501854141, val_acc: 0.5172413793103449, train_loss: 1.0682699404776612, val_loss: 1.1515559703845697 (35 / 80)\n",
            "train_acc: 0.5871446229913473, val_acc: 0.5221674876847291, train_loss: 1.0176640989164487, val_loss: 1.0990175234860386 (36 / 80)\n",
            "train_acc: 0.5648949320148331, val_acc: 0.5369458128078818, train_loss: 1.0176875025439174, val_loss: 1.1102534685228846 (37 / 80)\n",
            "train_acc: 0.5822002472187886, val_acc: 0.4827586206896552, train_loss: 0.9844393548181531, val_loss: 1.1835616744797806 (38 / 80)\n",
            "train_acc: 0.595797280593325, val_acc: 0.5763546798029556, train_loss: 0.9485775696848172, val_loss: 1.0594536512356085 (39 / 80)\n",
            "train_acc: 0.619283065512979, val_acc: 0.5172413793103449, train_loss: 0.927586193285118, val_loss: 1.1333699288039372 (40 / 80)\n",
            "train_acc: 0.6637824474660075, val_acc: 0.5566502463054187, train_loss: 0.8683675810492083, val_loss: 1.0564792737584983 (41 / 80)\n",
            "train_acc: 0.6501854140914709, val_acc: 0.5566502463054187, train_loss: 0.8714758255720433, val_loss: 1.2522004296626952 (42 / 80)\n",
            "train_acc: 0.6662546353522868, val_acc: 0.5270935960591133, train_loss: 0.8474766792561421, val_loss: 1.087948281776729 (43 / 80)\n",
            "train_acc: 0.688504326328801, val_acc: 0.5862068965517241, train_loss: 0.7707816268074498, val_loss: 0.997999136083819 (44 / 80)\n",
            "train_acc: 0.6847960444993819, val_acc: 0.5714285714285714, train_loss: 0.7343268505722395, val_loss: 1.0875994789189305 (45 / 80)\n",
            "train_acc: 0.7218788627935723, val_acc: 0.5862068965517241, train_loss: 0.6892400689708582, val_loss: 1.151689569351121 (46 / 80)\n",
            "train_acc: 0.723114956736712, val_acc: 0.625615763546798, train_loss: 0.7282105621654996, val_loss: 1.0770345509345896 (47 / 80)\n",
            "train_acc: 0.7700865265760197, val_acc: 0.5862068965517241, train_loss: 0.6285560884051329, val_loss: 1.084771000105759 (48 / 80)\n",
            "train_acc: 0.8516687268232386, val_acc: 0.6206896551724138, train_loss: 0.4791004715213375, val_loss: 1.0109444636429472 (49 / 80)\n",
            "train_acc: 0.8726823238566132, val_acc: 0.645320197044335, train_loss: 0.40177820348474386, val_loss: 1.0160301753452845 (50 / 80)\n",
            "train_acc: 0.861557478368356, val_acc: 0.6354679802955665, train_loss: 0.4090761588368339, val_loss: 1.0337259687226394 (51 / 80)\n",
            "train_acc: 0.8516687268232386, val_acc: 0.6354679802955665, train_loss: 0.3957240791307539, val_loss: 1.0483422343954076 (52 / 80)\n",
            "train_acc: 0.8788627935723115, val_acc: 0.6403940886699507, train_loss: 0.35914032818802505, val_loss: 1.047302991885857 (53 / 80)\n",
            "train_acc: 0.8862793572311496, val_acc: 0.645320197044335, train_loss: 0.33931027358335675, val_loss: 1.0650087060599491 (54 / 80)\n",
            "overfit -> train_accuracy-val_accuracy = 0.25328356482186243\n",
            "lr 0.0008652828929293144, batch 12, decay 0.00015810245434232114, gamma 0.056134226755877197, val accuracy 0.645320197044335, val loss 1.0160301753452845 [37 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00038041059192815333, 'batch_size': 9, 'weight_decay': 3.8372561126798785e-05, 'gamma': 0.057680309789029396}\n",
            "train_acc: 0.16440049443757726, val_acc: 0.2413793103448276, train_loss: 1.789699145978402, val_loss: 1.7833059532889004 (1 / 80)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7810269087295154, val_loss: 1.773933797047056 (2 / 80)\n",
            "train_acc: 0.2027194066749073, val_acc: 0.18226600985221675, train_loss: 1.7694724599274776, val_loss: 1.7606091511073372 (3 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.762833270213218, val_loss: 1.7518095629555839 (4 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.756026119914721, val_loss: 1.743641816923771 (5 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7483977110335795, val_loss: 1.7338300549925254 (6 / 80)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.2660098522167488, train_loss: 1.7370697995198818, val_loss: 1.7191161293114348 (7 / 80)\n",
            "train_acc: 0.29295426452410384, val_acc: 0.23645320197044334, train_loss: 1.7213686611802674, val_loss: 1.6981421603357851 (8 / 80)\n",
            "train_acc: 0.28059332509270707, val_acc: 0.30049261083743845, train_loss: 1.6925509071762688, val_loss: 1.6457968762355486 (9 / 80)\n",
            "train_acc: 0.31396786155747836, val_acc: 0.30049261083743845, train_loss: 1.6514953033001667, val_loss: 1.5798468008417215 (10 / 80)\n",
            "train_acc: 0.34857849196538937, val_acc: 0.3054187192118227, train_loss: 1.58606943344451, val_loss: 1.6242469226198244 (11 / 80)\n",
            "train_acc: 0.3399258343634116, val_acc: 0.3399014778325123, train_loss: 1.550743426321758, val_loss: 1.4665952938530833 (12 / 80)\n",
            "train_acc: 0.34239802224969096, val_acc: 0.3842364532019704, train_loss: 1.523548985293828, val_loss: 1.4593934032130125 (13 / 80)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.4187192118226601, train_loss: 1.4858193323845033, val_loss: 1.4002782455806075 (14 / 80)\n",
            "train_acc: 0.39555006180469715, val_acc: 0.30049261083743845, train_loss: 1.4825138260319148, val_loss: 1.5071833662211602 (15 / 80)\n",
            "train_acc: 0.38442521631644005, val_acc: 0.39408866995073893, train_loss: 1.4516359367240934, val_loss: 1.4175154270209702 (16 / 80)\n",
            "train_acc: 0.3967861557478368, val_acc: 0.42857142857142855, train_loss: 1.4140656191132714, val_loss: 1.3528993652371937 (17 / 80)\n",
            "train_acc: 0.411619283065513, val_acc: 0.4187192118226601, train_loss: 1.4258037532215802, val_loss: 1.372227030434632 (18 / 80)\n",
            "train_acc: 0.411619283065513, val_acc: 0.4187192118226601, train_loss: 1.4167322206261426, val_loss: 1.3982642378125871 (19 / 80)\n",
            "train_acc: 0.3967861557478368, val_acc: 0.4236453201970443, train_loss: 1.3783770673649274, val_loss: 1.3286990003632795 (20 / 80)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.39408866995073893, train_loss: 1.4010589791758836, val_loss: 1.3357034123002602 (21 / 80)\n",
            "train_acc: 0.43510506798516685, val_acc: 0.43842364532019706, train_loss: 1.3622694155489117, val_loss: 1.287375793668437 (22 / 80)\n",
            "train_acc: 0.4079110012360939, val_acc: 0.43842364532019706, train_loss: 1.3719997668295767, val_loss: 1.2912643158377097 (23 / 80)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.41379310344827586, train_loss: 1.3514800223490806, val_loss: 1.289079248611563 (24 / 80)\n",
            "train_acc: 0.43757725587144625, val_acc: 0.41379310344827586, train_loss: 1.3514914391804096, val_loss: 1.3695918729739824 (25 / 80)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.3842364532019704, train_loss: 1.3232598751967564, val_loss: 1.3076786155183915 (26 / 80)\n",
            "train_acc: 0.42398022249690975, val_acc: 0.45320197044334976, train_loss: 1.3564520120915435, val_loss: 1.267128479304572 (27 / 80)\n",
            "train_acc: 0.47095179233621753, val_acc: 0.4433497536945813, train_loss: 1.2938227598245713, val_loss: 1.2717401038836964 (28 / 80)\n",
            "train_acc: 0.4647713226205192, val_acc: 0.458128078817734, train_loss: 1.2894013779714464, val_loss: 1.2283997104085724 (29 / 80)\n",
            "train_acc: 0.45488257107540175, val_acc: 0.4975369458128079, train_loss: 1.2727721500750377, val_loss: 1.2238688281016985 (30 / 80)\n",
            "train_acc: 0.44870210135970334, val_acc: 0.4876847290640394, train_loss: 1.2753340598237235, val_loss: 1.1902123578076291 (31 / 80)\n",
            "train_acc: 0.4820766378244747, val_acc: 0.43349753694581283, train_loss: 1.2579303232789483, val_loss: 1.3301458323530375 (32 / 80)\n",
            "train_acc: 0.47713226205191595, val_acc: 0.3891625615763547, train_loss: 1.2524179988650073, val_loss: 1.3812076836971228 (33 / 80)\n",
            "train_acc: 0.484548825710754, val_acc: 0.4729064039408867, train_loss: 1.2013437782583485, val_loss: 1.2122017406477716 (34 / 80)\n",
            "train_acc: 0.48084054388133496, val_acc: 0.5073891625615764, train_loss: 1.232530011013795, val_loss: 1.1953774088709226 (35 / 80)\n",
            "train_acc: 0.5105067985166872, val_acc: 0.4630541871921182, train_loss: 1.1668575372183132, val_loss: 1.283944638345042 (36 / 80)\n",
            "train_acc: 0.484548825710754, val_acc: 0.4729064039408867, train_loss: 1.1986657029618735, val_loss: 1.1431085097378697 (37 / 80)\n",
            "train_acc: 0.5055624227441285, val_acc: 0.47783251231527096, train_loss: 1.178603492428552, val_loss: 1.168318121450875 (38 / 80)\n",
            "train_acc: 0.5426452410383189, val_acc: 0.45320197044334976, train_loss: 1.121105963942147, val_loss: 1.160601230090475 (39 / 80)\n",
            "train_acc: 0.5475896168108776, val_acc: 0.4630541871921182, train_loss: 1.1458943884805932, val_loss: 1.3820407000081292 (40 / 80)\n",
            "train_acc: 0.553770086526576, val_acc: 0.49261083743842365, train_loss: 1.131248204463492, val_loss: 1.14056606686174 (41 / 80)\n",
            "train_acc: 0.553770086526576, val_acc: 0.5566502463054187, train_loss: 1.1180136631092124, val_loss: 1.0755397163588425 (42 / 80)\n",
            "train_acc: 0.515451174289246, val_acc: 0.5320197044334976, train_loss: 1.1385461866708269, val_loss: 1.123633583778231 (43 / 80)\n",
            "train_acc: 0.5339925834363412, val_acc: 0.49261083743842365, train_loss: 1.0979731450564192, val_loss: 1.1619535060645325 (44 / 80)\n",
            "train_acc: 0.5525339925834364, val_acc: 0.5369458128078818, train_loss: 1.074384482519883, val_loss: 1.1025877031199451 (45 / 80)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.5270935960591133, train_loss: 1.0798022662311313, val_loss: 1.0570786157852323 (46 / 80)\n",
            "train_acc: 0.584672435105068, val_acc: 0.5221674876847291, train_loss: 1.0378121127304247, val_loss: 1.1163154223869587 (47 / 80)\n",
            "train_acc: 0.5760197775030902, val_acc: 0.5270935960591133, train_loss: 1.0163203236199132, val_loss: 1.0591257919231658 (48 / 80)\n",
            "train_acc: 0.619283065512979, val_acc: 0.5566502463054187, train_loss: 0.9471913743741138, val_loss: 1.0057028452751084 (49 / 80)\n",
            "train_acc: 0.646477132262052, val_acc: 0.5665024630541872, train_loss: 0.9215139929016382, val_loss: 1.0007456935978876 (50 / 80)\n",
            "train_acc: 0.6402966625463535, val_acc: 0.5615763546798029, train_loss: 0.8984559644359004, val_loss: 0.9791356471959006 (51 / 80)\n",
            "train_acc: 0.6563658838071693, val_acc: 0.5566502463054187, train_loss: 0.8668078715706933, val_loss: 1.0032379293970286 (52 / 80)\n",
            "train_acc: 0.6551297898640297, val_acc: 0.5517241379310345, train_loss: 0.8779582401509632, val_loss: 1.018571289595712 (53 / 80)\n",
            "train_acc: 0.6637824474660075, val_acc: 0.5763546798029556, train_loss: 0.8801031286180093, val_loss: 0.9801630712495062 (54 / 80)\n",
            "train_acc: 0.6477132262051916, val_acc: 0.5615763546798029, train_loss: 0.8924079467766954, val_loss: 1.0021135011330027 (55 / 80)\n",
            "train_acc: 0.688504326328801, val_acc: 0.5467980295566502, train_loss: 0.8497784337021954, val_loss: 1.0126683300939099 (56 / 80)\n",
            "train_acc: 0.6563658838071693, val_acc: 0.5615763546798029, train_loss: 0.8474863544382772, val_loss: 0.9945154545342394 (57 / 80)\n",
            "train_acc: 0.6699629171817059, val_acc: 0.5566502463054187, train_loss: 0.8424974243042643, val_loss: 1.0033998090058125 (58 / 80)\n",
            "train_acc: 0.6773794808405439, val_acc: 0.5763546798029556, train_loss: 0.8416622982847382, val_loss: 0.9933995161150476 (59 / 80)\n",
            "train_acc: 0.6847960444993819, val_acc: 0.5615763546798029, train_loss: 0.8071522274329724, val_loss: 0.9976960787632195 (60 / 80)\n",
            "train_acc: 0.6946847960444994, val_acc: 0.5566502463054187, train_loss: 0.8215282994641362, val_loss: 1.0091240811230513 (61 / 80)\n",
            "train_acc: 0.6897404202719407, val_acc: 0.5763546798029556, train_loss: 0.7955283780843572, val_loss: 1.0005435509047484 (62 / 80)\n",
            "train_acc: 0.681087762669963, val_acc: 0.5714285714285714, train_loss: 0.7980869770123725, val_loss: 1.000019727375707 (63 / 80)\n",
            "train_acc: 0.69221260815822, val_acc: 0.5812807881773399, train_loss: 0.8149194189880333, val_loss: 0.9898994650159564 (64 / 80)\n",
            "train_acc: 0.6724351050679852, val_acc: 0.5714285714285714, train_loss: 0.8111194465010069, val_loss: 0.9930276863387065 (65 / 80)\n",
            "train_acc: 0.6749072929542645, val_acc: 0.5862068965517241, train_loss: 0.8254295859407583, val_loss: 0.9829945702270921 (66 / 80)\n",
            "train_acc: 0.6736711990111248, val_acc: 0.5862068965517241, train_loss: 0.7989088730479053, val_loss: 0.9851430111330718 (67 / 80)\n",
            "train_acc: 0.6934487021013597, val_acc: 0.5960591133004927, train_loss: 0.782064136232817, val_loss: 0.9901883309991489 (68 / 80)\n",
            "train_acc: 0.6847960444993819, val_acc: 0.5960591133004927, train_loss: 0.7883396537695886, val_loss: 0.9816824876028916 (69 / 80)\n",
            "train_acc: 0.6860321384425216, val_acc: 0.6009852216748769, train_loss: 0.7872109420514667, val_loss: 0.9719738892733757 (70 / 80)\n",
            "train_acc: 0.6773794808405439, val_acc: 0.6059113300492611, train_loss: 0.8180236025484295, val_loss: 0.978402114120023 (71 / 80)\n",
            "train_acc: 0.7008652657601978, val_acc: 0.5862068965517241, train_loss: 0.7783310677492427, val_loss: 0.9927063170324992 (72 / 80)\n",
            "train_acc: 0.6971569839307787, val_acc: 0.6059113300492611, train_loss: 0.7877881621253505, val_loss: 0.9804931332912351 (73 / 80)\n",
            "train_acc: 0.6971569839307787, val_acc: 0.6059113300492611, train_loss: 0.781498365260762, val_loss: 0.9793838459282673 (74 / 80)\n",
            "train_acc: 0.7119901112484549, val_acc: 0.6059113300492611, train_loss: 0.7491387907595216, val_loss: 0.9748054339087068 (75 / 80)\n",
            "train_acc: 0.7206427688504327, val_acc: 0.5911330049261084, train_loss: 0.7470781526105807, val_loss: 1.0032081430768731 (76 / 80)\n",
            "train_acc: 0.7070457354758962, val_acc: 0.5960591133004927, train_loss: 0.753423026303573, val_loss: 0.9681589782531625 (77 / 80)\n",
            "train_acc: 0.7391841779975278, val_acc: 0.6108374384236454, train_loss: 0.7085875446628433, val_loss: 0.9877617955207825 (78 / 80)\n",
            "train_acc: 0.7045735475896168, val_acc: 0.6009852216748769, train_loss: 0.7331925435325566, val_loss: 0.982825829477733 (79 / 80)\n",
            "train_acc: 0.7107540173053152, val_acc: 0.5665024630541872, train_loss: 0.7379149193743103, val_loss: 1.0088042766589838 (80 / 80)\n",
            "lr 0.00038041059192815333, batch 9, decay 3.8372561126798785e-05, gamma 0.057680309789029396, val accuracy 0.6108374384236454, val loss 0.9877617955207825 [38 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.5131305519230423e-05, 'batch_size': 12, 'weight_decay': 1.4860241847080794e-06, 'gamma': 0.18665953494772602}\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7913136166901464, val_loss: 1.7917295324391331 (1 / 80)\n",
            "train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.7916342562297072, val_loss: 1.7912572240594573 (2 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.790546337223171, val_loss: 1.7908374030014564 (3 / 80)\n",
            "train_acc: 0.1965389369592089, val_acc: 0.18226600985221675, train_loss: 1.7900246274191312, val_loss: 1.7904171450384732 (4 / 80)\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7900346046030449, val_loss: 1.7899431524605587 (5 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.78993772694148, val_loss: 1.78950681122653 (6 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7892567950803948, val_loss: 1.7890726380747528 (7 / 80)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.788844946583063, val_loss: 1.7886488384801178 (8 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.787718929378005, val_loss: 1.7882174881808277 (9 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.7881658476569007, val_loss: 1.7878291236943211 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18294190358467244\n",
            "lr 1.5131305519230423e-05, batch 12, decay 1.4860241847080794e-06, gamma 0.18665953494772602, val accuracy 0.18226600985221675, val loss 1.7917295324391331 [39 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 9.143832457268922e-05, 'batch_size': 8, 'weight_decay': 1.7412576566860286e-06, 'gamma': 0.12497460919416367}\n",
            "train_acc: 0.1792336217552534, val_acc: 0.2019704433497537, train_loss: 1.7909984790497864, val_loss: 1.7895606098503902 (1 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18719211822660098, train_loss: 1.7884146748720788, val_loss: 1.7865227942396267 (2 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7865664185345984, val_loss: 1.783679689679827 (3 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7834547184895821, val_loss: 1.7810627862150445 (4 / 80)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.18226600985221675, train_loss: 1.7818947768181894, val_loss: 1.7781750856361953 (5 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7800667405275832, val_loss: 1.7754626244746994 (6 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7754740712079777, val_loss: 1.771985479763576 (7 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7740626812570617, val_loss: 1.7688473057864336 (8 / 80)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7709772703232076, val_loss: 1.766038359092374 (9 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7667479130482056, val_loss: 1.7624604061906561 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18541409147095178\n",
            "lr 9.143832457268922e-05, batch 8, decay 1.7412576566860286e-06, gamma 0.12497460919416367, val accuracy 0.2019704433497537, val loss 1.7895606098503902 [40 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0005577795740508891, 'batch_size': 9, 'weight_decay': 6.486576310917708e-05, 'gamma': 0.04377479936680016}\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7860448111415057, val_loss: 1.7749524093026599 (1 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.768044903929655, val_loss: 1.7553815448225425 (2 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.2512315270935961, train_loss: 1.753446543319853, val_loss: 1.7429047182862982 (3 / 80)\n",
            "train_acc: 0.22867737948084055, val_acc: 0.22660098522167488, train_loss: 1.7498038439874448, val_loss: 1.737780574507314 (4 / 80)\n",
            "train_acc: 0.22867737948084055, val_acc: 0.3399014778325123, train_loss: 1.7406670600728436, val_loss: 1.715461546564337 (5 / 80)\n",
            "train_acc: 0.3053152039555006, val_acc: 0.2660098522167488, train_loss: 1.7030418059145123, val_loss: 1.6508275475995293 (6 / 80)\n",
            "train_acc: 0.3238566131025958, val_acc: 0.3054187192118227, train_loss: 1.6241465353111106, val_loss: 1.6159488937537658 (7 / 80)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.29064039408866993, train_loss: 1.6062922574975729, val_loss: 1.4895335406505417 (8 / 80)\n",
            "train_acc: 0.3399258343634116, val_acc: 0.3842364532019704, train_loss: 1.5619810723845833, val_loss: 1.4690268803112612 (9 / 80)\n",
            "train_acc: 0.3646477132262052, val_acc: 0.3645320197044335, train_loss: 1.5388092580625536, val_loss: 1.4589095056937833 (10 / 80)\n",
            "train_acc: 0.3374536464771323, val_acc: 0.3645320197044335, train_loss: 1.5439506674873815, val_loss: 1.4598229512792502 (11 / 80)\n",
            "train_acc: 0.36711990111248455, val_acc: 0.35960591133004927, train_loss: 1.4966882977114324, val_loss: 1.4138836731464404 (12 / 80)\n",
            "train_acc: 0.37824474660074164, val_acc: 0.37438423645320196, train_loss: 1.4551516691745432, val_loss: 1.3803074218956708 (13 / 80)\n",
            "train_acc: 0.3695920889987639, val_acc: 0.39408866995073893, train_loss: 1.4365363617615292, val_loss: 1.3558785979970922 (14 / 80)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.3497536945812808, train_loss: 1.4319692710863499, val_loss: 1.4497304133006506 (15 / 80)\n",
            "train_acc: 0.4103831891223733, val_acc: 0.41379310344827586, train_loss: 1.408156399968528, val_loss: 1.3381615275232663 (16 / 80)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.39408866995073893, train_loss: 1.394780603857948, val_loss: 1.3109844881912758 (17 / 80)\n",
            "train_acc: 0.40667490729295425, val_acc: 0.35960591133004927, train_loss: 1.3807137508327498, val_loss: 1.4584460698912296 (18 / 80)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.43349753694581283, train_loss: 1.3571739618946213, val_loss: 1.2997960827033508 (19 / 80)\n",
            "train_acc: 0.40296662546353523, val_acc: 0.4630541871921182, train_loss: 1.3623128954058672, val_loss: 1.2790987609055242 (20 / 80)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.458128078817734, train_loss: 1.3460216067630075, val_loss: 1.3520245969001883 (21 / 80)\n",
            "train_acc: 0.44499381953028433, val_acc: 0.46798029556650245, train_loss: 1.324893816071476, val_loss: 1.3157438406803337 (22 / 80)\n",
            "train_acc: 0.4622991347342398, val_acc: 0.45320197044334976, train_loss: 1.3196409707163703, val_loss: 1.25650506154657 (23 / 80)\n",
            "train_acc: 0.44746600741656367, val_acc: 0.4630541871921182, train_loss: 1.2877370354861057, val_loss: 1.2648951014861685 (24 / 80)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.43349753694581283, train_loss: 1.2636452234719682, val_loss: 1.2182318441973532 (25 / 80)\n",
            "train_acc: 0.44252163164400493, val_acc: 0.4433497536945813, train_loss: 1.2869304369200292, val_loss: 1.3513341096821676 (26 / 80)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.4827586206896552, train_loss: 1.2588890806411488, val_loss: 1.16028305051362 (27 / 80)\n",
            "train_acc: 0.484548825710754, val_acc: 0.5270935960591133, train_loss: 1.2123961188589834, val_loss: 1.1844387163082366 (28 / 80)\n",
            "train_acc: 0.4857849196538937, val_acc: 0.47783251231527096, train_loss: 1.1833183229043251, val_loss: 1.1625927087708647 (29 / 80)\n",
            "train_acc: 0.5018541409147095, val_acc: 0.46798029556650245, train_loss: 1.1785187511641546, val_loss: 1.1630036933668728 (30 / 80)\n",
            "train_acc: 0.4894932014833127, val_acc: 0.4876847290640394, train_loss: 1.1678708786575402, val_loss: 1.16511981592977 (31 / 80)\n",
            "train_acc: 0.4956736711990111, val_acc: 0.458128078817734, train_loss: 1.199310521702242, val_loss: 1.2572202010107745 (32 / 80)\n",
            "train_acc: 0.5080346106304079, val_acc: 0.4827586206896552, train_loss: 1.1594068340966375, val_loss: 1.2239406795924521 (33 / 80)\n",
            "train_acc: 0.5278121137206427, val_acc: 0.4630541871921182, train_loss: 1.1190215200369968, val_loss: 1.1852889025739848 (34 / 80)\n",
            "train_acc: 0.5735475896168108, val_acc: 0.5320197044334976, train_loss: 1.0706860112171386, val_loss: 1.1690910966525525 (35 / 80)\n",
            "train_acc: 0.5500618046971569, val_acc: 0.5024630541871922, train_loss: 1.0931753814588814, val_loss: 1.1547145482354564 (36 / 80)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.47783251231527096, train_loss: 1.0665078894022222, val_loss: 1.1968908497852644 (37 / 80)\n",
            "train_acc: 0.5587144622991347, val_acc: 0.5221674876847291, train_loss: 1.058805494284895, val_loss: 1.0853376341570775 (38 / 80)\n",
            "train_acc: 0.5982694684796045, val_acc: 0.541871921182266, train_loss: 1.0053871414717401, val_loss: 1.074749557842762 (39 / 80)\n",
            "train_acc: 0.6131025957972805, val_acc: 0.5862068965517241, train_loss: 1.001449574904035, val_loss: 1.0439184857119481 (40 / 80)\n",
            "train_acc: 0.5982694684796045, val_acc: 0.5172413793103449, train_loss: 0.9892408337596026, val_loss: 1.0857425666794989 (41 / 80)\n",
            "train_acc: 0.6242274412855378, val_acc: 0.5123152709359606, train_loss: 0.9270585697646194, val_loss: 1.2296441784633205 (42 / 80)\n",
            "train_acc: 0.6180469715698393, val_acc: 0.4729064039408867, train_loss: 0.9360625100297892, val_loss: 1.2094108048330974 (43 / 80)\n",
            "train_acc: 0.6427688504326329, val_acc: 0.5665024630541872, train_loss: 0.9113006388448519, val_loss: 1.1658721392965081 (44 / 80)\n",
            "train_acc: 0.6477132262051916, val_acc: 0.625615763546798, train_loss: 0.8544824382357014, val_loss: 1.0536775315923643 (45 / 80)\n",
            "train_acc: 0.6823238566131026, val_acc: 0.541871921182266, train_loss: 0.8083807205682337, val_loss: 1.2325139145545772 (46 / 80)\n",
            "train_acc: 0.6773794808405439, val_acc: 0.5665024630541872, train_loss: 0.8281174911777817, val_loss: 1.1548321948850095 (47 / 80)\n",
            "train_acc: 0.6798516687268232, val_acc: 0.5862068965517241, train_loss: 0.8308740752957513, val_loss: 1.039291438798012 (48 / 80)\n",
            "train_acc: 0.788627935723115, val_acc: 0.6403940886699507, train_loss: 0.6001268172146216, val_loss: 0.9633257086641096 (49 / 80)\n",
            "train_acc: 0.7713226205191595, val_acc: 0.6403940886699507, train_loss: 0.6047877709856729, val_loss: 0.9532934399367553 (50 / 80)\n",
            "train_acc: 0.7688504326328801, val_acc: 0.6403940886699507, train_loss: 0.5861179459153942, val_loss: 0.9715526011483423 (51 / 80)\n",
            "train_acc: 0.7898640296662547, val_acc: 0.6305418719211823, train_loss: 0.5559189968888633, val_loss: 0.966982281854 (52 / 80)\n",
            "train_acc: 0.8170580964153276, val_acc: 0.645320197044335, train_loss: 0.5298969560983154, val_loss: 0.9754152801530115 (53 / 80)\n",
            "train_acc: 0.8331273176761433, val_acc: 0.6206896551724138, train_loss: 0.47407404989998775, val_loss: 0.9826424810393103 (54 / 80)\n",
            "train_acc: 0.8244746600741656, val_acc: 0.6157635467980296, train_loss: 0.4847969062101414, val_loss: 0.9889489189157346 (55 / 80)\n",
            "train_acc: 0.8467243510506799, val_acc: 0.6157635467980296, train_loss: 0.4754638935454254, val_loss: 0.985907159121753 (56 / 80)\n",
            "train_acc: 0.8195302843016069, val_acc: 0.625615763546798, train_loss: 0.5044974766865059, val_loss: 0.9764502259164021 (57 / 80)\n",
            "train_acc: 0.8257107540173053, val_acc: 0.6108374384236454, train_loss: 0.48907609800583646, val_loss: 0.9831406924818537 (58 / 80)\n",
            "train_acc: 0.8257107540173053, val_acc: 0.625615763546798, train_loss: 0.48556999501177056, val_loss: 0.982388881149844 (59 / 80)\n",
            "train_acc: 0.8071693448702101, val_acc: 0.6059113300492611, train_loss: 0.4915835602008663, val_loss: 0.9938770186137683 (60 / 80)\n",
            "train_acc: 0.8393077873918418, val_acc: 0.6305418719211823, train_loss: 0.44375110674552776, val_loss: 0.9938391640855762 (61 / 80)\n",
            "train_acc: 0.8368355995055624, val_acc: 0.6305418719211823, train_loss: 0.4374901921256953, val_loss: 1.0017071245926354 (62 / 80)\n",
            "train_acc: 0.8529048207663782, val_acc: 0.6157635467980296, train_loss: 0.43561697597483034, val_loss: 1.0253871817306932 (63 / 80)\n",
            "train_acc: 0.8467243510506799, val_acc: 0.625615763546798, train_loss: 0.4339170210772598, val_loss: 1.0160069668968323 (64 / 80)\n",
            "train_acc: 0.8504326328800988, val_acc: 0.625615763546798, train_loss: 0.4450492737780248, val_loss: 1.0178608427494031 (65 / 80)\n",
            "train_acc: 0.8529048207663782, val_acc: 0.625615763546798, train_loss: 0.4264867572327995, val_loss: 1.0157744885959061 (66 / 80)\n",
            "overfit -> train_accuracy-val_accuracy = 0.2532104952291645\n",
            "lr 0.0005577795740508891, batch 9, decay 6.486576310917708e-05, gamma 0.04377479936680016, val accuracy 0.645320197044335, val loss 0.9754152801530115 [41 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00026532122590550535, 'batch_size': 14, 'weight_decay': 1.2757375261150867e-05, 'gamma': 0.8065857126536472}\n",
            "train_acc: 0.15822002472187885, val_acc: 0.18226600985221675, train_loss: 1.7893748239180067, val_loss: 1.7841870044839794 (1 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.78139954856948, val_loss: 1.7782690689481537 (2 / 80)\n",
            "train_acc: 0.1668726823238566, val_acc: 0.18226600985221675, train_loss: 1.7771753053582644, val_loss: 1.77241810847973 (3 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7712282658507414, val_loss: 1.7655865776127782 (4 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7639643835048888, val_loss: 1.7584578620976414 (5 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7581502941836533, val_loss: 1.753539800643921 (6 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7599983343529908, val_loss: 1.74990556568935 (7 / 80)\n",
            "train_acc: 0.18788627935723115, val_acc: 0.18226600985221675, train_loss: 1.7595228263267924, val_loss: 1.7463781381475514 (8 / 80)\n",
            "train_acc: 0.19406674907292953, val_acc: 0.18226600985221675, train_loss: 1.7540618266693888, val_loss: 1.743560840343607 (9 / 80)\n",
            "train_acc: 0.2088998763906057, val_acc: 0.18226600985221675, train_loss: 1.7468488895701537, val_loss: 1.7374842660180454 (10 / 80)\n",
            "underfit -> train_accuracy = 0.2088998763906057\n",
            "lr 0.00026532122590550535, batch 14, decay 1.2757375261150867e-05, gamma 0.8065857126536472, val accuracy 0.18226600985221675, val loss 1.7841870044839794 [42 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00043660847130590896, 'batch_size': 10, 'weight_decay': 0.00025031720443271155, 'gamma': 0.011678955740792939}\n",
            "train_acc: 0.17676143386897405, val_acc: 0.18226600985221675, train_loss: 1.7865880147193653, val_loss: 1.7789650227635951 (1 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7760614719614847, val_loss: 1.7664478899810114 (2 / 80)\n",
            "train_acc: 0.1681087762669963, val_acc: 0.18226600985221675, train_loss: 1.7671138506442565, val_loss: 1.7545707219927182 (3 / 80)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7531603143005936, val_loss: 1.7451333670780576 (4 / 80)\n",
            "train_acc: 0.21755253399258342, val_acc: 0.18226600985221675, train_loss: 1.749959611774817, val_loss: 1.7375876551191207 (5 / 80)\n",
            "train_acc: 0.2200247218788628, val_acc: 0.20689655172413793, train_loss: 1.7368352427912879, val_loss: 1.7206326640885452 (6 / 80)\n",
            "train_acc: 0.2583436341161928, val_acc: 0.33004926108374383, train_loss: 1.726372303862507, val_loss: 1.689153160954931 (7 / 80)\n",
            "train_acc: 0.2904820766378245, val_acc: 0.31527093596059114, train_loss: 1.7025232994364867, val_loss: 1.6429129927029162 (8 / 80)\n",
            "train_acc: 0.33127317676143386, val_acc: 0.3251231527093596, train_loss: 1.6331978560378142, val_loss: 1.5847470032170488 (9 / 80)\n",
            "train_acc: 0.3238566131025958, val_acc: 0.3054187192118227, train_loss: 1.582550078446255, val_loss: 1.5166796416484665 (10 / 80)\n",
            "train_acc: 0.33868974042027195, val_acc: 0.3103448275862069, train_loss: 1.5564956551576574, val_loss: 1.5067604856538068 (11 / 80)\n",
            "train_acc: 0.3621755253399258, val_acc: 0.31527093596059114, train_loss: 1.5321827565489063, val_loss: 1.4870198618602284 (12 / 80)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.3251231527093596, train_loss: 1.4876633346154458, val_loss: 1.4330977039971375 (13 / 80)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.35960591133004927, train_loss: 1.49422907917697, val_loss: 1.4592336322286446 (14 / 80)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.4039408866995074, train_loss: 1.4581500459807766, val_loss: 1.3600162317600157 (15 / 80)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.4187192118226601, train_loss: 1.425563830085679, val_loss: 1.342789138479186 (16 / 80)\n",
            "train_acc: 0.3757725587144623, val_acc: 0.41379310344827586, train_loss: 1.455768857220342, val_loss: 1.3467571359549837 (17 / 80)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.39408866995073893, train_loss: 1.413513377658811, val_loss: 1.310821002340082 (18 / 80)\n",
            "train_acc: 0.39184177997527814, val_acc: 0.3891625615763547, train_loss: 1.4110905957605104, val_loss: 1.349714102416203 (19 / 80)\n",
            "train_acc: 0.41656365883807167, val_acc: 0.42857142857142855, train_loss: 1.3709391964086055, val_loss: 1.3067080211169615 (20 / 80)\n",
            "train_acc: 0.4276885043263288, val_acc: 0.4039408866995074, train_loss: 1.3774743752072856, val_loss: 1.402068326625918 (21 / 80)\n",
            "train_acc: 0.4004944375772559, val_acc: 0.4482758620689655, train_loss: 1.379337641453124, val_loss: 1.2778329687752747 (22 / 80)\n",
            "train_acc: 0.446229913473424, val_acc: 0.41379310344827586, train_loss: 1.3427660120432396, val_loss: 1.2605051454064882 (23 / 80)\n",
            "train_acc: 0.446229913473424, val_acc: 0.41379310344827586, train_loss: 1.3138643806738082, val_loss: 1.2793012463987754 (24 / 80)\n",
            "train_acc: 0.43139678615574784, val_acc: 0.35960591133004927, train_loss: 1.342376893103049, val_loss: 1.4518167367709682 (25 / 80)\n",
            "train_acc: 0.44499381953028433, val_acc: 0.4039408866995074, train_loss: 1.3623130147625107, val_loss: 1.280281174946301 (26 / 80)\n",
            "train_acc: 0.44252163164400493, val_acc: 0.4630541871921182, train_loss: 1.3118372784262093, val_loss: 1.2754234380909961 (27 / 80)\n",
            "train_acc: 0.44252163164400493, val_acc: 0.39408866995073893, train_loss: 1.3325150515446704, val_loss: 1.2922262076673836 (28 / 80)\n",
            "train_acc: 0.4672435105067985, val_acc: 0.4236453201970443, train_loss: 1.3037390386218342, val_loss: 1.3866954413540844 (29 / 80)\n",
            "train_acc: 0.484548825710754, val_acc: 0.458128078817734, train_loss: 1.2526851087919124, val_loss: 1.1928403066296882 (30 / 80)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.4088669950738916, train_loss: 1.2760934428172588, val_loss: 1.2526105821426279 (31 / 80)\n",
            "train_acc: 0.4610630407911001, val_acc: 0.5221674876847291, train_loss: 1.2742468373589817, val_loss: 1.2056354836290106 (32 / 80)\n",
            "train_acc: 0.4932014833127318, val_acc: 0.4975369458128079, train_loss: 1.2058338769728822, val_loss: 1.1916915424938859 (33 / 80)\n",
            "train_acc: 0.5043263288009888, val_acc: 0.49261083743842365, train_loss: 1.2408383633798663, val_loss: 1.1750614402329393 (34 / 80)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.5024630541871922, train_loss: 1.210724065050354, val_loss: 1.190560412524369 (35 / 80)\n",
            "train_acc: 0.5080346106304079, val_acc: 0.4630541871921182, train_loss: 1.2031614968302224, val_loss: 1.2203293454470894 (36 / 80)\n",
            "train_acc: 0.4857849196538937, val_acc: 0.5073891625615764, train_loss: 1.2193914034457967, val_loss: 1.1375359936887994 (37 / 80)\n",
            "train_acc: 0.5166872682323856, val_acc: 0.4827586206896552, train_loss: 1.1718380538141182, val_loss: 1.2002016188475886 (38 / 80)\n",
            "train_acc: 0.5129789864029666, val_acc: 0.4876847290640394, train_loss: 1.177525452247214, val_loss: 1.1422432032711987 (39 / 80)\n",
            "train_acc: 0.5030902348578492, val_acc: 0.4630541871921182, train_loss: 1.1585505388133752, val_loss: 1.2511093532804198 (40 / 80)\n",
            "train_acc: 0.5203955500618047, val_acc: 0.4876847290640394, train_loss: 1.136339764070452, val_loss: 1.1453341898953386 (41 / 80)\n",
            "train_acc: 0.5401730531520396, val_acc: 0.4729064039408867, train_loss: 1.1248007545659804, val_loss: 1.1960834740417932 (42 / 80)\n",
            "train_acc: 0.5562422744128553, val_acc: 0.4827586206896552, train_loss: 1.111083389802091, val_loss: 1.1411251704681096 (43 / 80)\n",
            "train_acc: 0.5525339925834364, val_acc: 0.4729064039408867, train_loss: 1.083899395206508, val_loss: 1.214334006379978 (44 / 80)\n",
            "train_acc: 0.5525339925834364, val_acc: 0.39408866995073893, train_loss: 1.0924274184647833, val_loss: 1.4907411331026426 (45 / 80)\n",
            "train_acc: 0.5611866501854141, val_acc: 0.5073891625615764, train_loss: 1.0699524634848123, val_loss: 1.1050394788164224 (46 / 80)\n",
            "train_acc: 0.5525339925834364, val_acc: 0.5073891625615764, train_loss: 1.0911380972791513, val_loss: 1.102385232014022 (47 / 80)\n",
            "train_acc: 0.6044499381953028, val_acc: 0.47783251231527096, train_loss: 1.0023885936613284, val_loss: 1.160236198913875 (48 / 80)\n",
            "train_acc: 0.6143386897404203, val_acc: 0.5221674876847291, train_loss: 0.9668443525677411, val_loss: 1.1036699373146583 (49 / 80)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5221674876847291, train_loss: 0.9793192358924667, val_loss: 1.0719547665177895 (50 / 80)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5517241379310345, train_loss: 0.9443867468863394, val_loss: 1.059858798980713 (51 / 80)\n",
            "train_acc: 0.6143386897404203, val_acc: 0.5517241379310345, train_loss: 0.9485395886694692, val_loss: 1.0547647024023121 (52 / 80)\n",
            "train_acc: 0.6266996291718171, val_acc: 0.5517241379310345, train_loss: 0.9263313146692566, val_loss: 1.0497129321685565 (53 / 80)\n",
            "train_acc: 0.6168108776266996, val_acc: 0.5517241379310345, train_loss: 0.9462585055783889, val_loss: 1.0518917197664384 (54 / 80)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.5517241379310345, train_loss: 0.9186390404795539, val_loss: 1.052727241234239 (55 / 80)\n",
            "train_acc: 0.646477132262052, val_acc: 0.5517241379310345, train_loss: 0.9151708715778346, val_loss: 1.051168212749688 (56 / 80)\n",
            "train_acc: 0.6279357231149567, val_acc: 0.5517241379310345, train_loss: 0.9113011545834346, val_loss: 1.0467483627385106 (57 / 80)\n",
            "train_acc: 0.6415327564894932, val_acc: 0.5517241379310345, train_loss: 0.9180373103715875, val_loss: 1.0489145156197948 (58 / 80)\n",
            "train_acc: 0.6341161928306551, val_acc: 0.5467980295566502, train_loss: 0.9056265963464791, val_loss: 1.052746930733103 (59 / 80)\n",
            "train_acc: 0.619283065512979, val_acc: 0.5566502463054187, train_loss: 0.9134039458886655, val_loss: 1.0403598074255318 (60 / 80)\n",
            "train_acc: 0.6625463535228677, val_acc: 0.5467980295566502, train_loss: 0.8844312122195259, val_loss: 1.049438250035488 (61 / 80)\n",
            "train_acc: 0.6489493201483313, val_acc: 0.5517241379310345, train_loss: 0.8934468386789187, val_loss: 1.0503384036383605 (62 / 80)\n",
            "train_acc: 0.6378244746600742, val_acc: 0.5467980295566502, train_loss: 0.9119024471977291, val_loss: 1.0473307553183269 (63 / 80)\n",
            "train_acc: 0.6501854140914709, val_acc: 0.5517241379310345, train_loss: 0.8841800624861558, val_loss: 1.0419107047207836 (64 / 80)\n",
            "train_acc: 0.6328800988875154, val_acc: 0.5517241379310345, train_loss: 0.9022341716127431, val_loss: 1.0446846391179878 (65 / 80)\n",
            "train_acc: 0.6402966625463535, val_acc: 0.5566502463054187, train_loss: 0.8901169461431845, val_loss: 1.0450495813280491 (66 / 80)\n",
            "train_acc: 0.6378244746600742, val_acc: 0.5566502463054187, train_loss: 0.8691373782782679, val_loss: 1.0485897472339312 (67 / 80)\n",
            "train_acc: 0.6501854140914709, val_acc: 0.5615763546798029, train_loss: 0.904204496715508, val_loss: 1.0453474251507537 (68 / 80)\n",
            "train_acc: 0.65389369592089, val_acc: 0.5566502463054187, train_loss: 0.8988655392996903, val_loss: 1.0463739806795356 (69 / 80)\n",
            "train_acc: 0.6477132262051916, val_acc: 0.5566502463054187, train_loss: 0.8742586683725987, val_loss: 1.0428755929317381 (70 / 80)\n",
            "train_acc: 0.6786155747836835, val_acc: 0.5517241379310345, train_loss: 0.8832794573015277, val_loss: 1.039697878466451 (71 / 80)\n",
            "train_acc: 0.6526576019777504, val_acc: 0.5467980295566502, train_loss: 0.8740419040356932, val_loss: 1.0386262615326003 (72 / 80)\n",
            "train_acc: 0.6427688504326329, val_acc: 0.5566502463054187, train_loss: 0.8966043037448737, val_loss: 1.0455158722811733 (73 / 80)\n",
            "train_acc: 0.6662546353522868, val_acc: 0.5615763546798029, train_loss: 0.8756257213827117, val_loss: 1.0453196388160066 (74 / 80)\n",
            "train_acc: 0.6625463535228677, val_acc: 0.5566502463054187, train_loss: 0.8501732182149098, val_loss: 1.0444753921677914 (75 / 80)\n",
            "train_acc: 0.6440049443757726, val_acc: 0.5566502463054187, train_loss: 0.8732267002240099, val_loss: 1.039204795665929 (76 / 80)\n",
            "train_acc: 0.6378244746600742, val_acc: 0.5566502463054187, train_loss: 0.8827303660520075, val_loss: 1.0417535992091513 (77 / 80)\n",
            "train_acc: 0.6600741656365884, val_acc: 0.5566502463054187, train_loss: 0.8608777470730143, val_loss: 1.0455184044509098 (78 / 80)\n",
            "train_acc: 0.6489493201483313, val_acc: 0.5517241379310345, train_loss: 0.8584172122263348, val_loss: 1.049813800551034 (79 / 80)\n",
            "train_acc: 0.6477132262051916, val_acc: 0.5615763546798029, train_loss: 0.8481491452683919, val_loss: 1.0495871452275167 (80 / 80)\n",
            "lr 0.00043660847130590896, batch 10, decay 0.00025031720443271155, gamma 0.011678955740792939, val accuracy 0.5615763546798029, val loss 1.0453474251507537 [43 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00027531434783290124, 'batch_size': 9, 'weight_decay': 4.3783604017624755e-06, 'gamma': 0.11844128056704877}\n",
            "train_acc: 0.1681087762669963, val_acc: 0.26108374384236455, train_loss: 1.789515652084822, val_loss: 1.7829278208352075 (1 / 80)\n",
            "train_acc: 0.20148331273176762, val_acc: 0.18226600985221675, train_loss: 1.7810920618373178, val_loss: 1.7727671667860059 (2 / 80)\n",
            "train_acc: 0.2249690976514215, val_acc: 0.18226600985221675, train_loss: 1.7706783784482034, val_loss: 1.7611288067155284 (3 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7633650280783881, val_loss: 1.753210654986903 (4 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7555340460705373, val_loss: 1.7459585637294601 (5 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.19704433497536947, train_loss: 1.7517901311256683, val_loss: 1.737717559772172 (6 / 80)\n",
            "train_acc: 0.2558714462299135, val_acc: 0.21674876847290642, train_loss: 1.745882660850459, val_loss: 1.7295560578407325 (7 / 80)\n",
            "train_acc: 0.24969097651421507, val_acc: 0.2857142857142857, train_loss: 1.7323077327683476, val_loss: 1.7146044893217791 (8 / 80)\n",
            "train_acc: 0.276885043263288, val_acc: 0.2857142857142857, train_loss: 1.7225249067077826, val_loss: 1.691358398334146 (9 / 80)\n",
            "train_acc: 0.2830655129789864, val_acc: 0.32019704433497537, train_loss: 1.6924087344052912, val_loss: 1.6664038415025608 (10 / 80)\n",
            "train_acc: 0.3065512978986403, val_acc: 0.3103448275862069, train_loss: 1.6600630376188659, val_loss: 1.5860737227453974 (11 / 80)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.35467980295566504, train_loss: 1.6022330992596112, val_loss: 1.557460825431523 (12 / 80)\n",
            "train_acc: 0.3263288009888752, val_acc: 0.3399014778325123, train_loss: 1.5895940504498476, val_loss: 1.550816316909978 (13 / 80)\n",
            "train_acc: 0.32756489493201485, val_acc: 0.3103448275862069, train_loss: 1.5452831526769253, val_loss: 1.5292246605962367 (14 / 80)\n",
            "train_acc: 0.3399258343634116, val_acc: 0.3399014778325123, train_loss: 1.5332631413515183, val_loss: 1.4713326651474525 (15 / 80)\n",
            "train_acc: 0.3720642768850433, val_acc: 0.3251231527093596, train_loss: 1.5137683993069586, val_loss: 1.542964786144313 (16 / 80)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.39408866995073893, train_loss: 1.5008339943785602, val_loss: 1.4470047334144855 (17 / 80)\n",
            "train_acc: 0.3547589616810878, val_acc: 0.23645320197044334, train_loss: 1.5244160892789531, val_loss: 1.6598917463142884 (18 / 80)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.2561576354679803, train_loss: 1.4812518724404984, val_loss: 1.5297598040162637 (19 / 80)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.39408866995073893, train_loss: 1.4722146404393672, val_loss: 1.4157374988635774 (20 / 80)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.3793103448275862, train_loss: 1.489987768702513, val_loss: 1.3927446216198023 (21 / 80)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.35467980295566504, train_loss: 1.4284748409233519, val_loss: 1.413456614381574 (22 / 80)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.39408866995073893, train_loss: 1.4169367466338338, val_loss: 1.378267019546678 (23 / 80)\n",
            "train_acc: 0.3831891223733004, val_acc: 0.3251231527093596, train_loss: 1.4110574097509585, val_loss: 1.4149605300038905 (24 / 80)\n",
            "train_acc: 0.3992583436341162, val_acc: 0.3251231527093596, train_loss: 1.4075280731481734, val_loss: 1.403992269426731 (25 / 80)\n",
            "train_acc: 0.39060568603213847, val_acc: 0.3694581280788177, train_loss: 1.4484783191468718, val_loss: 1.3654452979271048 (26 / 80)\n",
            "train_acc: 0.4264524103831891, val_acc: 0.42857142857142855, train_loss: 1.381077248764274, val_loss: 1.352748716406047 (27 / 80)\n",
            "train_acc: 0.4437577255871446, val_acc: 0.42857142857142855, train_loss: 1.3532433822216887, val_loss: 1.3134895342911406 (28 / 80)\n",
            "train_acc: 0.43510506798516685, val_acc: 0.45320197044334976, train_loss: 1.3340060757323455, val_loss: 1.3354991918126937 (29 / 80)\n",
            "train_acc: 0.43510506798516685, val_acc: 0.41379310344827586, train_loss: 1.36679592290237, val_loss: 1.3198121395604363 (30 / 80)\n",
            "train_acc: 0.4054388133498146, val_acc: 0.4039408866995074, train_loss: 1.368385611859476, val_loss: 1.4025142063648242 (31 / 80)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.43842364532019706, train_loss: 1.338058414137997, val_loss: 1.2938189342104156 (32 / 80)\n",
            "train_acc: 0.4264524103831891, val_acc: 0.4236453201970443, train_loss: 1.3082242378198319, val_loss: 1.306677850890042 (33 / 80)\n",
            "train_acc: 0.446229913473424, val_acc: 0.4236453201970443, train_loss: 1.3092440587775815, val_loss: 1.2718447487929772 (34 / 80)\n",
            "train_acc: 0.449938195302843, val_acc: 0.39901477832512317, train_loss: 1.2949707618602568, val_loss: 1.3612370215026028 (35 / 80)\n",
            "train_acc: 0.4796044499381953, val_acc: 0.46798029556650245, train_loss: 1.2985837160435831, val_loss: 1.2404561621214956 (36 / 80)\n",
            "train_acc: 0.4573547589616811, val_acc: 0.3891625615763547, train_loss: 1.2931225710952532, val_loss: 1.3150361368221601 (37 / 80)\n",
            "train_acc: 0.48702101359703337, val_acc: 0.4433497536945813, train_loss: 1.27908716876663, val_loss: 1.2999562491924304 (38 / 80)\n",
            "train_acc: 0.47713226205191595, val_acc: 0.3793103448275862, train_loss: 1.2285350802802333, val_loss: 1.3283260887479547 (39 / 80)\n",
            "train_acc: 0.47095179233621753, val_acc: 0.46798029556650245, train_loss: 1.2538710513574673, val_loss: 1.286630124000493 (40 / 80)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.49261083743842365, train_loss: 1.2668776942419329, val_loss: 1.2448265018134281 (41 / 80)\n",
            "train_acc: 0.49443757725587145, val_acc: 0.4827586206896552, train_loss: 1.2361403646221563, val_loss: 1.2686525518671046 (42 / 80)\n",
            "train_acc: 0.49443757725587145, val_acc: 0.4630541871921182, train_loss: 1.2208834751722102, val_loss: 1.2055285534835214 (43 / 80)\n",
            "train_acc: 0.5216316440049443, val_acc: 0.5073891625615764, train_loss: 1.1847897735602777, val_loss: 1.2476826404115837 (44 / 80)\n",
            "train_acc: 0.5006180469715699, val_acc: 0.5123152709359606, train_loss: 1.2074148743350073, val_loss: 1.256341669359818 (45 / 80)\n",
            "train_acc: 0.5179233621755254, val_acc: 0.5320197044334976, train_loss: 1.1792495755388945, val_loss: 1.1656013330802542 (46 / 80)\n",
            "train_acc: 0.5055624227441285, val_acc: 0.4236453201970443, train_loss: 1.170576975283723, val_loss: 1.2556593101012883 (47 / 80)\n",
            "train_acc: 0.546353522867738, val_acc: 0.4236453201970443, train_loss: 1.1577209319702921, val_loss: 1.3716240472394257 (48 / 80)\n",
            "train_acc: 0.519159456118665, val_acc: 0.5172413793103449, train_loss: 1.1544162574303607, val_loss: 1.1603963991691326 (49 / 80)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.5369458128078818, train_loss: 1.0748470403502692, val_loss: 1.147549316800874 (50 / 80)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.5369458128078818, train_loss: 1.0928303353276625, val_loss: 1.1402344019542188 (51 / 80)\n",
            "train_acc: 0.5574783683559951, val_acc: 0.5270935960591133, train_loss: 1.0693184270699623, val_loss: 1.1441263218818627 (52 / 80)\n",
            "train_acc: 0.5661310259579728, val_acc: 0.5369458128078818, train_loss: 1.0706590219171734, val_loss: 1.1288006114842268 (53 / 80)\n",
            "train_acc: 0.5760197775030902, val_acc: 0.5320197044334976, train_loss: 1.0598957618912601, val_loss: 1.1571698209335064 (54 / 80)\n",
            "train_acc: 0.588380716934487, val_acc: 0.5270935960591133, train_loss: 1.0499888457385513, val_loss: 1.1640510946659033 (55 / 80)\n",
            "train_acc: 0.5760197775030902, val_acc: 0.5270935960591133, train_loss: 1.0609526715260944, val_loss: 1.1408732562816788 (56 / 80)\n",
            "train_acc: 0.5970333745364648, val_acc: 0.5270935960591133, train_loss: 1.0413378379106226, val_loss: 1.1850912154014475 (57 / 80)\n",
            "train_acc: 0.5611866501854141, val_acc: 0.5270935960591133, train_loss: 1.0608248656848158, val_loss: 1.1462025557245528 (58 / 80)\n",
            "train_acc: 0.5797280593325093, val_acc: 0.5270935960591133, train_loss: 1.0244926930358, val_loss: 1.1515151356241387 (59 / 80)\n",
            "train_acc: 0.584672435105068, val_acc: 0.5123152709359606, train_loss: 1.0172668704291061, val_loss: 1.1530074254045346 (60 / 80)\n",
            "train_acc: 0.5772558714462299, val_acc: 0.5467980295566502, train_loss: 1.0450125163506223, val_loss: 1.1246963169774398 (61 / 80)\n",
            "train_acc: 0.5933250927070457, val_acc: 0.5270935960591133, train_loss: 1.0201862737333818, val_loss: 1.1182484694302375 (62 / 80)\n",
            "train_acc: 0.588380716934487, val_acc: 0.541871921182266, train_loss: 1.0194475634578426, val_loss: 1.136789337167599 (63 / 80)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.5172413793103449, train_loss: 1.0144589517555662, val_loss: 1.184902832425874 (64 / 80)\n",
            "train_acc: 0.5871446229913473, val_acc: 0.5467980295566502, train_loss: 1.006585879143148, val_loss: 1.1321259927867082 (65 / 80)\n",
            "train_acc: 0.6106304079110012, val_acc: 0.5270935960591133, train_loss: 0.9915826521786241, val_loss: 1.158678709286187 (66 / 80)\n",
            "train_acc: 0.6069221260815822, val_acc: 0.5270935960591133, train_loss: 1.0038738089379922, val_loss: 1.1465877347391815 (67 / 80)\n",
            "train_acc: 0.5933250927070457, val_acc: 0.5221674876847291, train_loss: 1.0104232778213227, val_loss: 1.1611234716006689 (68 / 80)\n",
            "train_acc: 0.6081582200247219, val_acc: 0.5172413793103449, train_loss: 0.9898606665128535, val_loss: 1.1474649095770173 (69 / 80)\n",
            "train_acc: 0.6205191594561187, val_acc: 0.5221674876847291, train_loss: 0.9735330383400392, val_loss: 1.1444096251074316 (70 / 80)\n",
            "train_acc: 0.6378244746600742, val_acc: 0.5467980295566502, train_loss: 0.9466300395643166, val_loss: 1.1339150050590778 (71 / 80)\n",
            "train_acc: 0.6613102595797281, val_acc: 0.5369458128078818, train_loss: 0.936907581883691, val_loss: 1.1345155920301164 (72 / 80)\n",
            "train_acc: 0.619283065512979, val_acc: 0.541871921182266, train_loss: 0.9346323846223475, val_loss: 1.128802098664157 (73 / 80)\n",
            "train_acc: 0.6143386897404203, val_acc: 0.5123152709359606, train_loss: 0.97086488382778, val_loss: 1.1715287055288042 (74 / 80)\n",
            "train_acc: 0.6254635352286774, val_acc: 0.5517241379310345, train_loss: 0.9465435934891954, val_loss: 1.117455956383879 (75 / 80)\n",
            "train_acc: 0.6266996291718171, val_acc: 0.541871921182266, train_loss: 0.9602093534136584, val_loss: 1.129639339858088 (76 / 80)\n",
            "train_acc: 0.6390605686032138, val_acc: 0.5270935960591133, train_loss: 0.9647477379319694, val_loss: 1.1538458713169755 (77 / 80)\n",
            "train_acc: 0.6402966625463535, val_acc: 0.5467980295566502, train_loss: 0.9651626690724872, val_loss: 1.1178476833944837 (78 / 80)\n",
            "train_acc: 0.6291718170580964, val_acc: 0.541871921182266, train_loss: 0.932879262271122, val_loss: 1.1078027715823922 (79 / 80)\n",
            "train_acc: 0.6452410383189122, val_acc: 0.5369458128078818, train_loss: 0.9112830362449618, val_loss: 1.1398767940516543 (80 / 80)\n",
            "lr 0.00027531434783290124, batch 9, decay 4.3783604017624755e-06, gamma 0.11844128056704877, val accuracy 0.5517241379310345, val loss 1.117455956383879 [44 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 6.0813156817757295e-05, 'batch_size': 15, 'weight_decay': 0.000659304406044606, 'gamma': 0.10670005341228928}\n",
            "train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.7918708655978606, val_loss: 1.7914425363681588 (1 / 80)\n",
            "train_acc: 0.17428924598269468, val_acc: 0.18226600985221675, train_loss: 1.7906590010533374, val_loss: 1.7902440384690985 (2 / 80)\n",
            "train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7901348643308812, val_loss: 1.7890720020961293 (3 / 80)\n",
            "train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7886088626788486, val_loss: 1.787948562593883 (4 / 80)\n",
            "train_acc: 0.18912237330037082, val_acc: 0.18226600985221675, train_loss: 1.7881470364604803, val_loss: 1.7868905707533136 (5 / 80)\n",
            "train_acc: 0.19530284301606923, val_acc: 0.18226600985221675, train_loss: 1.7869382489596957, val_loss: 1.78584486803985 (6 / 80)\n",
            "train_acc: 0.16440049443757726, val_acc: 0.18226600985221675, train_loss: 1.7861583025110668, val_loss: 1.7848686819593307 (7 / 80)\n",
            "train_acc: 0.19777503090234858, val_acc: 0.1921182266009852, train_loss: 1.7846108929925266, val_loss: 1.7837896799219066 (8 / 80)\n",
            "train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7854589942948633, val_loss: 1.782853931041774 (9 / 80)\n",
            "train_acc: 0.20519159456118666, val_acc: 0.21674876847290642, train_loss: 1.783212407115658, val_loss: 1.7818187322522618 (10 / 80)\n",
            "underfit -> train_accuracy = 0.20519159456118666\n",
            "lr 6.0813156817757295e-05, batch 15, decay 0.000659304406044606, gamma 0.10670005341228928, val accuracy 0.21674876847290642, val loss 1.7818187322522618 [45 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.00012146518694585026, 'batch_size': 14, 'weight_decay': 8.797099892896007e-06, 'gamma': 0.3126384411676614}\n",
            "train_acc: 0.18912237330037082, val_acc: 0.22167487684729065, train_loss: 1.7900188971214153, val_loss: 1.7874682319575343 (1 / 80)\n",
            "train_acc: 0.207663782447466, val_acc: 0.2561576354679803, train_loss: 1.7866180514817773, val_loss: 1.7844944082457443 (2 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.7849692828280963, val_loss: 1.7812803983688354 (3 / 80)\n",
            "train_acc: 0.207663782447466, val_acc: 0.18719211822660098, train_loss: 1.7808360847171363, val_loss: 1.7783180968514805 (4 / 80)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7777517367941782, val_loss: 1.774883664887527 (5 / 80)\n",
            "train_acc: 0.1903584672435105, val_acc: 0.18226600985221675, train_loss: 1.7769926556550675, val_loss: 1.7719902457862065 (6 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.774203565418352, val_loss: 1.7687244415283203 (7 / 80)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7696981027777616, val_loss: 1.765359405813546 (8 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.7663015029928446, val_loss: 1.7620141177341855 (9 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7647812590463494, val_loss: 1.7586647765389805 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18046971569839307\n",
            "lr 0.00012146518694585026, batch 14, decay 8.797099892896007e-06, gamma 0.3126384411676614, val accuracy 0.2561576354679803, val loss 1.7844944082457443 [46 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.7386471743748322e-05, 'batch_size': 14, 'weight_decay': 2.4519700674687268e-06, 'gamma': 0.07270305616738851}\n",
            "train_acc: 0.1792336217552534, val_acc: 0.17733990147783252, train_loss: 1.7915779733834662, val_loss: 1.7917469369954075 (1 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.17733990147783252, train_loss: 1.7911120532322284, val_loss: 1.7913520911644245 (2 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.79158278890829, val_loss: 1.790965668086348 (3 / 80)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7905581003361197, val_loss: 1.7905865661029159 (4 / 80)\n",
            "train_acc: 0.18665018541409148, val_acc: 0.18226600985221675, train_loss: 1.7903872956746294, val_loss: 1.7902300686671817 (5 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.789474595462436, val_loss: 1.78988071556749 (6 / 80)\n",
            "train_acc: 0.16440049443757726, val_acc: 0.18226600985221675, train_loss: 1.7903331833510523, val_loss: 1.7895434108273736 (7 / 80)\n",
            "train_acc: 0.1433868974042027, val_acc: 0.18226600985221675, train_loss: 1.7902729556940689, val_loss: 1.789189470225367 (8 / 80)\n",
            "train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.789244303891921, val_loss: 1.7888570900621086 (9 / 80)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18226600985221675, train_loss: 1.7883611460698696, val_loss: 1.7884881249789535 (10 / 80)\n",
            "underfit -> train_accuracy = 0.19283065512978986\n",
            "lr 1.7386471743748322e-05, batch 14, decay 2.4519700674687268e-06, gamma 0.07270305616738851, val accuracy 0.18226600985221675, val loss 1.790965668086348 [47 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 1.6379064077905427e-05, 'batch_size': 13, 'weight_decay': 1.1422900535947354e-06, 'gamma': 0.08452315467353534}\n",
            "train_acc: 0.15451174289245984, val_acc: 0.18226600985221675, train_loss: 1.7916661484721859, val_loss: 1.7916029568376213 (1 / 80)\n",
            "train_acc: 0.15822002472187885, val_acc: 0.18226600985221675, train_loss: 1.7928943878640646, val_loss: 1.791131357254066 (2 / 80)\n",
            "train_acc: 0.16934487021013597, val_acc: 0.18226600985221675, train_loss: 1.791210895121024, val_loss: 1.7906875099454607 (3 / 80)\n",
            "train_acc: 0.16069221260815822, val_acc: 0.18226600985221675, train_loss: 1.7910018222146336, val_loss: 1.7902416548705453 (4 / 80)\n",
            "train_acc: 0.18046971569839307, val_acc: 0.18226600985221675, train_loss: 1.7901018709128513, val_loss: 1.7898384033165542 (5 / 80)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7900355752230575, val_loss: 1.789391761930118 (6 / 80)\n",
            "train_acc: 0.1631644004944376, val_acc: 0.18226600985221675, train_loss: 1.789932271311398, val_loss: 1.7889231672427925 (7 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.18226600985221675, train_loss: 1.7891915643170972, val_loss: 1.7885129475241224 (8 / 80)\n",
            "train_acc: 0.17799752781211373, val_acc: 0.18226600985221675, train_loss: 1.7890028003119716, val_loss: 1.7880609552261277 (9 / 80)\n",
            "train_acc: 0.18541409147095178, val_acc: 0.18226600985221675, train_loss: 1.7885267077329279, val_loss: 1.7876615747442386 (10 / 80)\n",
            "underfit -> train_accuracy = 0.18541409147095178\n",
            "lr 1.6379064077905427e-05, batch 13, decay 1.1422900535947354e-06, gamma 0.08452315467353534, val accuracy 0.18226600985221675, val loss 1.7916029568376213 [48 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 0.0007220498435995008, 'batch_size': 14, 'weight_decay': 2.228552014354877e-05, 'gamma': 0.08113961287843949}\n",
            "train_acc: 0.18046971569839307, val_acc: 0.2955665024630542, train_loss: 1.7881970287695508, val_loss: 1.7805508909554317 (1 / 80)\n",
            "train_acc: 0.1915945611866502, val_acc: 0.18226600985221675, train_loss: 1.775877832187563, val_loss: 1.7660183084422145 (2 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.20689655172413793, train_loss: 1.76493245326397, val_loss: 1.7530682744651005 (3 / 80)\n",
            "train_acc: 0.1841779975278121, val_acc: 0.19704433497536947, train_loss: 1.7583734504371993, val_loss: 1.7459202552663868 (4 / 80)\n",
            "train_acc: 0.24103831891223734, val_acc: 0.2561576354679803, train_loss: 1.757315963690891, val_loss: 1.7366750774712398 (5 / 80)\n",
            "train_acc: 0.25339925834363414, val_acc: 0.22167487684729065, train_loss: 1.7372464476468683, val_loss: 1.7187977412651325 (6 / 80)\n",
            "train_acc: 0.273176761433869, val_acc: 0.3891625615763547, train_loss: 1.7118799634858026, val_loss: 1.6686303862209977 (7 / 80)\n",
            "train_acc: 0.29171817058096416, val_acc: 0.33004926108374383, train_loss: 1.665375268798234, val_loss: 1.5936569879794944 (8 / 80)\n",
            "train_acc: 0.3065512978986403, val_acc: 0.33497536945812806, train_loss: 1.6005170250999914, val_loss: 1.6105702539970135 (9 / 80)\n",
            "train_acc: 0.32014833127317677, val_acc: 0.3103448275862069, train_loss: 1.5750990248433738, val_loss: 1.510849693725849 (10 / 80)\n",
            "train_acc: 0.33498145859085293, val_acc: 0.39901477832512317, train_loss: 1.5665613473272146, val_loss: 1.4846715474950856 (11 / 80)\n",
            "train_acc: 0.35599505562422745, val_acc: 0.31527093596059114, train_loss: 1.5260466774845005, val_loss: 1.478164792060852 (12 / 80)\n",
            "train_acc: 0.36341161928306553, val_acc: 0.3793103448275862, train_loss: 1.5009620886650603, val_loss: 1.4024047974882454 (13 / 80)\n",
            "train_acc: 0.3794808405438813, val_acc: 0.32019704433497537, train_loss: 1.4656078075743724, val_loss: 1.525628768164536 (14 / 80)\n",
            "train_acc: 0.3473423980222497, val_acc: 0.3891625615763547, train_loss: 1.5044236673854927, val_loss: 1.4119674953921089 (15 / 80)\n",
            "train_acc: 0.37082818294190356, val_acc: 0.33004926108374383, train_loss: 1.4549992346793081, val_loss: 1.5255034175412407 (16 / 80)\n",
            "train_acc: 0.3819530284301607, val_acc: 0.43349753694581283, train_loss: 1.4796075916702873, val_loss: 1.4034416675567627 (17 / 80)\n",
            "train_acc: 0.38936959208899874, val_acc: 0.43842364532019706, train_loss: 1.4345085261336659, val_loss: 1.3707676016051193 (18 / 80)\n",
            "train_acc: 0.38813349814585907, val_acc: 0.3891625615763547, train_loss: 1.3792150058911374, val_loss: 1.4897505373790347 (19 / 80)\n",
            "train_acc: 0.4004944375772559, val_acc: 0.43349753694581283, train_loss: 1.3999692891230542, val_loss: 1.3983055312058021 (20 / 80)\n",
            "train_acc: 0.4103831891223733, val_acc: 0.4433497536945813, train_loss: 1.3954120967237853, val_loss: 1.4204003769775917 (21 / 80)\n",
            "train_acc: 0.4264524103831891, val_acc: 0.4088669950738916, train_loss: 1.350277659330144, val_loss: 1.2965024134208416 (22 / 80)\n",
            "train_acc: 0.4338689740420272, val_acc: 0.4236453201970443, train_loss: 1.3599289568746635, val_loss: 1.3509461838623573 (23 / 80)\n",
            "train_acc: 0.44128553770086526, val_acc: 0.3891625615763547, train_loss: 1.3310955210286106, val_loss: 1.2754795427980095 (24 / 80)\n",
            "train_acc: 0.44128553770086526, val_acc: 0.47783251231527096, train_loss: 1.3305153770116704, val_loss: 1.2360254526138306 (25 / 80)\n",
            "train_acc: 0.4227441285537701, val_acc: 0.46798029556650245, train_loss: 1.3392909202351706, val_loss: 1.259647733178632 (26 / 80)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.4630541871921182, train_loss: 1.2971242791495305, val_loss: 1.2817159726701934 (27 / 80)\n",
            "train_acc: 0.45241038318912236, val_acc: 0.46798029556650245, train_loss: 1.3142431644338317, val_loss: 1.2284786660095741 (28 / 80)\n",
            "train_acc: 0.46971569839307786, val_acc: 0.43349753694581283, train_loss: 1.2683400857286489, val_loss: 1.2541539874570122 (29 / 80)\n",
            "train_acc: 0.46600741656365885, val_acc: 0.4236453201970443, train_loss: 1.2605556270248073, val_loss: 1.28880290327401 (30 / 80)\n",
            "train_acc: 0.4857849196538937, val_acc: 0.4187192118226601, train_loss: 1.231080527800712, val_loss: 1.272082859072192 (31 / 80)\n",
            "train_acc: 0.4907292954264524, val_acc: 0.4433497536945813, train_loss: 1.2163227696353927, val_loss: 1.2302645198230087 (32 / 80)\n",
            "train_acc: 0.5092707045735476, val_acc: 0.4975369458128079, train_loss: 1.213291444796125, val_loss: 1.1701111526324832 (33 / 80)\n",
            "train_acc: 0.4919653893695921, val_acc: 0.4975369458128079, train_loss: 1.1799366930948643, val_loss: 1.1393290264853115 (34 / 80)\n",
            "train_acc: 0.522867737948084, val_acc: 0.45320197044334976, train_loss: 1.1732677683989403, val_loss: 1.296565109285815 (35 / 80)\n",
            "train_acc: 0.48331273176761436, val_acc: 0.4827586206896552, train_loss: 1.2006441194135857, val_loss: 1.2015130108800427 (36 / 80)\n",
            "train_acc: 0.5241038318912238, val_acc: 0.5221674876847291, train_loss: 1.1510368142788137, val_loss: 1.1549516011928689 (37 / 80)\n",
            "train_acc: 0.5475896168108776, val_acc: 0.4876847290640394, train_loss: 1.124926025404771, val_loss: 1.1613649376507462 (38 / 80)\n",
            "train_acc: 0.519159456118665, val_acc: 0.47783251231527096, train_loss: 1.1733023370299558, val_loss: 1.1711093516185367 (39 / 80)\n",
            "train_acc: 0.5562422744128553, val_acc: 0.5024630541871922, train_loss: 1.1045166707893532, val_loss: 1.1394754504335338 (40 / 80)\n",
            "train_acc: 0.5710754017305315, val_acc: 0.4876847290640394, train_loss: 1.0825195467221576, val_loss: 1.1942935318782413 (41 / 80)\n",
            "train_acc: 0.5562422744128553, val_acc: 0.46798029556650245, train_loss: 1.0936420548831576, val_loss: 1.2374129685862312 (42 / 80)\n",
            "train_acc: 0.5673671199011124, val_acc: 0.5073891625615764, train_loss: 1.047066808041595, val_loss: 1.1634983358712032 (43 / 80)\n",
            "train_acc: 0.5859085290482077, val_acc: 0.5369458128078818, train_loss: 1.0322085857686065, val_loss: 1.1045475499383335 (44 / 80)\n",
            "train_acc: 0.5945611866501854, val_acc: 0.5714285714285714, train_loss: 1.0053027321587977, val_loss: 1.082403540611267 (45 / 80)\n",
            "train_acc: 0.6316440049443758, val_acc: 0.5665024630541872, train_loss: 0.9484147845740961, val_loss: 0.9977504323268759 (46 / 80)\n",
            "train_acc: 0.6069221260815822, val_acc: 0.5566502463054187, train_loss: 0.9787143197725671, val_loss: 1.0958263586307395 (47 / 80)\n",
            "train_acc: 0.6032138442521632, val_acc: 0.5270935960591133, train_loss: 0.9524291649884435, val_loss: 1.0535995425849125 (48 / 80)\n",
            "train_acc: 0.6662546353522868, val_acc: 0.5911330049261084, train_loss: 0.8420271270796749, val_loss: 0.9512410616052562 (49 / 80)\n",
            "train_acc: 0.6983930778739185, val_acc: 0.5665024630541872, train_loss: 0.8039952395725604, val_loss: 0.959685572262468 (50 / 80)\n",
            "train_acc: 0.6897404202719407, val_acc: 0.5763546798029556, train_loss: 0.7713249922535476, val_loss: 0.9783378531192911 (51 / 80)\n",
            "train_acc: 0.7045735475896168, val_acc: 0.6009852216748769, train_loss: 0.7579518233889849, val_loss: 0.9631050002985987 (52 / 80)\n",
            "train_acc: 0.7206427688504327, val_acc: 0.6009852216748769, train_loss: 0.7237698729754378, val_loss: 0.977959316352318 (53 / 80)\n",
            "train_acc: 0.6983930778739185, val_acc: 0.6009852216748769, train_loss: 0.7471811640247868, val_loss: 0.9632335839600399 (54 / 80)\n",
            "train_acc: 0.7379480840543882, val_acc: 0.5862068965517241, train_loss: 0.7029766685735752, val_loss: 0.9887880909031835 (55 / 80)\n",
            "train_acc: 0.7391841779975278, val_acc: 0.5911330049261084, train_loss: 0.7040955469251711, val_loss: 0.9962217458363237 (56 / 80)\n",
            "train_acc: 0.73053152039555, val_acc: 0.6009852216748769, train_loss: 0.6985282924472918, val_loss: 0.9615138728043129 (57 / 80)\n",
            "train_acc: 0.7354758961681088, val_acc: 0.5911330049261084, train_loss: 0.6754857798294615, val_loss: 0.9604118137524046 (58 / 80)\n",
            "train_acc: 0.7379480840543882, val_acc: 0.5862068965517241, train_loss: 0.6628412305940362, val_loss: 0.9806778019872205 (59 / 80)\n",
            "train_acc: 0.7589616810877626, val_acc: 0.5812807881773399, train_loss: 0.6337335777592158, val_loss: 0.994507986923744 (60 / 80)\n",
            "train_acc: 0.7564894932014833, val_acc: 0.5714285714285714, train_loss: 0.6584987179899982, val_loss: 0.9917906974924022 (61 / 80)\n",
            "train_acc: 0.7552533992583437, val_acc: 0.5911330049261084, train_loss: 0.6337540800992874, val_loss: 0.9899602326853522 (62 / 80)\n",
            "train_acc: 0.7466007416563659, val_acc: 0.5714285714285714, train_loss: 0.6674106027861902, val_loss: 0.9973794242431377 (63 / 80)\n",
            "train_acc: 0.7626699629171817, val_acc: 0.5862068965517241, train_loss: 0.6207023555843438, val_loss: 1.0072453309749734 (64 / 80)\n",
            "train_acc: 0.7428924598269468, val_acc: 0.5960591133004927, train_loss: 0.6432984572995284, val_loss: 0.9858952966229669 (65 / 80)\n",
            "train_acc: 0.7824474660074165, val_acc: 0.625615763546798, train_loss: 0.598874320500566, val_loss: 0.9968108699239534 (66 / 80)\n",
            "train_acc: 0.792336217552534, val_acc: 0.6108374384236454, train_loss: 0.5732729586373153, val_loss: 0.9976336791597563 (67 / 80)\n",
            "train_acc: 0.7762669962917181, val_acc: 0.6059113300492611, train_loss: 0.5831800087347019, val_loss: 1.0018430352210999 (68 / 80)\n",
            "train_acc: 0.7824474660074165, val_acc: 0.6059113300492611, train_loss: 0.5881076389543795, val_loss: 0.981618949051561 (69 / 80)\n",
            "train_acc: 0.7861557478368356, val_acc: 0.5665024630541872, train_loss: 0.5517931265206213, val_loss: 0.993649096324526 (70 / 80)\n",
            "train_acc: 0.7725587144622992, val_acc: 0.5911330049261084, train_loss: 0.6014265656029191, val_loss: 0.9785787596784788 (71 / 80)\n",
            "train_acc: 0.8059332509270705, val_acc: 0.5862068965517241, train_loss: 0.5513273769315005, val_loss: 1.0378426580593503 (72 / 80)\n",
            "train_acc: 0.788627935723115, val_acc: 0.5812807881773399, train_loss: 0.5692256344263574, val_loss: 1.0177200066632237 (73 / 80)\n",
            "train_acc: 0.7775030902348579, val_acc: 0.5960591133004927, train_loss: 0.5741170822321559, val_loss: 1.0088841421850796 (74 / 80)\n",
            "train_acc: 0.8084054388133498, val_acc: 0.6059113300492611, train_loss: 0.5550223060679819, val_loss: 1.0167614221572876 (75 / 80)\n",
            "train_acc: 0.8170580964153276, val_acc: 0.625615763546798, train_loss: 0.5294333023326506, val_loss: 1.001431810444799 (76 / 80)\n",
            "train_acc: 0.8096415327564895, val_acc: 0.5960591133004927, train_loss: 0.5307225883375434, val_loss: 1.0052972579824513 (77 / 80)\n",
            "train_acc: 0.8207663782447466, val_acc: 0.5812807881773399, train_loss: 0.5245843454550754, val_loss: 1.036936474257502 (78 / 80)\n",
            "train_acc: 0.8059332509270705, val_acc: 0.625615763546798, train_loss: 0.523579870630401, val_loss: 1.0161522061660373 (79 / 80)\n",
            "train_acc: 0.830655129789864, val_acc: 0.6157635467980296, train_loss: 0.4774454846812414, val_loss: 1.0500169910233597 (80 / 80)\n",
            "lr 0.0007220498435995008, batch 14, decay 2.228552014354877e-05, gamma 0.08113961287843949, val accuracy 0.625615763546798, val loss 0.9968108699239534 [49 / 50]\n",
            "-------------------------------------\n",
            "{'lr': 3.6661047485897464e-05, 'batch_size': 8, 'weight_decay': 1.5264111438721608e-05, 'gamma': 0.02284163656975739}\n",
            "train_acc: 0.18541409147095178, val_acc: 0.1724137931034483, train_loss: 1.790871512315182, val_loss: 1.7889578841589941 (1 / 80)\n",
            "train_acc: 0.15822002472187885, val_acc: 0.17733990147783252, train_loss: 1.789294233722946, val_loss: 1.7873982819430347 (2 / 80)\n",
            "train_acc: 0.17058096415327564, val_acc: 0.18226600985221675, train_loss: 1.7880759194990614, val_loss: 1.7861518900969933 (3 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7865162426225925, val_loss: 1.7846857755642218 (4 / 80)\n",
            "train_acc: 0.1792336217552534, val_acc: 0.18226600985221675, train_loss: 1.7859108079643862, val_loss: 1.7833868464812856 (5 / 80)\n",
            "train_acc: 0.20642768850432633, val_acc: 0.18226600985221675, train_loss: 1.7837074984726122, val_loss: 1.7819899391070964 (6 / 80)\n",
            "train_acc: 0.17552533992583436, val_acc: 0.18226600985221675, train_loss: 1.7826298109827878, val_loss: 1.7804657138627151 (7 / 80)\n",
            "train_acc: 0.18294190358467244, val_acc: 0.18226600985221675, train_loss: 1.7818842388054055, val_loss: 1.7791601548641187 (8 / 80)\n",
            "train_acc: 0.19283065512978986, val_acc: 0.18719211822660098, train_loss: 1.7807903942572614, val_loss: 1.777679783957345 (9 / 80)\n",
            "train_acc: 0.19901112484548825, val_acc: 0.1921182266009852, train_loss: 1.7783244463657715, val_loss: 1.7761882937013223 (10 / 80)\n",
            "underfit -> train_accuracy = 0.19901112484548825\n",
            "lr 3.6661047485897464e-05, batch 8, decay 1.5264111438721608e-05, gamma 0.02284163656975739, val accuracy 0.1921182266009852, val loss 1.7761882937013223 [50 / 50]\n",
            "--------------------------------------------\n",
            "\n",
            "{'lr': 0.000953839039916152, 'batch_size': 9, 'weight_decay': 0.00015319613136737035, 'gamma': 0.9262521929212527}, best val accuracy 0.6650246305418719, best val loss 1.0745352168975792\n",
            "val accuracies\n",
            "[0.19704433497536947, 0.5172413793103449, 0.20689655172413793, 0.6650246305418719, 0.6305418719211823, 0.24630541871921183, 0.20689655172413793, 0.6403940886699507, 0.19704433497536947, 0.21674876847290642, 0.1921182266009852, 0.645320197044335, 0.6157635467980296, 0.18719211822660098, 0.2413793103448276, 0.6551724137931034, 0.2857142857142857, 0.21674876847290642, 0.5566502463054187, 0.2019704433497537, 0.22167487684729065, 0.23645320197044334, 0.270935960591133, 0.24630541871921183, 0.24630541871921183, 0.2512315270935961, 0.2413793103448276, 0.18226600985221675, 0.17733990147783252, 0.18719211822660098, 0.20689655172413793, 0.18226600985221675, 0.18226600985221675, 0.541871921182266, 0.3103448275862069, 0.27586206896551724, 0.645320197044335, 0.6108374384236454, 0.18226600985221675, 0.2019704433497537, 0.645320197044335, 0.18226600985221675, 0.5615763546798029, 0.5517241379310345, 0.21674876847290642, 0.2561576354679803, 0.18226600985221675, 0.18226600985221675, 0.625615763546798, 0.1921182266009852]\n",
            "val losses\n",
            "[1.782029298138736, 1.1399509746746477, 1.7793491139200521, 1.0745352168975792, 1.0403618915327664, 1.7889860991773934, 1.7895073068553005, 0.9407640683827142, 1.7806962082538698, 1.784052419545028, 1.7816807748061683, 1.0584736058277449, 1.0746936818649029, 1.7819292927023225, 1.7827796489734369, 0.9024412734755154, 1.784512523359853, 1.7868574845967033, 1.1655204400346784, 1.7855143999231273, 1.7332626827831925, 1.7826859375526165, 1.786708316779489, 1.7879804173126597, 1.7853131300122866, 1.7821070014549594, 1.7808871592206907, 1.7875813492413224, 1.789126704479086, 1.7889673110886748, 1.7925488778523035, 1.789510677600729, 1.7924881515831783, 1.1236164769515615, 1.7242449345847068, 1.7857967808916064, 1.0160301753452845, 0.9877617955207825, 1.7917295324391331, 1.7895606098503902, 0.9754152801530115, 1.7841870044839794, 1.0453474251507537, 1.117455956383879, 1.7818187322522618, 1.7844944082457443, 1.790965668086348, 1.7916029568376213, 0.9968108699239534, 1.7761882937013223]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxj7-SlSKb_3"
      },
      "source": [
        "**Grid search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aPmSObkPKbu3",
        "outputId": "4ee4e2f6-7038-49db-9ba6-667755ca46e8",
        "trusted": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        }
      },
      "source": [
        "NUM_CLASSES = 6\n",
        "DEVICE = 'cuda'\n",
        "#BATCH_SIZE = 16\n",
        "#LR = 0.001\n",
        "MOMENTUM = 0.9\n",
        "#WEIGHT_DECAY = 5e-5\n",
        "NUM_EPOCHS = 100\n",
        "STEP_SIZE = 60\n",
        "#GAMMA = 0.1\n",
        "\n",
        "lr_range = [0.005, 0.001, 0.0005]\n",
        "batch_size_range = [16, 8]\n",
        "weight_decay_range = [5e-5, 5e-3]\n",
        "gamma_range = [0.1, 0.01]\n",
        "hyperparameters_sets = []\n",
        "\n",
        "for lr in lr_range:\n",
        "  for batch_size in batch_size_range:\n",
        "    for weight_decay in weight_decay_range:\n",
        "      for gamma in gamma_range:\n",
        "        hyperparameters_sets.append({'lr': lr, 'batch_size': batch_size, 'weight_decay': weight_decay, 'gamma': gamma})\n",
        "\n",
        "for set in hyperparameters_sets:\n",
        "  print(set)\n",
        "\n",
        "\n",
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec-224'\n",
        "compose=[#transforms.Resize(224),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.RandomGrayscale(),\n",
        "         transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
        "         transforms.ToTensor()\n",
        "         ]\n",
        "train_dataset, val_dataset = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, compose)\n",
        "\n",
        "train_indexes = [idx for idx in range(len(train_dataset)) if idx % 5]\n",
        "val_indexes = [idx for idx in range(len(train_dataset)) if not idx % 5]\n",
        "val_dataset = Subset(val_dataset, val_indexes)\n",
        "train_dataset = Subset(train_dataset, train_indexes)\n",
        "print('training set {}'.format(len(train_dataset)))\n",
        "print('validation set {}'.format(len(val_dataset)))\n",
        "\n",
        "best_net = vgg11()\n",
        "best_net = best_net.to(DEVICE)\n",
        "best_net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "best_set = {}\n",
        "best_accuracy = 0.0\n",
        "best_loss = 0.0\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "\n",
        "for set in hyperparameters_sets:\n",
        "\n",
        "  net = vgg11()\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "  current_net, val_accuracy, val_loss = train_network(net, net.parameters(), set['lr'], NUM_EPOCHS, set['batch_size'], set['weight_decay'], STEP_SIZE, set['gamma'], train_dataset, val_dataset=val_dataset)\n",
        "  val_accuracies.append(val_accuracy)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  if val_accuracy > best_accuracy:\n",
        "    best_accuracy = val_accuracy\n",
        "    best_loss = val_loss\n",
        "    best_net = copy.deepcopy(current_net)\n",
        "    best_set = copy.deepcopy(set)\n",
        "  \n",
        "  print(\"({}), val accuracy {}, val loss {}\".format(set, val_accuracy, val_loss))\n",
        "\n",
        "print(\"\\n\\n({}), best val accuracy {}, best val loss {}\\n\".format(best_set, best_accuracy, best_loss))\n",
        "print(\"\\nval_accuracies\")\n",
        "print(val_accuracies)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lr': 0.005, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.1}\n",
            "{'lr': 0.005, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.01}\n",
            "{'lr': 0.005, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.1}\n",
            "{'lr': 0.005, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.01}\n",
            "{'lr': 0.005, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.1}\n",
            "{'lr': 0.005, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.01}\n",
            "{'lr': 0.005, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.1}\n",
            "{'lr': 0.005, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.01}\n",
            "{'lr': 0.001, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.1}\n",
            "{'lr': 0.001, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.01}\n",
            "{'lr': 0.001, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.1}\n",
            "{'lr': 0.001, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.01}\n",
            "{'lr': 0.001, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.1}\n",
            "{'lr': 0.001, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.01}\n",
            "{'lr': 0.001, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.1}\n",
            "{'lr': 0.001, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.01}\n",
            "{'lr': 0.0005, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.1}\n",
            "{'lr': 0.0005, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.01}\n",
            "{'lr': 0.0005, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.1}\n",
            "{'lr': 0.0005, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.01}\n",
            "{'lr': 0.0005, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.1}\n",
            "{'lr': 0.0005, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.01}\n",
            "{'lr': 0.0005, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.1}\n",
            "{'lr': 0.0005, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.01}\n",
            "Cloning into 'AIML_project'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 24392 (delta 10), reused 13 (delta 5), pack-reused 24373\u001b[K\n",
            "Receiving objects: 100% (24392/24392), 2.15 GiB | 47.69 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "Checking out files: 100% (24636/24636), done.\n",
            "training set 809\n",
            "validation set 203\n",
            "({'lr': 0.005, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.1}), val accuracy 0.4876847290640394, val loss 1.3147739389259827\n",
            "({'lr': 0.005, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.01}), val accuracy 0.6059113300492611, val loss 1.5047687838230226\n",
            "({'lr': 0.005, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.1}), val accuracy 0.3448275862068966, val loss 1.6433078479297056\n",
            "({'lr': 0.005, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.01}), val accuracy 0.5812807881773399, val loss 2.414570222347241\n",
            "({'lr': 0.005, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.1}), val accuracy 0.2955665024630542, val loss 1.7655747535780733\n",
            "({'lr': 0.005, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.01}), val accuracy 0.4876847290640394, val loss 1.3504160378366856\n",
            "({'lr': 0.005, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.1}), val accuracy 0.18226600985221675, val loss 1.7624990276515191\n",
            "({'lr': 0.005, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.01}), val accuracy 0.270935960591133, val loss 1.7162450311219164\n",
            "({'lr': 0.001, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.1}), val accuracy 0.6108374384236454, val loss 2.431815377597151\n",
            "({'lr': 0.001, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.01}), val accuracy 0.6551724137931034, val loss 1.9367152525873608\n",
            "({'lr': 0.001, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.1}), val accuracy 0.6403940886699507, val loss 1.528708166676789\n",
            "({'lr': 0.001, 'batch_size': 16, 'weight_decay': 0.005, 'gamma': 0.01}), val accuracy 0.6600985221674877, val loss 1.645547920847174\n",
            "({'lr': 0.001, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.1}), val accuracy 0.7536945812807881, val loss 1.4387520968620413\n",
            "({'lr': 0.001, 'batch_size': 8, 'weight_decay': 5e-05, 'gamma': 0.01}), val accuracy 0.6995073891625616, val loss 2.476622701278461\n",
            "({'lr': 0.001, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.1}), val accuracy 0.6650246305418719, val loss 3.20528265466831\n",
            "({'lr': 0.001, 'batch_size': 8, 'weight_decay': 0.005, 'gamma': 0.01}), val accuracy 0.6305418719211823, val loss 1.9122127376753708\n",
            "({'lr': 0.0005, 'batch_size': 16, 'weight_decay': 5e-05, 'gamma': 0.1}), val accuracy 0.6502463054187192, val loss 1.084208725121221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S1laZWm8Q0tm"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKl555WRQ1AF",
        "trusted": false,
        "colab": {}
      },
      "source": [
        "# todo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jJGI06ylKePa"
      },
      "source": [
        "**Mean / std computation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDJptx12L1OL",
        "trusted": false,
        "outputId": "3faa7a3e-7dce-418a-8a16-5ac13471f165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "TRAIN_DATA_DIR = 'AIML_project/ravdess-emotional-song-spec-672'\n",
        "pixel_mean = np.zeros(3)\n",
        "pixel_std = np.zeros(3)\n",
        "k = 1\n",
        "dataset, _ = get_datasets(TRAIN_DATA_DIR, TRAIN_DATA_DIR, [])\n",
        "for image, _ in tqdm(dataset, \"Computing mean/std\", len(dataset), unit=\"samples\"):\n",
        "    image = np.array(image)\n",
        "    pixels = image.reshape((-1, image.shape[2]))\n",
        "\n",
        "    for pixel in pixels:\n",
        "        diff = pixel - pixel_mean\n",
        "        pixel_mean += diff / k\n",
        "        pixel_std += diff * (pixel - pixel_mean)\n",
        "        k += 1\n",
        "\n",
        "pixel_std = np.sqrt(pixel_std / (k - 2))\n",
        "print(pixel_mean)\n",
        "print(pixel_std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AIML_project'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 24421 (delta 28), reused 33 (delta 14), pack-reused 24373\u001b[K\n",
            "Receiving objects: 100% (24421/24421), 2.15 GiB | 48.26 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Checking out files: 100% (24638/24638), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Computing mean/std: 100%|| 1012/1012 [47:31<00:00,  2.82s/samples]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[45.6068733   0.81077038 57.85301916]\n",
            "[66.92374056  9.88349788 49.96761776]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}